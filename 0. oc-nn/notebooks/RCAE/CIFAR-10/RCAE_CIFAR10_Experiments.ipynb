{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RCAE_CIFAR10_Experiments.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"6TKSizAjYUU1","colab_type":"code","outputId":"a9aba84e-e18c-49d9-cd18-b8cca397bfd9","executionInfo":{"status":"ok","timestamp":1547252499690,"user_tz":-660,"elapsed":9377,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install picklable_itertools\n","\n","!pip install fuel\n","%matplotlib inline\n","import numpy as np\n","%reload_ext autoreload\n","%autoreload 2\n","\n","PROJECT_DIR = \"/content/drive/My Drive/2019/testing/oc-nn/\"\n","import sys,os\n","import numpy as np\n","sys.path.append(PROJECT_DIR)"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: picklable_itertools in /usr/local/lib/python3.6/dist-packages (0.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from picklable_itertools) (1.11.0)\n","Requirement already satisfied: fuel in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.14.6)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fuel) (4.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fuel) (2.18.4)\n","Requirement already satisfied: picklable-itertools in /usr/local/lib/python3.6/dist-packages (from fuel) (0.1.1)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fuel) (17.0.0)\n","Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from fuel) (3.4.4)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from fuel) (3.38.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.1.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fuel) (1.11.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fuel) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fuel) (2.8.0)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->fuel) (0.46)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2.6)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (1.22)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2018.11.29)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (3.0.4)\n","Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->fuel) (2.6.9)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->fuel) (2.3.0)\n"],"name":"stdout"}]},{"metadata":{"id":"XtdtEZSYVlyb","colab_type":"text"},"cell_type":"markdown","source":["## Aeroplane Vs All "]},{"metadata":{"id":"pkAcM3JTVkia","colab_type":"code","outputId":"4024c205-4571-4433-cceb-264b90773352","executionInfo":{"status":"ok","timestamp":1547253648468,"user_tz":-660,"elapsed":1144049,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":19691}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","# RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2019/testing/oc-nn/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-1-35e4c1f576ad>\", line 6, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2019/testing/oc-nn//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","_X_test_beforegcn, (6500, 32, 32, 3) 1.0 0.0\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2019/testing/oc-nn//reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","_X_test_beforegcn, (6500, 32, 32, 3) 1.0 0.0\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","_X_test_beforegcn, (6500, 32, 32, 3) 1.0 0.0\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2019/testing/oc-nn//reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","_X_test_beforegcn, (6500, 32, 32, 3) 1.0 0.0\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/500\n","5850/5850 [==============================] - 5s 884us/step - loss: 1.3642 - val_loss: 1.4031\n","Epoch 2/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3360 - val_loss: 1.3583\n","Epoch 3/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3315 - val_loss: 1.3481\n","Epoch 4/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3294 - val_loss: 1.3408\n","Epoch 5/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3276 - val_loss: 1.3360\n","Epoch 6/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3265 - val_loss: 1.3342\n","Epoch 7/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3256 - val_loss: 1.3337\n","Epoch 8/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3247 - val_loss: 1.3330\n","Epoch 9/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3243 - val_loss: 1.3330\n","Epoch 10/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3238 - val_loss: 1.3334\n","Epoch 11/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3234 - val_loss: 1.3301\n","Epoch 12/500\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3228 - val_loss: 1.3300\n","Epoch 13/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3226 - val_loss: 1.3294\n","Epoch 14/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3223 - val_loss: 1.3298\n","Epoch 15/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3221 - val_loss: 1.3285\n","Epoch 16/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3218 - val_loss: 1.3286\n","Epoch 17/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3215 - val_loss: 1.3268\n","Epoch 18/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3212 - val_loss: 1.3274\n","Epoch 19/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3210 - val_loss: 1.3272\n","Epoch 20/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3208 - val_loss: 1.3274\n","Epoch 21/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3207 - val_loss: 1.3256\n","Epoch 22/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3205 - val_loss: 1.3267\n","Epoch 23/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3202 - val_loss: 1.3258\n","Epoch 24/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3200 - val_loss: 1.3245\n","Epoch 25/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3199 - val_loss: 1.3252\n","Epoch 26/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3198 - val_loss: 1.3249\n","Epoch 27/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3196 - val_loss: 1.3243\n","Epoch 28/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3194 - val_loss: 1.3260\n","Epoch 29/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3194 - val_loss: 1.3239\n","Epoch 30/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3194 - val_loss: 1.3243\n","Epoch 31/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3193 - val_loss: 1.3239\n","Epoch 32/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3191 - val_loss: 1.3242\n","Epoch 33/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3189 - val_loss: 1.3237\n","Epoch 34/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3187 - val_loss: 1.3228\n","Epoch 35/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3187 - val_loss: 1.3235\n","Epoch 36/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3185 - val_loss: 1.3228\n","Epoch 37/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3186 - val_loss: 1.3237\n","Epoch 38/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3186 - val_loss: 1.3233\n","Epoch 39/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3183 - val_loss: 1.3225\n","Epoch 40/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3183 - val_loss: 1.3228\n","Epoch 41/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3183 - val_loss: 1.3226\n","Epoch 42/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3181 - val_loss: 1.3224\n","Epoch 43/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3180 - val_loss: 1.3217\n","Epoch 44/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3179 - val_loss: 1.3226\n","Epoch 45/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3180 - val_loss: 1.3222\n","Epoch 46/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3178 - val_loss: 1.3230\n","Epoch 47/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3177 - val_loss: 1.3224\n","Epoch 48/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3179 - val_loss: 1.3224\n","Epoch 49/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3177 - val_loss: 1.3214\n","Epoch 50/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3176 - val_loss: 1.3223\n","Epoch 51/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3175 - val_loss: 1.3213\n","Epoch 52/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3173 - val_loss: 1.3215\n","Epoch 53/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3174 - val_loss: 1.3241\n","Epoch 54/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3173 - val_loss: 1.3211\n","Epoch 55/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3172 - val_loss: 1.3212\n","Epoch 56/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3172 - val_loss: 1.3210\n","Epoch 57/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3171 - val_loss: 1.3208\n","Epoch 58/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3169 - val_loss: 1.3207\n","Epoch 59/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3169 - val_loss: 1.3205\n","Epoch 60/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3169 - val_loss: 1.3208\n","Epoch 61/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3167 - val_loss: 1.3203\n","Epoch 62/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3167 - val_loss: 1.3211\n","Epoch 63/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3168 - val_loss: 1.3207\n","Epoch 64/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3167 - val_loss: 1.3204\n","Epoch 65/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3166 - val_loss: 1.3206\n","Epoch 66/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3165 - val_loss: 1.3206\n","Epoch 67/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3165 - val_loss: 1.3205\n","Epoch 68/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3164 - val_loss: 1.3200\n","Epoch 69/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3165 - val_loss: 1.3199\n","Epoch 70/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3164 - val_loss: 1.3202\n","Epoch 71/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3165 - val_loss: 1.3199\n","Epoch 72/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3163 - val_loss: 1.3204\n","Epoch 73/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3162 - val_loss: 1.3208\n","Epoch 74/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3161 - val_loss: 1.3201\n","Epoch 75/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3161 - val_loss: 1.3206\n","Epoch 76/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3162 - val_loss: 1.3198\n","Epoch 77/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3161 - val_loss: 1.3199\n","Epoch 78/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3159 - val_loss: 1.3193\n","Epoch 79/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3159 - val_loss: 1.3191\n","Epoch 80/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3159 - val_loss: 1.3194\n","Epoch 81/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3158 - val_loss: 1.3191\n","Epoch 82/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3157 - val_loss: 1.3192\n","Epoch 83/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3157 - val_loss: 1.3190\n","Epoch 84/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3155 - val_loss: 1.3189\n","Epoch 85/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3156 - val_loss: 1.3204\n","Epoch 86/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3156 - val_loss: 1.3190\n","Epoch 87/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3155 - val_loss: 1.3188\n","Epoch 88/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3155 - val_loss: 1.3188\n","Epoch 89/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3154 - val_loss: 1.3188\n","Epoch 90/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3154 - val_loss: 1.3187\n","Epoch 91/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3152 - val_loss: 1.3183\n","Epoch 92/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3153 - val_loss: 1.3188\n","Epoch 93/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3152 - val_loss: 1.3185\n","Epoch 94/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3151 - val_loss: 1.3184\n","Epoch 95/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3152 - val_loss: 1.3184\n","Epoch 96/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3152 - val_loss: 1.3190\n","Epoch 97/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3151 - val_loss: 1.3184\n","Epoch 98/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3150 - val_loss: 1.3183\n","Epoch 99/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3150 - val_loss: 1.3183\n","Epoch 100/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3149 - val_loss: 1.3184\n","Epoch 101/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3148 - val_loss: 1.3183\n","Epoch 102/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3148 - val_loss: 1.3182\n","Epoch 103/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3148 - val_loss: 1.3183\n","Epoch 104/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3148 - val_loss: 1.3186\n","Epoch 105/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3150 - val_loss: 1.3211\n","Epoch 106/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3148 - val_loss: 1.3182\n","Epoch 107/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3147 - val_loss: 1.3183\n","Epoch 108/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3147 - val_loss: 1.3179\n","Epoch 109/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3147 - val_loss: 1.3182\n","Epoch 110/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3145 - val_loss: 1.3178\n","Epoch 111/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3145 - val_loss: 1.3178\n","Epoch 112/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3145 - val_loss: 1.3175\n","Epoch 113/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3145 - val_loss: 1.3178\n","Epoch 114/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3143 - val_loss: 1.3173\n","Epoch 115/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3144 - val_loss: 1.3172\n","Epoch 116/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3144 - val_loss: 1.3174\n","Epoch 117/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3143 - val_loss: 1.3175\n","Epoch 118/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3144 - val_loss: 1.3178\n","Epoch 119/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3144 - val_loss: 1.3173\n","Epoch 120/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3142 - val_loss: 1.3173\n","Epoch 121/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3141 - val_loss: 1.3170\n","Epoch 122/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3142 - val_loss: 1.3175\n","Epoch 123/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3142 - val_loss: 1.3169\n","Epoch 124/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3140 - val_loss: 1.3171\n","Epoch 125/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3141 - val_loss: 1.3171\n","Epoch 126/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3142 - val_loss: 1.3174\n","Epoch 127/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3141 - val_loss: 1.3173\n","Epoch 128/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3140 - val_loss: 1.3171\n","Epoch 129/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3140 - val_loss: 1.3167\n","Epoch 130/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3139 - val_loss: 1.3172\n","Epoch 131/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3139 - val_loss: 1.3166\n","Epoch 132/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3139 - val_loss: 1.3167\n","Epoch 133/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3138 - val_loss: 1.3168\n","Epoch 134/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3138 - val_loss: 1.3168\n","Epoch 135/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3142 - val_loss: 1.3176\n","Epoch 136/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3139 - val_loss: 1.3175\n","Epoch 137/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3138 - val_loss: 1.3173\n","Epoch 138/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 139/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3137 - val_loss: 1.3164\n","Epoch 140/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3137 - val_loss: 1.3168\n","Epoch 141/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3137 - val_loss: 1.3167\n","Epoch 142/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3137 - val_loss: 1.3163\n","Epoch 143/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 144/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3136 - val_loss: 1.3165\n","Epoch 145/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 146/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3136 - val_loss: 1.3163\n","Epoch 147/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3135 - val_loss: 1.3165\n","Epoch 148/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3135 - val_loss: 1.3163\n","Epoch 149/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3133 - val_loss: 1.3163\n","Epoch 150/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3134 - val_loss: 1.3163\n","Epoch 151/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3133 - val_loss: 1.3161\n","Epoch 152/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3133 - val_loss: 1.3161\n","Epoch 153/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3134 - val_loss: 1.3161\n","Epoch 154/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3134 - val_loss: 1.3165\n","Epoch 155/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3133 - val_loss: 1.3162\n","Epoch 156/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3134 - val_loss: 1.3159\n","Epoch 157/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3132 - val_loss: 1.3160\n","Epoch 158/500\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3163\n","Epoch 159/500\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3161\n","Epoch 160/500\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3134 - val_loss: 1.3161\n","Epoch 161/500\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3132 - val_loss: 1.3160\n","Epoch 162/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3209\n","Epoch 163/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3133 - val_loss: 1.3167\n","Epoch 164/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3132 - val_loss: 1.3165\n","Epoch 165/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3132 - val_loss: 1.3160\n","Epoch 166/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3133 - val_loss: 1.3160\n","Epoch 167/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3131 - val_loss: 1.3160\n","Epoch 168/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3131 - val_loss: 1.3157\n","Epoch 169/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3132 - val_loss: 1.3159\n","Epoch 170/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3130 - val_loss: 1.3156\n","Epoch 171/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 172/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 173/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3131 - val_loss: 1.3158\n","Epoch 174/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3130 - val_loss: 1.3156\n","Epoch 175/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3129 - val_loss: 1.3156\n","Epoch 176/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3129 - val_loss: 1.3154\n","Epoch 177/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3129 - val_loss: 1.3156\n","Epoch 178/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3157\n","Epoch 179/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 180/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3131 - val_loss: 1.3158\n","Epoch 181/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3129 - val_loss: 1.3154\n","Epoch 182/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3159\n","Epoch 183/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3129 - val_loss: 1.3158\n","Epoch 184/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3128 - val_loss: 1.3156\n","Epoch 185/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3128 - val_loss: 1.3155\n","Epoch 186/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3128 - val_loss: 1.3153\n","Epoch 187/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3128 - val_loss: 1.3168\n","Epoch 188/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3147 - val_loss: 1.3320\n","Epoch 189/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3136 - val_loss: 1.3234\n","Epoch 190/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3132 - val_loss: 1.3206\n","Epoch 191/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3130 - val_loss: 1.3178\n","Epoch 192/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3129 - val_loss: 1.3171\n","Epoch 193/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3128 - val_loss: 1.3162\n","Epoch 194/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3129 - val_loss: 1.3154\n","Epoch 195/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3129 - val_loss: 1.3155\n","Epoch 196/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3128 - val_loss: 1.3154\n","Epoch 197/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3129 - val_loss: 1.3161\n","Epoch 198/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3128 - val_loss: 1.3152\n","Epoch 199/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3127 - val_loss: 1.3152\n","Epoch 200/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3127 - val_loss: 1.3151\n","Epoch 201/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3127 - val_loss: 1.3176\n","Epoch 202/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3148 - val_loss: 1.3419\n","Epoch 203/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3136 - val_loss: 1.3252\n","Epoch 204/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3132 - val_loss: 1.3199\n","Epoch 205/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3176\n","Epoch 206/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3129 - val_loss: 1.3163\n","Epoch 207/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3129 - val_loss: 1.3160\n","Epoch 208/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3127 - val_loss: 1.3157\n","Epoch 209/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3128 - val_loss: 1.3153\n","Epoch 210/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3128 - val_loss: 1.3153\n","Epoch 211/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3127 - val_loss: 1.3152\n","Epoch 212/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3126 - val_loss: 1.3150\n","Epoch 213/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3127 - val_loss: 1.3150\n","Epoch 214/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3126 - val_loss: 1.3150\n","Epoch 215/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3126 - val_loss: 1.3149\n","Epoch 216/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3126 - val_loss: 1.3150\n","Epoch 217/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3126 - val_loss: 1.3149\n","Epoch 218/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3125 - val_loss: 1.3148\n","Epoch 219/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3126 - val_loss: 1.3151\n","Epoch 220/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3126 - val_loss: 1.3150\n","Epoch 221/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3125 - val_loss: 1.3149\n","Epoch 222/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3126 - val_loss: 1.3149\n","Epoch 223/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3125 - val_loss: 1.3150\n","Epoch 224/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3125 - val_loss: 1.3148\n","Epoch 225/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3125 - val_loss: 1.3149\n","Epoch 226/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3125 - val_loss: 1.3149\n","Epoch 227/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3124 - val_loss: 1.3150\n","Epoch 228/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3124 - val_loss: 1.3147\n","Epoch 229/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3124 - val_loss: 1.3148\n","Epoch 230/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3124 - val_loss: 1.3148\n","Epoch 231/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 232/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3124 - val_loss: 1.3147\n","Epoch 233/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3123 - val_loss: 1.3147\n","Epoch 234/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 235/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3123 - val_loss: 1.3147\n","Epoch 236/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 237/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3123 - val_loss: 1.3149\n","Epoch 238/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 239/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3123 - val_loss: 1.3149\n","Epoch 240/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3123 - val_loss: 1.3147\n","Epoch 241/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3124 - val_loss: 1.3148\n","Epoch 242/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3122 - val_loss: 1.3147\n","Epoch 243/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3122 - val_loss: 1.3145\n","Epoch 244/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3121 - val_loss: 1.3147\n","Epoch 245/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3122 - val_loss: 1.3150\n","Epoch 246/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3123 - val_loss: 1.3146\n","Epoch 247/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3122 - val_loss: 1.3147\n","Epoch 248/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3123 - val_loss: 1.3149\n","Epoch 249/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 250/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3124 - val_loss: 1.3154\n","Epoch 251/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3122 - val_loss: 1.3148\n","Epoch 252/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3122 - val_loss: 1.3148\n","Epoch 253/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3122 - val_loss: 1.3146\n","Epoch 254/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3121 - val_loss: 1.3146\n","Epoch 255/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3147\n","Epoch 256/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3122 - val_loss: 1.3147\n","Epoch 257/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3121 - val_loss: 1.3145\n","Epoch 258/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3122 - val_loss: 1.3146\n","Epoch 259/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3121 - val_loss: 1.3146\n","Epoch 260/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 261/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3122 - val_loss: 1.3146\n","Epoch 262/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3144\n","Epoch 263/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3121 - val_loss: 1.3149\n","Epoch 264/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3121 - val_loss: 1.3148\n","Epoch 265/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3124 - val_loss: 1.3156\n","Epoch 266/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3121 - val_loss: 1.3151\n","Epoch 267/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3121 - val_loss: 1.3146\n","Epoch 268/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3120 - val_loss: 1.3146\n","Epoch 269/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3121 - val_loss: 1.3144\n","Epoch 270/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3120 - val_loss: 1.3145\n","Epoch 271/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3120 - val_loss: 1.3145\n","Epoch 272/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 273/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3120 - val_loss: 1.3143\n","Epoch 274/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3120 - val_loss: 1.3143\n","Epoch 275/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3120 - val_loss: 1.3144\n","Epoch 276/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3120 - val_loss: 1.3144\n","Epoch 277/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 278/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3120 - val_loss: 1.3144\n","Epoch 279/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3121 - val_loss: 1.3145\n","Epoch 280/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3120 - val_loss: 1.3144\n","Epoch 281/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 282/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 283/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3119 - val_loss: 1.3145\n","Epoch 284/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3119 - val_loss: 1.3143\n","Epoch 285/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3118 - val_loss: 1.3148\n","Epoch 286/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3149\n","Epoch 287/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 288/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3118 - val_loss: 1.3143\n","Epoch 289/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3118 - val_loss: 1.3148\n","Epoch 290/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3119 - val_loss: 1.3145\n","Epoch 291/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 292/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3120 - val_loss: 1.3143\n","Epoch 293/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3146\n","Epoch 294/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3118 - val_loss: 1.3142\n","Epoch 295/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3118 - val_loss: 1.3142\n","Epoch 296/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3118 - val_loss: 1.3147\n","Epoch 297/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 298/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3120 - val_loss: 1.3148\n","Epoch 299/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 300/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 301/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3117 - val_loss: 1.3142\n","Epoch 302/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3118 - val_loss: 1.3141\n","Epoch 303/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3118 - val_loss: 1.3142\n","Epoch 304/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 305/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3119 - val_loss: 1.3149\n","Epoch 306/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3156\n","Epoch 307/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3149\n","Epoch 308/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3122 - val_loss: 1.3167\n","Epoch 309/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3119 - val_loss: 1.3158\n","Epoch 310/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3117 - val_loss: 1.3150\n","Epoch 311/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3117 - val_loss: 1.3148\n","Epoch 312/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3117 - val_loss: 1.3144\n","Epoch 313/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3116 - val_loss: 1.3147\n","Epoch 314/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3117 - val_loss: 1.3142\n","Epoch 315/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3121 - val_loss: 1.3148\n","Epoch 316/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3118 - val_loss: 1.3146\n","Epoch 317/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3125 - val_loss: 1.3160\n","Epoch 318/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3118 - val_loss: 1.3149\n","Epoch 319/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3118 - val_loss: 1.3148\n","Epoch 320/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3117 - val_loss: 1.3142\n","Epoch 321/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3117 - val_loss: 1.3141\n","Epoch 322/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3117 - val_loss: 1.3141\n","Epoch 323/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3117 - val_loss: 1.3140\n","Epoch 324/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3117 - val_loss: 1.3140\n","Epoch 325/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3116 - val_loss: 1.3140\n","Epoch 326/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 327/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3117 - val_loss: 1.3142\n","Epoch 328/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3117 - val_loss: 1.3139\n","Epoch 329/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3116 - val_loss: 1.3139\n","Epoch 330/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3116 - val_loss: 1.3141\n","Epoch 331/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3119 - val_loss: 1.3146\n","Epoch 332/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3117 - val_loss: 1.3140\n","Epoch 333/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3116 - val_loss: 1.3140\n","Epoch 334/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3116 - val_loss: 1.3140\n","Epoch 335/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3116 - val_loss: 1.3139\n","Epoch 336/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3116 - val_loss: 1.3139\n","Epoch 337/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3115 - val_loss: 1.3139\n","Epoch 338/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3116 - val_loss: 1.3138\n","Epoch 339/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3116 - val_loss: 1.3141\n","Epoch 340/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3116 - val_loss: 1.3139\n","Epoch 341/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3115 - val_loss: 1.3138\n","Epoch 342/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 343/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3116 - val_loss: 1.3139\n","Epoch 344/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3116 - val_loss: 1.3140\n","Epoch 345/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 346/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3115 - val_loss: 1.3142\n","Epoch 347/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3116 - val_loss: 1.3141\n","Epoch 348/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3115 - val_loss: 1.3139\n","Epoch 349/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 350/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 351/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 352/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3115 - val_loss: 1.3138\n","Epoch 353/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3115 - val_loss: 1.3139\n","Epoch 354/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 355/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3115 - val_loss: 1.3137\n","Epoch 356/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 357/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3115 - val_loss: 1.3139\n","Epoch 358/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3114 - val_loss: 1.3139\n","Epoch 359/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3114 - val_loss: 1.3140\n","Epoch 360/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3114 - val_loss: 1.3139\n","Epoch 361/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 362/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3114 - val_loss: 1.3144\n","Epoch 363/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3119 - val_loss: 1.3148\n","Epoch 364/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3115 - val_loss: 1.3142\n","Epoch 365/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3115 - val_loss: 1.3141\n","Epoch 366/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3114 - val_loss: 1.3141\n","Epoch 367/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3115 - val_loss: 1.3143\n","Epoch 368/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3126 - val_loss: 1.3273\n","Epoch 369/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3125 - val_loss: 1.3184\n","Epoch 370/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3118 - val_loss: 1.3160\n","Epoch 371/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3117 - val_loss: 1.3150\n","Epoch 372/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3116 - val_loss: 1.3144\n","Epoch 373/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3115 - val_loss: 1.3142\n","Epoch 374/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 375/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3115 - val_loss: 1.3139\n","Epoch 376/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 377/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3116 - val_loss: 1.3138\n","Epoch 378/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3114 - val_loss: 1.3137\n","Epoch 379/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 380/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3114 - val_loss: 1.3139\n","Epoch 381/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 382/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3114 - val_loss: 1.3136\n","Epoch 383/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 384/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 385/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3115 - val_loss: 1.3140\n","Epoch 386/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 387/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 388/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3115 - val_loss: 1.3139\n","Epoch 389/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3115 - val_loss: 1.3137\n","Epoch 390/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3114 - val_loss: 1.3137\n","Epoch 391/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3113 - val_loss: 1.3138\n","Epoch 392/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3114 - val_loss: 1.3136\n","Epoch 393/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 394/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 395/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 396/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 397/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3114 - val_loss: 1.3137\n","Epoch 398/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3113 - val_loss: 1.3138\n","Epoch 399/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3114 - val_loss: 1.3137\n","Epoch 400/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3113 - val_loss: 1.3135\n","Epoch 401/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3114 - val_loss: 1.3137\n","Epoch 402/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3114 - val_loss: 1.3138\n","Epoch 403/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3114 - val_loss: 1.3136\n","Epoch 404/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3113 - val_loss: 1.3136\n","Epoch 405/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3114 - val_loss: 1.3141\n","Epoch 406/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3139\n","Epoch 407/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3136\n","Epoch 408/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 409/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3113 - val_loss: 1.3135\n","Epoch 410/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 411/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 412/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3113 - val_loss: 1.3138\n","Epoch 413/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 414/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3113 - val_loss: 1.3136\n","Epoch 415/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3135\n","Epoch 416/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 417/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3113 - val_loss: 1.3138\n","Epoch 418/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3115 - val_loss: 1.3142\n","Epoch 419/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3114 - val_loss: 1.3140\n","Epoch 420/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3113 - val_loss: 1.3139\n","Epoch 421/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 422/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3112 - val_loss: 1.3138\n","Epoch 423/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3112 - val_loss: 1.3138\n","Epoch 424/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3113 - val_loss: 1.3136\n","Epoch 425/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3112 - val_loss: 1.3135\n","Epoch 426/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3142\n","Epoch 427/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3117 - val_loss: 1.3166\n","Epoch 428/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3114 - val_loss: 1.3144\n","Epoch 429/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3113 - val_loss: 1.3140\n","Epoch 430/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 431/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 432/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 433/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3112 - val_loss: 1.3136\n","Epoch 434/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3113 - val_loss: 1.3136\n","Epoch 435/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3112 - val_loss: 1.3136\n","Epoch 436/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 437/500\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 438/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3120 - val_loss: 1.3160\n","Epoch 439/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3115 - val_loss: 1.3145\n","Epoch 440/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3116 - val_loss: 1.3145\n","Epoch 441/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3113 - val_loss: 1.3139\n","Epoch 442/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3113 - val_loss: 1.3138\n","Epoch 443/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 444/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 445/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3112 - val_loss: 1.3136\n","Epoch 446/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3111 - val_loss: 1.3136\n","Epoch 447/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3112 - val_loss: 1.3135\n","Epoch 448/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3111 - val_loss: 1.3138\n","Epoch 449/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3117 - val_loss: 1.3161\n","Epoch 450/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3114 - val_loss: 1.3150\n","Epoch 451/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3141\n","Epoch 452/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3113 - val_loss: 1.3140\n","Epoch 453/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 454/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3113 - val_loss: 1.3138\n","Epoch 455/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3113 - val_loss: 1.3137\n","Epoch 456/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 457/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3112 - val_loss: 1.3134\n","Epoch 458/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3111 - val_loss: 1.3136\n","Epoch 459/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3111 - val_loss: 1.3136\n","Epoch 460/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3111 - val_loss: 1.3133\n","Epoch 461/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3111 - val_loss: 1.3135\n","Epoch 462/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3111 - val_loss: 1.3137\n","Epoch 463/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3112 - val_loss: 1.3134\n","Epoch 464/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 465/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 466/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 467/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3111 - val_loss: 1.3135\n","Epoch 468/500\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3112 - val_loss: 1.3139\n","Epoch 469/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3116 - val_loss: 1.3166\n","Epoch 470/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3113 - val_loss: 1.3142\n","Epoch 471/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3112 - val_loss: 1.3137\n","Epoch 472/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3111 - val_loss: 1.3136\n","Epoch 473/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3112 - val_loss: 1.3135\n","Epoch 474/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3111 - val_loss: 1.3135\n","Epoch 475/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3111 - val_loss: 1.3133\n","Epoch 476/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 477/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 478/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3111 - val_loss: 1.3136\n","Epoch 479/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 480/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3112 - val_loss: 1.3134\n","Epoch 481/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 482/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3112 - val_loss: 1.3135\n","Epoch 483/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3110 - val_loss: 1.3133\n","Epoch 484/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3111 - val_loss: 1.3133\n","Epoch 485/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3112 - val_loss: 1.3134\n","Epoch 486/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3111 - val_loss: 1.3133\n","Epoch 487/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3110 - val_loss: 1.3135\n","Epoch 488/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3110 - val_loss: 1.3134\n","Epoch 489/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3110 - val_loss: 1.3134\n","Epoch 490/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 491/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3110 - val_loss: 1.3134\n","Epoch 492/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3110 - val_loss: 1.3136\n","Epoch 493/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3110 - val_loss: 1.3132\n","Epoch 494/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3109 - val_loss: 1.3133\n","Epoch 495/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3110 - val_loss: 1.3133\n","Epoch 496/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3110 - val_loss: 1.3132\n","Epoch 497/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3111 - val_loss: 1.3134\n","Epoch 498/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3110 - val_loss: 1.3133\n","Epoch 499/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3110 - val_loss: 1.3139\n","Epoch 500/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3124 - val_loss: 1.3295\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1614687 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.6848175525665283\n","The max value of N 0.7103922963142395\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2019/testing/oc-nn//reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2019/testing/oc-nn//reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6960138333333333\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6960138333333333]\n","AUROC ===== 0.6960138333333333 +/- 0.0\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nNzkAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7fa666915668>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"TpodD97_Wb-4","colab_type":"text"},"cell_type":"markdown","source":["## Automobile Vs All "]},{"metadata":{"id":"_6-RBsdSWaW6","colab_type":"code","outputId":"9ecffa50-3606-46be-c8f3-3fc69c29ab72","executionInfo":{"status":"ok","timestamp":1541655758161,"user_tz":-660,"elapsed":1165093,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":20813}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","# RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/500\n","5850/5850 [==============================] - 5s 909us/step - loss: 1.4862 - val_loss: 1.8324\n","Epoch 2/500\n","5850/5850 [==============================] - 2s 412us/step - loss: 1.3491 - val_loss: 1.4091\n","Epoch 3/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3273 - val_loss: 1.3799\n","Epoch 4/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3206 - val_loss: 1.3550\n","Epoch 5/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3166 - val_loss: 1.3109\n","Epoch 6/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3132 - val_loss: 1.3174\n","Epoch 7/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3110 - val_loss: 1.3108\n","Epoch 8/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3094 - val_loss: 1.3087\n","Epoch 9/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3083 - val_loss: 1.3081\n","Epoch 10/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3077 - val_loss: 1.3077\n","Epoch 11/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3074 - val_loss: 1.3075\n","Epoch 12/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3070 - val_loss: 1.3069\n","Epoch 13/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3069 - val_loss: 1.3075\n","Epoch 14/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3067 - val_loss: 1.3070\n","Epoch 15/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 16/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 17/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3064 - val_loss: 1.3074\n","Epoch 18/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3071 - val_loss: 1.3162\n","Epoch 19/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3066 - val_loss: 1.3131\n","Epoch 20/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3064 - val_loss: 1.3094\n","Epoch 21/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3062 - val_loss: 1.3113\n","Epoch 22/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3065 - val_loss: 1.3219\n","Epoch 23/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3076 - val_loss: 1.3112\n","Epoch 24/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3070 - val_loss: 1.3078\n","Epoch 25/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3065 - val_loss: 1.3069\n","Epoch 26/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3064 - val_loss: 1.3067\n","Epoch 27/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3067\n","Epoch 28/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 29/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3062 - val_loss: 1.3067\n","Epoch 30/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3061 - val_loss: 1.3068\n","Epoch 31/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 32/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 33/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 34/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 35/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 36/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 37/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 38/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 39/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 40/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 41/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 42/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 43/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3066\n","Epoch 44/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 45/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 46/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 47/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 48/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3076\n","Epoch 49/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 50/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 51/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 52/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 53/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 54/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3069\n","Epoch 55/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 56/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 57/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 58/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 59/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 60/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 61/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 62/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 66/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 67/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 68/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 69/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 71/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 72/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 73/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 74/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 75/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 76/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 77/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 78/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3073\n","Epoch 79/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3074\n","Epoch 80/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 81/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 82/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 83/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 84/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 85/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 86/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 87/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 88/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 89/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 90/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 91/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 92/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 93/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 94/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 95/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 96/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 97/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 98/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 99/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 100/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 101/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 102/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 103/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 104/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 105/500\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 106/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 107/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 108/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 109/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 110/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 111/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 112/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 113/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 114/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 115/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 116/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 117/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 118/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 119/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 120/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 121/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 122/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 123/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 124/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 125/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 126/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 127/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 128/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 129/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 130/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 131/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 132/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 133/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 134/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 135/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 136/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 137/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 138/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 139/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 140/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 141/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 142/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 143/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 144/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 145/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 146/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 147/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 148/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 149/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 150/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 151/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 152/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 153/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 154/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 155/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 156/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 157/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 158/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 159/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 160/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 161/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 162/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 163/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 164/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 165/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 166/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 167/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 168/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 169/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 170/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 171/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 172/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 173/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 174/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 175/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 176/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 177/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 178/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 179/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 180/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 181/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 182/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 183/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 184/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 185/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 186/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 187/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 188/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 189/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 190/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 191/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 192/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 193/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 194/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 195/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 196/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 197/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 198/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 199/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 200/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 201/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 202/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 203/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 204/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 205/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 206/500\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 207/500\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 208/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 209/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 210/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 211/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 212/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 213/500\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 214/500\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 215/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 216/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 217/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 218/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 219/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 220/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 221/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 222/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 223/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 224/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 225/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 226/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 227/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 228/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 229/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 230/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 231/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 232/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 233/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 234/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 235/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 236/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 237/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 238/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 239/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 240/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 241/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 242/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 243/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 244/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 245/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 246/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 247/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 248/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 249/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 250/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 251/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 252/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 253/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 254/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 255/500\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 256/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 257/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 258/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 259/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 260/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 261/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 262/500\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 263/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 264/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 265/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 266/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 267/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 268/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 269/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 270/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 271/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 272/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 273/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 274/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 275/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 276/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 277/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 278/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 279/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 280/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 281/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 282/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 283/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 284/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 285/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 286/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 287/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3145\n","Epoch 288/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3532\n","Epoch 289/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3084\n","Epoch 290/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 291/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 292/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 293/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 294/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 295/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 296/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 297/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 298/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 299/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 300/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 301/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 302/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 303/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 304/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 305/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 306/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 307/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 308/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 309/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 310/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 311/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 312/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 313/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 314/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 315/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 316/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 317/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 318/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 319/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 320/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 321/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 322/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 323/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 324/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 325/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 326/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 327/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 328/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 329/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 330/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 331/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 332/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 333/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 334/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 335/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 336/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 337/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 338/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 339/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 340/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 341/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 342/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 343/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 344/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 345/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 346/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 347/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 348/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 349/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 350/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 351/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 352/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 353/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 354/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 355/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 356/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 357/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 358/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 359/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 360/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 361/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 362/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 363/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 364/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 365/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 366/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 367/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 368/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 369/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 370/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 371/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 372/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 373/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 374/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 375/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 376/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 377/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 378/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 379/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 380/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 381/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 382/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 383/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 384/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 385/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 386/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 387/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 388/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 389/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 390/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 391/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 392/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 393/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 394/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 395/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 396/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 397/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 398/500\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 399/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 400/500\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 401/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 402/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 403/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 404/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 405/500\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 406/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 407/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 408/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 409/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 410/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 411/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 412/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 413/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 414/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 415/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 416/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 417/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 418/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 419/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 420/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 421/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 422/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 423/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 424/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 425/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 426/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 427/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 428/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 429/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 430/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 431/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 432/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 433/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 434/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 435/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 436/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 437/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 438/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 439/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 440/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 441/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 442/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 443/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 444/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 445/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 446/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 447/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 448/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 449/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 450/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 451/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 452/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 453/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 454/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 455/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 456/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 457/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 458/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 459/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 460/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 461/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 462/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 463/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 464/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 465/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 466/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 467/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 468/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 469/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 470/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 471/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 472/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 473/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 474/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 475/500\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 476/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 477/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 478/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 479/500\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 480/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 481/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 482/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 483/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 484/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 485/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 486/500\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 487/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 488/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 489/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 490/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 491/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 492/500\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 493/500\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 494/500\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 495/500\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 496/500\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 497/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 498/500\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 499/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 500/500\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3058\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 0 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  0.0\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6305554999999999\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6305554999999999]\n","AUROC ===== 0.6305554999999999 +/- 0.0\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n20cdoa4YXwAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f54d61428d0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"_mbxye_PcosH","colab_type":"text"},"cell_type":"markdown","source":["## Bird Vs All "]},{"metadata":{"id":"ndkLI8wlcn1p","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZJt-QbKSc0GY","colab_type":"text"},"cell_type":"markdown","source":["## Cat Vs All "]},{"metadata":{"id":"oug1IAQEczpZ","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"gfCplR9Lc99m","colab_type":"text"},"cell_type":"markdown","source":["## Deer Vs All "]},{"metadata":{"id":"pSewrNSUc86w","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"LJzLiHTRyW8z","colab_type":"text"},"cell_type":"markdown","source":["## Dog Vs All "]},{"metadata":{"id":"qXddMnxryTI8","colab_type":"code","outputId":"e9ea5466-6d74-417f-9ce3-a8cb8390b262","executionInfo":{"status":"ok","timestamp":1541765132290,"user_tz":-660,"elapsed":3664651,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":64319}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 952us/step - loss: 1.4967 - val_loss: 1.7668\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3538 - val_loss: 1.3619\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3255 - val_loss: 1.3397\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3160 - val_loss: 1.3092\n","Epoch 5/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3117 - val_loss: 1.3084\n","Epoch 6/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3102 - val_loss: 1.3062\n","Epoch 7/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3091 - val_loss: 1.3062\n","Epoch 8/150\n","5850/5850 [==============================] - 3s 437us/step - loss: 1.3099 - val_loss: 1.3129\n","Epoch 9/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3081 - val_loss: 1.3075\n","Epoch 10/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3093 - val_loss: 1.3279\n","Epoch 11/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3079 - val_loss: 1.3080\n","Epoch 12/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3073 - val_loss: 1.3070\n","Epoch 13/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3071 - val_loss: 1.3065\n","Epoch 14/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3068 - val_loss: 1.3064\n","Epoch 15/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3067 - val_loss: 1.3062\n","Epoch 16/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3065 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 3s 446us/step - loss: 1.3065 - val_loss: 1.3064\n","Epoch 18/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3064 - val_loss: 1.3063\n","Epoch 19/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 20/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 21/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 22/150\n","5850/5850 [==============================] - 3s 427us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 23/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 427us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 426us/step - loss: 1.3062 - val_loss: 1.3060\n","Epoch 26/150\n","5850/5850 [==============================] - 3s 430us/step - loss: 1.3065 - val_loss: 1.3067\n","Epoch 27/150\n","5850/5850 [==============================] - 3s 434us/step - loss: 1.3064 - val_loss: 1.3060\n","Epoch 28/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 29/150\n","5850/5850 [==============================] - 3s 430us/step - loss: 1.3062 - val_loss: 1.3064\n","Epoch 30/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 31/150\n","5850/5850 [==============================] - 3s 434us/step - loss: 1.3061 - val_loss: 1.3072\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 426us/step - loss: 1.3062 - val_loss: 1.3067\n","Epoch 33/150\n","5850/5850 [==============================] - 3s 433us/step - loss: 1.3061 - val_loss: 1.3099\n","Epoch 34/150\n","5850/5850 [==============================] - 3s 435us/step - loss: 1.3062 - val_loss: 1.3176\n","Epoch 35/150\n","5850/5850 [==============================] - 3s 436us/step - loss: 1.3061 - val_loss: 1.3079\n","Epoch 36/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 37/150\n","5850/5850 [==============================] - 3s 434us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 38/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 39/150\n","5850/5850 [==============================] - 3s 430us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 3s 437us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 41/150\n","5850/5850 [==============================] - 3s 435us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 3s 434us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 45/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 46/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 50/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 52/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 53/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 54/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 55/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 56/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 57/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 58/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 59/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 60/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 61/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 62/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 63/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3060 - val_loss: 1.3069\n","Epoch 64/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 65/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 66/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 68/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 69/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 70/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 71/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 72/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 73/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 75/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 76/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 77/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 78/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 79/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 80/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 81/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 82/150\n","5850/5850 [==============================] - 3s 469us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 83/150\n","5850/5850 [==============================] - 3s 472us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 3s 467us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 85/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 86/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 87/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 88/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 89/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 90/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 91/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 92/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 93/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 94/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 95/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 96/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 97/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 98/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 99/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 100/150\n","5850/5850 [==============================] - 3s 464us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 101/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 102/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 103/150\n","5850/5850 [==============================] - 3s 463us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 104/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 105/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 106/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 108/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 109/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 110/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 111/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 112/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 113/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 114/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 115/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 116/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 117/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 118/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 119/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 120/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 121/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 122/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 123/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 124/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 125/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 126/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 127/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 128/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 129/150\n","5850/5850 [==============================] - 3s 439us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 130/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 131/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 132/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 133/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 134/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 135/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 136/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 137/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 138/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 139/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 140/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 141/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 142/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 143/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 144/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 145/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 146/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 147/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 148/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 149/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 150/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3059 - val_loss: 1.3059\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 6 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7712744474411011\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7040363333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 890us/step - loss: 1.4802 - val_loss: 1.4621\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 425us/step - loss: 1.3495 - val_loss: 1.3161\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3264 - val_loss: 1.3106\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3209 - val_loss: 1.3109\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3165 - val_loss: 1.3097\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3129 - val_loss: 1.3076\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3105 - val_loss: 1.3060\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3108 - val_loss: 1.3076\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3096 - val_loss: 1.3061\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3097 - val_loss: 1.3074\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3083 - val_loss: 1.3066\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3070 - val_loss: 1.3062\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3070 - val_loss: 1.3070\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3066 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3068\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3059 - val_loss: 1.3071\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3059 - val_loss: 1.3074\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3069 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3058\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1753 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7818845510482788\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6602281666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.5089 - val_loss: 2.0750\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 427us/step - loss: 1.3592 - val_loss: 1.9308\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3260 - val_loss: 1.3082\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3166 - val_loss: 1.3067\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3114 - val_loss: 1.3051\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3092 - val_loss: 1.3044\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3078 - val_loss: 1.3048\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3064 - val_loss: 1.3048\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3041\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3053\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3052 - val_loss: 1.3041\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3058 - val_loss: 1.3135\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3049 - val_loss: 1.3045\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3044 - val_loss: 1.3041\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3042 - val_loss: 1.3039\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3040 - val_loss: 1.3038\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3039 - val_loss: 1.3037\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3040 - val_loss: 1.3037\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3039 - val_loss: 1.3037\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3037 - val_loss: 1.3037\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3036 - val_loss: 1.3037\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3040\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3034\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 135 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.78167724609375\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6262536666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.5482 - val_loss: 1.8039\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 421us/step - loss: 1.4032 - val_loss: 1.3260\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3194 - val_loss: 1.3095\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3092 - val_loss: 1.3084\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3077 - val_loss: 1.3076\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3072 - val_loss: 1.3071\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3068 - val_loss: 1.3068\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3067 - val_loss: 1.3066\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 0 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  0.0\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.5972505\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.5083 - val_loss: 1.5508\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 425us/step - loss: 1.3592 - val_loss: 1.7629\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3291 - val_loss: 1.3453\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3127 - val_loss: 1.3074\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3072 - val_loss: 1.3016\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3011\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3039 - val_loss: 1.3008\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3025 - val_loss: 1.3007\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3019 - val_loss: 1.3007\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3016 - val_loss: 1.3011\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3012 - val_loss: 1.3009\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3010 - val_loss: 1.3008\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3009 - val_loss: 1.3006\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3008 - val_loss: 1.3008\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3017 - val_loss: 1.3005\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3008 - val_loss: 1.3005\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3007 - val_loss: 1.3005\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 245 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7722344398498535\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6299731666666668\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4808 - val_loss: 1.6685\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 408us/step - loss: 1.3518 - val_loss: 1.3313\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3294 - val_loss: 1.3456\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3202 - val_loss: 1.3683\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3168 - val_loss: 1.3399\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3135 - val_loss: 1.3061\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3104 - val_loss: 1.3058\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3089 - val_loss: 1.3057\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3117 - val_loss: 1.3107\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3112 - val_loss: 1.3066\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3085 - val_loss: 1.3060\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3077 - val_loss: 1.3059\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3075 - val_loss: 1.3059\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3082 - val_loss: 1.3075\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3082 - val_loss: 1.3064\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3074 - val_loss: 1.3061\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3070 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3071 - val_loss: 1.3060\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3068 - val_loss: 1.3060\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3066 - val_loss: 1.3059\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3063 - val_loss: 1.3058\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3065 - val_loss: 1.3068\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3064 - val_loss: 1.3071\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3062 - val_loss: 1.3067\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3058\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3085\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3088\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3091\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3094\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3082\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3072\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3092\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3135\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3058 - val_loss: 1.3143\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3126\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3057 - val_loss: 1.3083\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3075\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3057 - val_loss: 1.3080\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3090\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3083\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3071\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3076\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3095\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3106\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3097\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3096\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3097\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3101\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3095\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3083\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3080\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3080\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3071\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1460 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7870450019836426\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6130696666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.5117 - val_loss: 1.5487\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3628 - val_loss: 1.4895\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3330 - val_loss: 1.3170\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3228 - val_loss: 1.3130\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3176 - val_loss: 1.3109\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3150 - val_loss: 1.3110\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3141 - val_loss: 1.3114\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3127 - val_loss: 1.3105\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3121 - val_loss: 1.3120\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3114 - val_loss: 1.3105\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3107 - val_loss: 1.3104\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3103 - val_loss: 1.3107\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3102 - val_loss: 1.3101\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3101 - val_loss: 1.3109\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3109 - val_loss: 1.3106\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3103 - val_loss: 1.3101\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3101 - val_loss: 1.3099\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3100 - val_loss: 1.3098\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3099 - val_loss: 1.3098\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3099 - val_loss: 1.3098\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3099 - val_loss: 1.3099\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3101 - val_loss: 1.3104\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3156 - val_loss: 1.3192\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3104 - val_loss: 1.3120\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3102 - val_loss: 1.3103\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3100 - val_loss: 1.3100\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3100 - val_loss: 1.3100\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3099 - val_loss: 1.3099\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3097 - val_loss: 1.3104\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3097 - val_loss: 1.3113\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3117\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3103\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3105\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3109\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3114\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3113\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3112\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3117\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3117\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3128\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3126\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3128\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3125\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3125\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3126\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3123\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3123\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3142\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3253\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3211\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3174\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3168\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3147\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3136\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3128\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3123\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3117\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3114\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3119\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3117\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3117\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3114\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3114\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3114\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3113\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3105\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3108\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3112\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3112\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3114\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3113\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3117\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3107\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3107\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3108\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3112\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3114\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3113\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3116\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3122\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3124\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3122\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3122\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3120\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3122\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3123\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3124\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3101\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3102\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3107\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3115\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3119\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3125\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3122\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3125\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3119\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3123\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3118\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 8064 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8067736625671387\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6675996666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.5062 - val_loss: 1.4518\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3593 - val_loss: 1.3275\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3366 - val_loss: 1.3621\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3235 - val_loss: 1.3218\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3167 - val_loss: 1.3089\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3144 - val_loss: 1.3070\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3105 - val_loss: 1.3074\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3090 - val_loss: 1.3126\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3081 - val_loss: 1.3080\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3072 - val_loss: 1.3064\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3070 - val_loss: 1.3061\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3100 - val_loss: 1.3075\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3066 - val_loss: 1.3089\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3062 - val_loss: 1.3073\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3070\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3062\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3065\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3066\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3083\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3081\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3070\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3072\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3072\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3071\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3068 - val_loss: 1.3562\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3142\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3086\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3110\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3102\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3088\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3075\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3080\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3095\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3092\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3112\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3094\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3104\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3092\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3117\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3090\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3101\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 209 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7823607921600342\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.652037\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.4893 - val_loss: 1.4723\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 406us/step - loss: 1.3548 - val_loss: 1.4057\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3249 - val_loss: 1.3378\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3183 - val_loss: 1.3097\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3120 - val_loss: 1.3066\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3094 - val_loss: 1.3070\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3113 - val_loss: 1.3576\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3091 - val_loss: 1.3097\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3071 - val_loss: 1.3063\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3064 - val_loss: 1.3059\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3058\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3056\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3066 - val_loss: 1.3081\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3135 - val_loss: 1.3309\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3084 - val_loss: 1.3227\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3078 - val_loss: 1.3172\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3068 - val_loss: 1.3073\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3058\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3275\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3123\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3057 - val_loss: 1.3085\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3059\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1887 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8662159442901611\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6372869999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.5307 - val_loss: 1.6183\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 446us/step - loss: 1.3637 - val_loss: 1.6707\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 427us/step - loss: 1.3303 - val_loss: 1.6798\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 399us/step - loss: 1.3204 - val_loss: 1.3825\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3182 - val_loss: 1.3218\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3147 - val_loss: 1.3182\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3122 - val_loss: 1.3087\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3063\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3084 - val_loss: 1.3065\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3083 - val_loss: 1.3091\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3077 - val_loss: 1.3067\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3072 - val_loss: 1.3065\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3079 - val_loss: 1.3147\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3072 - val_loss: 1.3084\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3066 - val_loss: 1.3071\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3066\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3070\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3068\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3088\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3072\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3066\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3057 - val_loss: 1.3063\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3057 - val_loss: 1.3066\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 378 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7770562171936035\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6087178333333334\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.7040363333333333, 0.6602281666666666, 0.6262536666666667, 0.5972505, 0.6299731666666668, 0.6130696666666666, 0.6675996666666666, 0.652037, 0.6372869999999999, 0.6087178333333334]\n","AUROC ===== 0.6396453 +/- 0.03040295964629829\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzt3XucXHV9//HXOWdm75tkk2y4S0Tg\nI5p6o6g0QtFSBIsPK2Jb7zek1ku12movaG1//n60WgQEbUUr1tp6aeXWoqjcUVQQFUHgi5BAIAnJ\nJtnd7HV25pzv749zZi/J7mbZzGSGPe+nj8jMuX5mdvd8zvdyvt/Ae4+IiORP2OgARESkMZQARERy\nSglARCSnlABERHJKCUBEJKeUAEREckoJQGQBzOyLZvbxfWzzVjO7fqHLRRpNCUBEJKcKjQ5ApNbM\nbC3wI+BC4B1AALwZ+CjwPOC7zrm3Z9u+Fvhb0r+FLcA7nXMPm9kq4GvAMcB9wCjweLbPs4B/Bg4B\nSsDbnHM/XWBsK4F/AZ4LxMC/Oef+MVv3CeC1WbyPA290zm2Za/livx+RKpUAZKlaDTzhnDPgl8A3\ngLcAzwFeb2bPMLOnAV8Aft8590zgWuDz2f4fAfqcc08H3gO8HMDMQuAq4CvOuWOBdwFXm9lCb6b+\nH9CfxfUS4N1m9hIzezbwB8C67LhXAqfOtXzxX4vIFCUAWaoKwH9lr+8B7nTO7XDO7QS2AocCvwvc\n5Jx7KNvui8BLs4v5ycA3AZxzjwC3ZNs8E1gDfClb90OgD/itBcb1e8Dnsn13AVcApwEDQC/wBjPr\ncc5d4pz7yjzLRfabEoAsVbFzbqz6Ghievg6ISC+s/dWFzrlB0mqW1cBKYHDaPtXtVgAdwP1m9oCZ\nPUCaEFYtMK4Z58xer3HObQbOIq3q2WRm15rZEXMtX+C5ROalNgDJs23AidU3ZtYDJMAO0gvz8mnb\n9gIbSNsJdmdVRjOY2VsXeM5VwKbs/apsGc65m4CbzKwT+CfgH4A3zLV8wZ9SZA4qAUiefR842cyO\nyt6/C/iec65C2oj8agAzewZpfT3Ao8DjZnZ2tm61mX0tuzgvxP8C51b3Jb27v9bMTjOzz5pZ6Jwb\nAe4G/FzL9/eDi4ASgOSYc+5x4BzSRtwHSOv9/zhbfT5wpJltBC4hravHOeeBPwLem+1zK3BDdnFe\niPOAnmn7/oNz7o7sdQfwoJn9CvhD4GPzLBfZb4HmAxARySeVAEREckoJQEQkp5QARERySglARCSn\nnjLPAfT1DS26tbqnp4P+/tFahlNzirE2FGNtKMb91yzx9fZ2B3Otq2sCMLNPAidl5znfOXfFtHVt\npOOuPNs595v1jKNQiOp5+JpQjLWhGGtDMe6/Zo8P6lgFZGYvJR3A6kTgdOCiPTb5FPCLep1fRETm\nV882gFtJxy+BdECrTjObnhL/mnRkQxERaYAD8iCYmZ0LnOSce9Mey9cC/72QKqBKJfZPhSKViEiT\naUwbAICZvYp0Uo7T9uc4+9OY0tvbTV/f0P6cvu4UY20oxtpQjPuvWeLr7e2ec129G4FfDvwNcHo2\n1K6IiDSJuiUAM1tO2tB7ajbxhYiINJF6lgD+kHRijW+aWXXZjcA9zrkrzey/gCMAM7Obgcucc/9Z\nx3hERGSauiUA59xlwGXzrH/tXOtqaaBU5tYHNvPiFV20RHrwWUSkaslfEe8bGOG6Ddt4eKjxT+SJ\niADcfPMNC9ru4osvYMuWzXWLY8kngGKY9oAaryQNjkREBLZu3cL11393Qdu+//0f4tBDD6tbLE+Z\nsYAWqzWr9hmPlQBEpPE+/el/5P77f8VJJ53AaaedwdatW7joos9x/vl/T1/fdsbGxnj7289l/fqT\neO97z+WDH/wwN910AyMjw2za9CibNz/On/7phzjxxPX7HcuSTwBtWQIoKQGIyB6+eeND3PnA9poe\n84RnruEPXnb0nOtf97o3ccUV3+TpT38GmzY9wuc+90X6+3fxwhe+mDPOOJPNmx/nox/9S9avP2nG\nftu3b+Of/ukz/PjHt3P11d9SAliI1lAlABFpTscd92wAuruXcf/9v+Kaa64gCEJ27977sannPOd5\nAKxZs4bh4eGanH/pJwCVAERkDn/wsqPnvVuvt2KxCMD3v38du3fv5rOf/SK7d+/mnHPetNe2UTQ1\nFE6thvBZ8o3AqgISkWYShiFxHM9YNjAwwCGHHEoYhtxyy42Uy+UDE8sBOUsDVRPAeKIEICKNd+SR\nT8e5BxgZmarGOeWUl3H77bfx/vf/Ce3t7axZs4bLL/9C3WM5IKOB1sJiZwRLvOe8nz7E2u52zn3m\n4bUOq2aaZeCo+SjG2lCMtdHsMTZLfPPNCLbkSwBhENAahaoCEhHZw5JPAADtxUgJQERkD/lIAIVI\n3UBFRPaQiwTQVlAVkIjInnKRANoLEbH3lNUTSERkUi4SQFs2l7BKASIiU3KRANqVAESkiSx0OOiq\nX/ziZ/T3135ixbomADP7pJn9yMzuNLOz9lh3qpndka3/aD3jqCYANQSLSKM9meGgq6699pq6JIB6\nzgn8UmCdc+5EM1sF/By4YtomnwFeDmwGbjGzbznn7qtHLG0FDQchIs2hOhz0l750GRs2PMTQ0BBx\nHPOBD/wFRx99DF/96pe55ZabCMOQ9etP4rjjnsVtt93Mxo0b+MQnPsnBBx9cs1jqORjcrcAd2esB\noNPMIudcbGZHAbucc48BmNm3gd8B6pIAVAIQkdlc8dD/8vPt99T0mM9f8xucdfSZc66vDgcdhiEv\netFv8cpX/j4bN27g4ov/iYsu+hxf//pXueqq64iiiKuu+hYnnPBijj76WD74wQ/X9OIP9Z0TOAZG\nsrfvAL6dLQM4GOibtvl24BnzHa+np4NCIZpvkzm1j5cAaOlsobe3e1HHOBCaObYqxVgbirE29jfG\njs0tROGcIyUs7pjtU9eZ2eJbsaKD1tYizv2KXbt2cdNN3wOgVBqjt7eb008/nQ9/+E8588wzef3r\nX0tXVxctLQV6ejpr/jOp+3DQZvYq0gRw2jyb7fMn0N+/+Dl9q72A+vpH6WtpWfRx6qlZxg2Zj2Ks\nDcVYG7WI8fTDTuP0w+a7NC1OX9/QnPENDIxSKpXxPuB97/sQ69Y9Z8Z+733vn/Poo49w443f53Wv\nez2XXfZvTExU6O8fWdTnnS9p1LsR+OXA3wBnOOemz3CwhbQUUHVYtqwu2tUGICJNojoc9LOetY5b\nb70ZgI0bN/D1r3+V4eFhLr/8Cxx55Fre9rZ30t29nNHRkVmHkK6FejYCLwc+BZzqnJvRfO2ce8TM\nlpnZWuBx4EzgDfWKRc8BiEizqA4Hfcghh7Jt2xO8+93nkCQJH/jAn9PV1cXAQD/vfOebaW/vYN26\n57Bs2XKe97wXcN55H+H88y/gqKPmrS1/UupZBfSHwGrgm2ZWXXYjcI9z7krgT4CvZcu/4Zx7sF6B\nqBFYRJpFT08PV1xx7Zzr/+zPPrzXsre//Vze/vZzax5LPRuBLwMum2f9rcCJ9Tr/dHoQTERkb7l4\nErhaBaRZwUREpuQkAagRWERkT7lIAGEQ0BIGSgAiItPkIgEAtEWaFEZEZLrcJIDWKFQCEBGZJjcJ\noE0Tw4uIzJCbBNAahcTeU1FPIBERIGcJAPQwmIhIVW4SQFukrqAiItPlLgGoBCAikspNAmhVCUBE\nZAYlABGRnMpNAihms/5MJL7BkYiINIfcJIAwm3TMowQgIgJ5SgDZpJMqAIiIpHKTAIIgKwF4ZQAR\nEchRAqh+UDUBi4ik6jklJGa2DrgauNA5d+ke614FnAeUgK/vub7WwqwEkKgEICIC1LEEYGadwCXA\nDbOsC4FLgVcAJwOvNLPD6xULQKA2ABGRGepZBVQivcBvmWXdamDAOdfnnEtIk8SpdYxlsheQSgAi\nIql6TgpfASpmNtvqPqDbzI4BHgFeCtw83/F6ejooZHP7LkbPinYAOjpb6e3tXvRx6qlZ45pOMdaG\nYqyNZo+x2eOraxvAXJxz3szeAnwJGAQ2QnaLPof+/tFFn6+3t5uh3WMADA2X6OsbWvSx6qW3t7sp\n45pOMdaGYqyNZo+xWeKbLwk1JAEAOOduAU4CMLPzSUsCdRPoQTARkRkalgDM7DvAW4AR4JXABfU8\nX/VBsFjXfxERoI4JwMyOJ72orwXKZnY2cA2w0Tl3JfAF4HuAB853zu2oVyww1Q1UD4KJiKTq2Qh8\nF3DKPOuvAK6o1/n3VG1g0INgIiKp3DwJHOlBMBGRGXKTAKoPgun6LyKSyk0C0INgIiIz5ScBVEsA\njQ1DRKRp5CYBVIeDjlUCEBEBcpQAQrUBiIjMkJsEUH0SOFElkIgIkKMEEGk4aBGRGXKTADQlpIjI\nTLlJAJoSUkRkpvwkAD0JLCIyQ24SgKaEFBGZKTcJINR8ACIiM+QnAagEICIyQ44SgNoARESmy00C\nqM4HoMu/iEiqrlNCmtk64GrgQufcpXusew/wRiAGfuqc+0A9Y1EVkIjITHUrAZhZJ3AJcMMs65YB\nfwGc5Jx7CfAsM3txPeIYq4zxg0fvwPv0CQBVAYmIpOpZBVQCXgFsmWXdRPavy8wKQAewqx5B/OSJ\nn/GZH1/OwwMbAT0IJiJSVc85gStAxcxmWzduZn8HbADGgK875x6c73g9PR0UCtGTjqN9VxGAtu6I\ngJhCIaS3t/tJH+dAaNa4plOMtaEYa6PZY2z2+OraBjCXrAror4Fjgd3AjWb2XOfc3XPt098/uqhz\njY2UAdg1MEwYtFOaiOnrG1rUseqpt7e7KeOaTjHWhmKsjWaPsVnimy8JNaoX0HHABufcDufcBHAb\ncHw9ThSF6UdMfEIYBGoDEBHJNCoBPAIcZ2bt2fvfBH5djxOFwVQCCFA3UBGRqrpVAZnZ8cAFwFqg\nbGZnA9cAG51zV5rZp4CbzKwC3O6cu60ecYRB2m4QqwQgIjJDPRuB7wJOmWf954HP1+v8VVG1BJDE\nhIGeAxARqVryTwJXE0DsEwICTQkpIpJZ8gmgWgWUNgJrUngRkaolnwC2jjwBwODE7rQNQCUAEREg\nBwlg1/hA+t8tY3Td+DjsnmhwRCIizWHJJ4AoqwKq9EeEpZhQCUBEBMhBAiiEaUenpJJW/Xh1AxIR\nAXKRALJuoHH63qsVWEQEyEMCCNISgK+k772GAxURAfKQALIqIB9rRhgRkemWfAKIJhNA+l5VQCIi\nqSWfAIph2guomgA0I4yISGrJJ4BqGwBZFZB6AYmIpJZ+AohmJgC8VzWQiAg5SADFIJ0SkiT9qIH3\nGgxCRIQcJIBqL6DJEkCijkAiIpCDBFCsVgHNKAEoA4iIPOkEYGatZnZEPYKph2JWAgji7KN6lQBE\nRGCBM4KZ2V8Bw8C/Aj8Fhszse865j+5jv3XA1cCFzrlLpy0/DPiPaZseBfylc+4/n2T8+1QM0zaA\noFoCSLymhRQRYeFTQr4SWA+8Gfgf59xHzOzG+XYws07gEuCGPdc55zaTTRdpZgXgZtL5gmuuGOxd\nAtDlX0Rk4VVAZeecB84ArsqWRfvYpwS8Atiyj+3eCnzLOTe8wFielEJUBA+Bn2oDUAlARGThJYAB\nM7sWONw59yMzO5N9PFPrnKsAFTPb17HPAU7b10Y9PR0UCvvKOXvzHSXCZNp+HnpWdrKireVJH6ve\nenu7Gx3CPinG2lCMtdHsMTZ7fAtNAK8Hfhf4YfZ+HHjL/p7czE4EHnDO7d7Xtv39o4s6x+7SBMG0\nBBAknr4dw5Rbi4s6Xr309nbT1zfU6DDmpRhrQzHWRrPH2CzxzZeEFloF1Av0Oef6zOydwOuAzhrE\ndiZwfQ2OM6coDAmTqY8ZqA1ARARYeAK4HJgws+eTVtl8C/hMDc5/AnB3DY4zpyiIZpQAUBuAiAiw\n8ATgnXN3Aq8GLnXOfRsI5tvBzI43s5tJG3nfb2Y3m9kHzezV0zY7BNj+5MNeuCiIZrQBTBQH+doD\n/8nOsV31PK2ISNNbaBtAl5mdAJwN/LaZtQI98+3gnLuLrKvnPNv8xgLPv2hREE4mgImWMbYc+iPi\nXeM80H8c69tfVO/Ti4g0rYWWAC4AvgB83jnXB3wcqPlDW/UQBiFBEhFHZR499k7iwjgAleokwSIi\nObWgEoBz7hvAN8xspZn1AH+dPRfQ9B4bGSfqPYVdQ/dT6himpdTNROsQlaTS6NBERBpqQSUAM1tv\nZg8DDwC/Bu43s9+sa2Q18ujwOEFrD2PL0m6kywbSYYyUAEQk7xZaBXQ+8Crn3Brn3GrSbqCfrl9Y\ntVMI0rbqiWL6oHHreNontuJVBSQi+bbQBBA75+6tvnHO/Rx4StxCR1kCKBeHKJbaCeO01kslABHJ\nu4X2AkrM7DXA97P3pwNPiVvoKAzwfoK4ME77yOrJh8KUAEQk7xZaAngX8E7gEWAj6TAQf1ynmGoq\nCiBOBgFoGeucHBVUvYBEJO/mLQGY2W1MjZwQAL/KXi8DvgycXLfIaiQKApIsAbSOdxImaZWQSgAi\nknf7qgI674BEUUeFICBJBgBoHe+anBim4pUARCTf5k0AzrlbDlQg9RKFUyWAtnI3cTaItUoAIpJ3\nS35S+CgIiJMBAl+glY5pjcBqAxCRfFvyCSDEkySDFOIuopZoqg1AVUAiknNLPgGMlIeAmEK8jKgY\nEqoXkIgIkIMEMFDaCUAhWUZYCAkICQjUBiAiubfkE8DuiX4AIt89+WnDIFICEJHcW/IJwFYcQ0tx\nHS3Joel8kEAUFJQARCT3FjoUxKKY2TrgauBC59yle6w7Avga0AL8zDn3rnrEsKp9Je1tJxKEuyFM\nL/phEKkRWERyr24lADPrBC4BbphjkwuAC5xzLwRiM3taPeKIwrTXj4+YLAGEhGoEFpHcq2cJoAS8\nAvjInivMLAROIh1WGufce+oVRHU4aB8yOYux2gBEROqYAJxzFaBiZrOt7gWGgAvN7AXAbc65v5rv\neD09HRQK0XybzGq8nF7ofQCFljQDFMICZSbo7e1+0serp2aLZzaKsTYUY200e4zNHl9d2wDmEQCH\nAReTjjB6rZn9nnPu2rl26O8fXdSJJsppVY8PAypxVu3jQ8pJhb6+oUUdsx56e7ubKp7ZKMbaUIy1\n0ewxNkt88yWhRvUC2gE86px72DkXk7YTPLseJ0oq1QTAtDYAVQGJiDQkAWTVQxvM7Jhs0fGAq8e5\n4oqHxOPDAB+mI8EFRCQ+IfFJPU4pIvKUULcqIDM7nrSnz1qgbGZnA9cAG51zVwIfAL6cNQjfA/xP\nPeKoVGKCxKeVTtVGYKaGg2iJlvyjECIis6pnI/BdwCnzrH8IeEm9zl9VKScEPm0D8MFUCQDSIaFb\nomK9QxARaUpL/va3Ukkg8RAEk20AAZoURkRk6SeAcloF5IOpEkDop0oAIiJ5tfQTQCWtAiIMp1UB\nVdsAlABEJL+WfgIox5NVQJMJwGtOABGRHCSAJOsFFOKrbQDVKiC1AYhIji39BFCtAkIlABGR6ZZ8\nAihNTFBJtpEQkATpBX8qAagEICL5teQTwM/G76O/cB1J8gRJUH3yVwlARGTJJ4BiSxsAcbKLhKwN\nIFECEBFZ8glgVdtKAJJkaGosoGoVkFcbgIjk19JPAO3VBLCbOBsLKNCDYCIiSz8BJOUiEw8/l7g8\nMtkGoCogEZEcJIDHtw8T7zyEymCRuNoI7NOigBKAiOTZkk8AHe3pgKc+DigHE+nCRG0AIiJLPgF0\ntbekL5ICE4wDeg5ARARykAC6O9Lx/n0cUQ6yBKA2ABGRpZ8ABoazap+4QIVS+jqptgGoCkhE8qtu\nM4IBmNk64GrgQufcpXusewR4DKhehd/gnNtc6xg23/NzoBPiiHKWAFQCEBGp75zAncAlwA3zbHaG\nc264XjEArNq6EYJ1dA6HxNUSQKwZwURE6lkFVAJeAWyp4zn2qS1LcUGlSIW0OihI1A1URKSek8JX\ngIqZzbfZv5jZWuAHwF855/xcG/b0dFAoRE86jqilAGNApYinQhxWCGnJ1gX09nY/6WPWSzPFMhfF\nWBuKsTaaPcZmj6+ubQD78DHgOmAXcBXwGuC/59q4v390USeJg7SQ45P0o060jVKotAIwMjpOX9/Q\noo5ba7293U0Ty1wUY20oxtpo9hibJb75klDDEoBz7ivV12b2beA3mCcBLFbP4Z7CQzEBaemh3DpK\nMUnHB9KDYCKSZw3pBmpmy83su2aWPaXFbwP31uNcxe4WWqKEJEhzXRxVphqB1QYgIjlWz15AxwMX\nAGuBspmdDVwDbHTOXZnd9f/YzMaAn1OHu3+AFiZoKcRMlNKLfhJW1AgsIkJ9G4HvAk6ZZ/3FwMX1\nOn9V0Y/SEsWM+hYiIIliKENAoAQgIrm25J8E7m8/lJGoi7KP8B6SMIYkoBBGehJYRHJtySeAna1r\nKEcteELwYdoG4D2FsKAHwUQk15Z8AugpFgmi7GPGUVoFlHgKQUFVQCKSa0s+AVSGI4IobfT1SSFN\nAJ60BKAEICI5tuQTQKtvJ5xRAqikJQC1AYhIzi35BBBFHRSibCrISpQ2AqsNQERk6SeAgUFojdIL\nfTRRyEoAUAgiVQGJSK4t+QTQ2l6kNUwv9MVy28wSgKqARCTHlnwCaO8o0pYlgLDShZ+WAGIfk/ik\nwRGKiDTGkk8AHV0ttFEGIPQdJFGFIEl7AQHEKgWISE4t/QTQ2UqbTxNAQBtJWH0QLB0dVA3BIpJX\nSz4BdHa30JY19npa8YHH+4RCNjqo2gFEJK+WfAJo72yhLUlLAIlPR59OgvJkFZB6AolIXi35BNDa\nWqBYSe/yK74IgA9joiCrAlIJQERyaskngCAICLNrfOzTi34SVoiqJQC1AYhITi35BAAQZj094yQE\nQpIoJqJaAlACEJF8qmsCMLN1Zvawmb13nm3ON7Ob6xlHmM0AlsSeMOhMZwVD00KKSL7VLQGYWSdw\nCXDDPNs8Czi5XjFUJT4iCj0+TgjCLpUARESobwmgBLwC2DLPNhcAf1PHGADwFCkUEnzsCYMukjCe\nVgJQI7CI5FM95wSuABUzm3W9mb0VuAV4ZCHH6+npoFCIFhdM2EZLlFCOPWHYRRKV6exoB6Cju0hv\nb/fijltjzRLHfBRjbSjG2mj2GJs9vrolgPmY2UrgbcCpwGEL2ae/f3TR5wujdloLIwyPJoRBN0nY\nR2k0vfPfOTBEX8vQoo9dK7293fT1NT6O+SjG2lCMtdHsMTZLfPMloUb1AnoZ0AvcBlwJvMDMLqzX\nyVraOmkLy/jYQ5CNB+TVBiAi+daQEoBz7r+B/wYws7XAl51zf1av87V3dtIymPYFDekmCWNCn+Y+\nDQYnInlVtwRgZseTNvKuBcpmdjZwDbDROXdlvc47m46udgpBmgCCpJ04qlAt/JRVAhCRnKpnI/Bd\nwCkL2O6RhWy3Pzo6ixRCD4BPIpJiRGuQNgIPlYfreWoRkaaViyeBO7taKQRZAqgk+JYWusLlAOwc\n29XI0EREGiYXCaBrWStR+jAwPvYkLa10hJ0EBPSN7WxscCIiDZKLBNDd3UpWA4SPEyi08VgphqCT\nJ0Z2NDY4EZEGyUcCWNFO9RGypOJpHz+YDUOjhOEyRivDTMTlhsYnItIIuUgAnV2tdBWyOQF2TxAF\nyxh5fIgwTB+Q2D46VQ3kvedH2wYYKCkp7A/vPSNDpUaHISLzyEUCKLZE9BTLhEHCxMA4lbaI4lCZ\ntihtCL6v/4nJbR8ZHud/NvVx89b+RoW7JNz3iy38++d+xI5tjX8SUkRml4sEEAQBpbiNQ5aNUB4q\nUy4GFIcmOH5NOgrFrwe2TW67fWwCgG1junvdH1sfG8R72LJpsNGhiMgccpEAALbvXsHTVgyCh9JY\nTFRKWLfsIAA2T2sI7htPE8D2sQm89w2JdSno35GO3aQSgEjzyk0CGKfA6ta0Xr88UCJui4iGWgEY\nKQ/Qn9X592UlgLE4YbiiYSIWI0k8/buyBLBdD9qJNKvcJAAK0FZOH3yeGChRaYsY7qtQDFtIkiF+\nPZhesKolAIBtYxOzHkrmNzQ4RlxJh97o3zE6+VpEmktuEoAvBAwNLmd5Z5ny4AQTnQV2bhtmZdtK\nkmSIDbtHmYgTBiYqZM+MTbYHyJOzK6v+CcOAJPHs2jHS4IhEZDa5SQBBAfr7l3HkigF84tndFrB9\n8yAHdawCymwY6p+8+z+yqw2A7TVqCL6rb5BrN/URJ/loU+jPLvhHPL0HgB3bVA0k0oxykwCKYZnE\nhxzSmfZK2bltnF3DE/QU04vUQKmfR4fGADiup4uAmVVApXiCf7jjIr6zcc4pjmdVSRKufWwHP9w2\nwDc2PEFcw4blXaUy/3zfYzySxd0sqg3Axzw7bWRvVALoHx9gy/AT+95QJKdykwC6lseMdO0i6d7J\nIQeXKe+e4JGWgJEH04fAkmSAh7ML6SHtraxqK87oCfSzbXfz2PAWrt90M+OVhZcMHhwcZTxOKIYB\n9/YP842Hn6AU16ZO/MfbBnhsZJxrHt1O0kQ9lnbtGCEqhKw9ejVBADu2H/ieQEMTw3zqp5fwf+/4\nNJfd8xWeGNm2751EciY3CeCYFUez6di7uDPazQuf+RCF7iL94zE/35RW94yN3co9264lSUbobW9h\nTVvLjJ5At2+9A4DxuMRd23+x4PPevSu9+L312MM4squNe/uHueRXmyZLG4sVez957CfGJrj0l9/j\ngV2/3ud+jw1tYXiifnXy3nsGdo7Ss7KDYkvEilUd7Nw+ckC71Hrv+Y8H/ovBiSFWtfVwd9+9fOIn\nn+bzv/w37u67l0d2b2KwpO6pIg2ZEawROlvbWLXx6Ww//EGuH93E6uecxNCDg5R2riZ56DkUDt3A\nSPBrgrHtXPuDIpWVXQBsGhxhResoGwYf5Yjuw3h8aAs/2PwT1h/6IiC92Fy/6Ud0FNtZf+jzZ5yz\nFCc8MDDC6taIwwrDvMMO4/rNu7jtiX6+6B7nHDucI7vbF/V5Ht49ylA55uhlrTw0OMbm8dVc+ovL\neeMzX8OLDz1+1n1uffx2vvHgVaxuW8lfnPA+uoqde22zc3yCTcPj7C5XKAQBT+tq55COVgphMMsR\n9zY0OE6lktDT2wFA70Hd9O+FRG+qAAAOgUlEQVQYZbB/jBUrOxZ0DO89FR9TScpUknhy2s4wiIiC\nkDAICYOAMAgJyP4bBAQEbB3azrc33MI9O+7n2J6jed/zzuGeHffzvUdv4pc7fsUvd/xq8jwHdazh\n6BVrWdW2kp62Faxs62FF6zLaCm20Ra0Uwtz8eeRS4hMSnxAFEUGwsN/vJ8t7T+IT4uxcxbBAFEb7\n3vEACZ4qDzv19Q0tOtDe3m4eenAb3/zqHfziuO9C4OlofynF4lG82l9H/66A2x9cw9be7RQO2kQ8\nsJr2zetIDjmIZaPDDLX+lImOUcKJNsK4SIynp7WTI/oH2bxqnJ2d6d1kBytYHvdSISTwEWHbIYy1\nH8Xzg3t5YXgPmypreKhyJCNhN1vCNUQkrGUrBV+h2BpQmQim3SkH4MEDAR4PeA9B1kdpS7iS4bCL\nyvDNxC29tLY8m3L5McYnfsoyX2BZsIZi0EYYREBAf2mQ7aM7CQlJqNBdbOO4VcdMXjwn4oTNoyPs\nrJYOfBEoAp5CWKKrbYhisUSpUiIMIlYUVtEVdVNKypSTCp4KMWXY0UL3fU9j5MitjB2xg9YtK+ne\ncBjjy4bYeuyD+KgCQQJ4gnJER38P7buXk4QVyi3jlNqGGWsfoNQ6yuQQrgBJQKHSQqHSQlRuSReF\nMW3jXXQMrWSibYQdB2/AZ/u0hu28+qC30Bml4z1579k69DhPbN9GebTCWGWMXYWtxMQs6z+IYqmd\n4eV9DK58gon29Dtonehg2dBBlFYNUCjunQzSn8rkm9le7vUO0t5RcbJ3NWBAQBAEhIQEpD+X9Hdg\n+jGmv6u+9pP7F4IiYRClFzfiyd+nrkI3ndEyPJ7Ex8TZv4TK1Gsf47P/RVFIHCdTZ/Pp/yV4EmIS\nn+B9Alm8rWEbnVE3LWHrVFQ+3Tb2FSrZv+o5ZsTvp15PxVchphpnhcTPUm0azPr1zm7a9b0aV1VH\n2Mnh7UexvNCTxVkm9hXKyQTjyTilZIxSMk7sKzMO6Emo+DKJjwmDiIAQT5J99+n3kzD7z7kQFAiD\nAkn2vaQ/+5CQMLuxSX8LgiAkCiLOPOo01h/+mwv8sDP19nbPmd3qmgDMbB1wNXChc+7SPda9E3gH\nEAN3A+9xzs0ZzP4mgL6+IUZHJvj07d9ka/RLVg6/kPiQ5xIkCUf4rbQWJrhv+FDi0rWUijtJSu0k\nA734cguEMdHyHYSdQ/gkJBjqISx1ELRMEK/YBuPtkBSgYwjiAlGpk9C3E4dFfBCxKhgk8p5i6AmA\nShAzQjvDwTJCP0oc9eHDClHcQrHSQZgUiHxEkESESQS+QOBDfJCQBB4qESPFNUTErBzZig9Chrp7\nmSi2AJ4giYl8hSCpECWV7CtOZlwsICHwBaKkSODDbHGSXZwTCNI/RzwEgU8vJMG0IwQ++/tLU1S6\nPCDwRfABQThK5MvEASS0E/giPogJSMCHBL4ARJMJbabqX3YCPsjeB3ts67McmW0H+CDBB2XwYfov\nCSYPF2R/WNM+AFPpNCDw1QkjAghiICZK2rP3nqQwBkGy4OtN9USB9+x9lcrOWo3DT18cZD+DWb6T\nuU5ePU4wX3TzrJv35tfvsef0K+nU95vGPPeB9j77fCfd47P6ebad8869+vNcwGnDBKb9Hk/9/k3f\nOcBP2znwSXpHllSXzkzFM481R3iB3/uzzbJ5EHhC38rHznrD7Mfah/kSQD3nBO4ELgH26jZjZh3A\nHwEnOefKZnYjcCJwe73iAejobOGc3zqLz9+7nNHx3ax8cICRwzrZ1HkYeCh2wlG7ns+W5F4GV2wn\nPmjT5L7eByTDqyEqESzfSULWeDzeQfmhF+ErLUQHOaIVffj23QTB1Bg4s884MAz0kQBJqR0/vhzf\nPkzcOrDAT/MwMfBEd/X9ozPWaixTkaWjdfeRdTluPSs5S8ArgI/sucI5Nwr8Dkwmg+XAAemvd3Bn\nB3/7oldSThIe2LiLO657kJE4vbOrtEeMhMtYFq2newzGW3dR3D1GUAxpj7optEQERU/MKBOFCnHg\naSt202qeQlImiI8krjyN0qAnLo/TsnuE0mhIxYMPPHHg8YRpacEHBFFCEAUEYSdxMSIIIIli4rCC\nD2OSsIIP4vTuKvAESQhBCIUyPirRXh5Nb9J9SOxDIu8J8GnNiQdPSBwU8EFI2t4fpDdMPr0jjsOY\nSljBZ3dvPrtz8dnddvVuxnsgmHYHPu2uJcjeT5YB0o3T6osgAO8JkuqdUVpE9kFCEMy8w5pRrRFM\nv0OuLtv7ZxlUSwfZHbCf/Ax+lhtAn30in24zWZrx1YNNbu8DTwIEWVWVD/yMuz+mHXNuwbxrZX6z\nfXtTd9dzlkkOmKlYZpYdamHPTxgARw211fAMU+o5KXwFqJjZnNuY2V8C7wcucs5tmO94PT0dFAqL\nbzzp7e3ea9mhBy1n/fMO52c/2cTIUImJUszERIWJUoWkElMeCUmK6euoElMoxfg4Jo4LJHFaR5ok\n42lJENKfWhgRFot42vHlbnyc1jWm1ZzTK4r95G16Wg1XLfYHpHXvxUV+0oVVjM5+QXuKWFC15UI/\nX+0u07rgN7+n4s8oAE5cf8Ss17D9Pna9G4HN7OPAjj3bAKatbwe+DZznnPvhXMepRRtAM1OMtaEY\na0Mx7r9miW++NoCGPAdgZivN7GQA59wY8B1gfSNiERHJq0Y9CFYEvmxmXdn7FwKuQbGIiORSPXsB\nHQ9cAKwFymZ2NnANsNE5d6WZ/T1wk5lVSLuBXlOvWEREZG/1bAS+CzhlnvVfBr5cr/OLiMj8cjMW\nkIiIzKQEICKSU0oAIiI5pQQgIpJTT5nRQEVEpLZUAhARySklABGRnFICEBHJKSUAEZGcUgIQEckp\nJQARkZxSAhARyal6TgnZFMzsQuDFpJMBvd85d2eDQwLAzD4JnET6MzgfuBP4dyACtgJvcs6VGhdh\nKpuw517g/5DO79xUMZrZG4APAxXgY8AvaaIYsyHPvwL0AK3A35FOf/rPpL+Tv3TO/UmDYlsHXA1c\n6Jy71MyOYJbvLvuOP0A6bd1lzrl/bXCMl5MOKV8G3uice6KZYpy2/OXAdc65IHvfsBjnsqRLAGb2\n28AxzrkTgXcAn2lwSACY2UuBdVlcpwMXAX8PfNY5dxLwEPD2BoY43XnArux1U8VoZquAvwVeApwJ\nvIomixF4K+Cccy8FzgYuJv15v985tx5YbmZnHOigzKwTuIQ0qVft9d1l230MOJV0dN8/M7OVDYzx\nE6QXz98GrgQ+2IQxYmZtwF+RJlIaGeN8lnQCIJ14/ioA59z9QI+ZLWtsSADcCrw2ez0AdJL+UlTn\nRPgf0l+UhjKzZwLPAq7NFp1Cc8V4KnC9c27IObfVOXcuzRfjDmBV9rqHNJk+fVpJtFExloBXAFum\nLTuFvb+7FwF3OucGs9n7fsiBm71vthjfDXwre91H+t02W4wAfw18FpjI3jcyxjkt9QRwMOkvSVVf\ntqyhnHOxc24ke/sO0jmRO6dVVWwHDmlIcDNdAHxw2vtmi3Et0GFm15jZbWb2OzRZjM65rwNPM7OH\nSBP/nwP90zZpSIzOuUp2IZputu9uz7+hAxbvbDE650acc7GZRcB7gP9sthjN7Fjguc65/5q2uGEx\nzmepJ4A9zTk5ciOY2atIE8B791jV8DjN7M3Aj5xzG+fYpOExksawCjiLtKrlcmbG1fAYzeyNwCbn\n3NHAy4Cv7rFJw2Ocw1xxNTze7OL/78CNzrkbZtmk0TFeyMwbp9k0OkZg6SeALcy84z+UrE6u0bIG\nor8BznDODQLDWYMrwGHsXaQ80H4PeJWZ/Rg4B/gozRfjNuD27C7sYWAIGGqyGNcD3wVwzt0NtAOr\np61vhhirZvv57vk31AzxXg782jn3d9n7ponRzA4Dngn8R/a3c4iZ3UITxTjdUk8A3yNteMPMXgBs\ncc4NNTYkMLPlwKeAM51z1QbW64HXZK9fA1zXiNiqnHN/6Jw7wTn3YuCLpL2AmipG0p/vy8wszBqE\nu2i+GB8irf/FzI4kTVL3m9lLsvVn0fgYq2b77n4CnGBmK7IeTeuB2xoUX7UnzYRz7m+nLW6aGJ1z\nm51zz3DOvTj729maNVg3TYzTLfnhoM3sH4CTSbtevSe7C2soMzsX+Djw4LTFbyG90LYBjwJvc86V\nD3x0ezOzjwOPkN7JfoUmitHM/pi0Gg3SHiJ30kQxZn/sXwIOIu3y+1HSbqCfJ70B+4lzbl/VBfWI\n63jSNp61pN0pNwNvIJ2ne8Z3Z2ZnA39B2m31EufcfzQwxjXAOLA72+w+59y7myzGs6o3dmb2iHNu\nbfa6ITHOZ8knABERmd1SrwISEZE5KAGIiOSUEoCISE4pAYiI5JQSgIhITikBiBwAZvZWM9vzKWCR\nhlICEBHJKT0HIDKNmb0P+APSh7YeAD4J/C/wHeC52WZ/5JzbbGa/RzrE72j279xs+YtIh3yeIB39\n882kT9aeRfoA07NIH7Q6yzmnP0BpGJUARDJm9kLg1cDJ2VwNA6RDIh8FXJ6Nk38z8CEz6yB9cvs1\n2Vj/3yF9EhnSAd/emQ0BcAvpuEoAzwbOBY4H1gEvOBCfS2QuS35GMJEn4RTgaOAmM4N0nobDgJ3O\nubuybX5IOqvTscA259zj2fKbgXeZ2WpghXPuXgDn3EWQtgGQjgc/mr3fDKyo/0cSmZsSgMiUEnCN\nc25yeG4zWwv8bNo2AelYLntW3UxfPlfJujLLPiINoyogkSk/BM7IBnDDzN5NOmlHj5k9P9vmJaTz\nDj8IrDGzp2XLTwV+7JzbCewwsxOyY3woO45I01ECEMk4535KOo3fzWb2A9IqoUHSER7famY3kg7j\ne2E2C9Q7gG+Y2c2k04+elx3qTcDF2TjwJ7P3JDAiTUG9gETmkVUB/cA5d3ijYxGpNZUARERySiUA\nEZGcUglARCSnlABERHJKCUBEJKeUAEREckoJQEQkp/4/NJvSWkWp4CsAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7fc24c776400>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"jtvC1Mz-yfPM","colab_type":"text"},"cell_type":"markdown","source":["## Frog Vs All"]},{"metadata":{"id":"RH48U2WpyToN","colab_type":"code","outputId":"a4ce932d-6a35-4649-987c-29307471169b","executionInfo":{"status":"ok","timestamp":1541768792905,"user_tz":-660,"elapsed":3237908,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":64319}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 883us/step - loss: 1.4823 - val_loss: 1.5897\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3469 - val_loss: 1.7401\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3229 - val_loss: 1.4283\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3177 - val_loss: 1.3226\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3239 - val_loss: 1.5132\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3128 - val_loss: 1.3179\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3107 - val_loss: 1.3133\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3092 - val_loss: 1.3067\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3078 - val_loss: 1.3084\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3073 - val_loss: 1.3070\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3069 - val_loss: 1.3074\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3067 - val_loss: 1.3071\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3065 - val_loss: 1.3068\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3064 - val_loss: 1.3067\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3062 - val_loss: 1.3093\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3471\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3149\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3070\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3060\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1465 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7790945768356323\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6310293333333332\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 4s 769us/step - loss: 1.4794 - val_loss: 1.7819\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3490 - val_loss: 1.4140\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3251 - val_loss: 1.3307\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3218 - val_loss: 1.3930\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3181 - val_loss: 1.3470\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3141 - val_loss: 1.3069\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3118 - val_loss: 1.3250\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3119 - val_loss: 1.3102\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3096 - val_loss: 1.3108\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 320us/step - loss: 1.3095 - val_loss: 1.3088\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3108 - val_loss: 1.3332\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3100 - val_loss: 1.3206\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3083 - val_loss: 1.3126\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3078 - val_loss: 1.3125\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3077 - val_loss: 1.3103\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3072 - val_loss: 1.3101\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3076 - val_loss: 1.3058\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3074 - val_loss: 1.3058\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3070 - val_loss: 1.3060\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3066 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3067 - val_loss: 1.3114\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3063 - val_loss: 1.3104\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3091\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3059 - val_loss: 1.3094\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3058 - val_loss: 1.3101\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3058 - val_loss: 1.3071\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3057 - val_loss: 1.3069\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3056 - val_loss: 1.3084\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3111\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3191\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3201\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3191\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3056 - val_loss: 1.3115\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3056 - val_loss: 1.3124\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3146\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3173\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3163\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3155\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3154\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3140\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3141\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3056 - val_loss: 1.3152\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3151\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3056 - val_loss: 1.3145\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3145\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3114\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3072\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3056 - val_loss: 1.3075\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3083\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3094\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3147\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3143\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3149\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3144\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3145\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3154\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3156\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3146\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3135\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3056 - val_loss: 1.3139\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3080\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3081\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3068\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3056 - val_loss: 1.3071\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3070\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3070\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3072\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3070\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3056 - val_loss: 1.3071\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3073\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3077\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3080\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3077\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3081\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3083\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3056 - val_loss: 1.3084\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3056 - val_loss: 1.3085\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3056 - val_loss: 1.3089\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3085\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3087\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3086\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3098\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3102\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3056 - val_loss: 1.3084\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3080\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3056 - val_loss: 1.3081\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3056 - val_loss: 1.3085\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3056 - val_loss: 1.3097\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3086\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3084\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3055 - val_loss: 1.3087\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3055 - val_loss: 1.3093\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3098\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3095\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3084\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3087\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3055 - val_loss: 1.3095\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3095\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3098\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3055 - val_loss: 1.3085\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3089\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3090\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3085\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3055 - val_loss: 1.3082\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3055 - val_loss: 1.3087\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3055 - val_loss: 1.3093\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3087\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3055 - val_loss: 1.3084\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3055 - val_loss: 1.3084\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3055 - val_loss: 1.3087\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3055 - val_loss: 1.3088\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3055 - val_loss: 1.3088\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3055 - val_loss: 1.3089\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3093\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3090\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3092\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3055 - val_loss: 1.3095\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3055 - val_loss: 1.3090\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3087\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3055 - val_loss: 1.3095\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3055 - val_loss: 1.3092\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3055 - val_loss: 1.3077\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3055 - val_loss: 1.3091\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3055 - val_loss: 1.3070\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 5756 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8108022212982178\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7121816666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 867us/step - loss: 1.5060 - val_loss: 1.6153\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3559 - val_loss: 1.6646\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3253 - val_loss: 1.3274\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3166 - val_loss: 1.3059\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3131 - val_loss: 1.3098\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3094 - val_loss: 1.3062\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3095 - val_loss: 1.3140\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3070 - val_loss: 1.3056\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3058 - val_loss: 1.3049\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3050 - val_loss: 1.3044\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3047 - val_loss: 1.3045\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3044 - val_loss: 1.3043\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3042 - val_loss: 1.3039\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3041 - val_loss: 1.3038\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3045 - val_loss: 1.3041\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3041 - val_loss: 1.3038\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3040 - val_loss: 1.3037\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3039 - val_loss: 1.3036\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3041 - val_loss: 1.3035\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3051 - val_loss: 1.3058\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3041 - val_loss: 1.3045\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3040 - val_loss: 1.3038\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3038 - val_loss: 1.3037\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3040 - val_loss: 1.3037\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 319us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 320us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 33 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7842861413955688\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6137783333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 994us/step - loss: 1.5467 - val_loss: 1.5617\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.4082 - val_loss: 1.3322\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3224 - val_loss: 1.3108\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3098 - val_loss: 1.3088\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3080 - val_loss: 1.3080\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3073 - val_loss: 1.3075\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3069 - val_loss: 1.3071\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3067 - val_loss: 1.3069\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3066 - val_loss: 1.3067\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 321us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 323us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 322us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 324us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3060 - val_loss: 1.3060\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 0 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  0.0\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.5793148333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.4904 - val_loss: 1.6433\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3438 - val_loss: 1.7039\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3223 - val_loss: 1.4347\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3161 - val_loss: 1.4391\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3154 - val_loss: 1.3342\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3097 - val_loss: 1.3313\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3088 - val_loss: 1.3180\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3063 - val_loss: 1.3032\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3063 - val_loss: 1.3574\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3045 - val_loss: 1.3070\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3040 - val_loss: 1.3045\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3058 - val_loss: 1.3095\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3038 - val_loss: 1.3046\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3031 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3025 - val_loss: 1.3051\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3025 - val_loss: 1.3035\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3019 - val_loss: 1.3039\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3019 - val_loss: 1.3034\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3016 - val_loss: 1.3035\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3014 - val_loss: 1.3030\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3014 - val_loss: 1.3034\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3022 - val_loss: 1.3077\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3017 - val_loss: 1.3052\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3019 - val_loss: 1.3073\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3018 - val_loss: 1.3048\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3015 - val_loss: 1.3034\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3014 - val_loss: 1.3029\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3012 - val_loss: 1.3024\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3012 - val_loss: 1.3021\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3011 - val_loss: 1.3021\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3010 - val_loss: 1.3019\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3011 - val_loss: 1.3023\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3010 - val_loss: 1.3025\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3010 - val_loss: 1.3020\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3009 - val_loss: 1.3021\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3009 - val_loss: 1.3019\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3009 - val_loss: 1.3017\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3008 - val_loss: 1.3015\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3008 - val_loss: 1.3015\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3008 - val_loss: 1.3014\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3008 - val_loss: 1.3014\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3007 - val_loss: 1.3013\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3007 - val_loss: 1.3012\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3007 - val_loss: 1.3012\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3007 - val_loss: 1.3012\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3007 - val_loss: 1.3014\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3007 - val_loss: 1.3014\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3007 - val_loss: 1.3014\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3010 - val_loss: 1.3049\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3012 - val_loss: 1.3186\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3010 - val_loss: 1.3033\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3009 - val_loss: 1.3024\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3009 - val_loss: 1.3021\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3009 - val_loss: 1.3024\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3008 - val_loss: 1.3020\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3008 - val_loss: 1.3016\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3007 - val_loss: 1.3015\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3007 - val_loss: 1.3014\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3007 - val_loss: 1.3014\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3007 - val_loss: 1.3016\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3007 - val_loss: 1.3014\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3007 - val_loss: 1.3013\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3007 - val_loss: 1.3013\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3006 - val_loss: 1.3013\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3006 - val_loss: 1.3012\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3006 - val_loss: 1.3012\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3006 - val_loss: 1.3011\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3006 - val_loss: 1.3011\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3006 - val_loss: 1.3010\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3006 - val_loss: 1.3010\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3006 - val_loss: 1.3010\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3006 - val_loss: 1.3011\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3006 - val_loss: 1.3010\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3006 - val_loss: 1.3010\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3006 - val_loss: 1.3011\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3006 - val_loss: 1.3012\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3006 - val_loss: 1.3013\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3006 - val_loss: 1.3014\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3006 - val_loss: 1.3015\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3024\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3005 - val_loss: 1.3011\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3014\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3029\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3028\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3031\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3048\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3005 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3033\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3040\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3035\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3005 - val_loss: 1.3013\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3021\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3024\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3022\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3019\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3005 - val_loss: 1.3022\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3028\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3041\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3030\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3019\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3021\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3021\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3005 - val_loss: 1.3015\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3005 - val_loss: 1.3024\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3019\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3030\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 326us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 325us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3004 - val_loss: 1.3005\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 256 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.789546012878418\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7209375\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4752 - val_loss: 1.8789\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3462 - val_loss: 1.4488\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3258 - val_loss: 1.3627\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3207 - val_loss: 1.3199\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3197 - val_loss: 1.3075\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3148 - val_loss: 1.3094\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3116 - val_loss: 1.3062\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3092 - val_loss: 1.3057\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3085 - val_loss: 1.3057\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3085 - val_loss: 1.3065\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3127 - val_loss: 1.3390\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3090 - val_loss: 1.3232\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3100 - val_loss: 1.3265\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3080 - val_loss: 1.3142\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3074 - val_loss: 1.3112\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3071 - val_loss: 1.3106\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3067 - val_loss: 1.3097\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3067 - val_loss: 1.3091\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3065 - val_loss: 1.3087\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3064 - val_loss: 1.3079\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3064 - val_loss: 1.3077\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3069 - val_loss: 1.3158\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3065 - val_loss: 1.3116\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3062 - val_loss: 1.3110\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3061 - val_loss: 1.3097\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3060 - val_loss: 1.3104\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3059 - val_loss: 1.3095\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3059 - val_loss: 1.3104\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3058 - val_loss: 1.3079\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3058 - val_loss: 1.3080\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3057 - val_loss: 1.3064\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3057 - val_loss: 1.3067\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3057 - val_loss: 1.3062\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3057 - val_loss: 1.3062\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3057 - val_loss: 1.3068\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3057 - val_loss: 1.3069\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3070\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3075\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3071\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3056 - val_loss: 1.3075\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3071\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3068\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3069\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3057\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1088 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7800190448760986\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.690616\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.5239 - val_loss: 1.9362\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3628 - val_loss: 1.7946\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3308 - val_loss: 1.4747\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3250 - val_loss: 1.3327\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3174 - val_loss: 1.3133\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3152 - val_loss: 1.3103\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3136 - val_loss: 1.3109\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3133 - val_loss: 1.3107\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3119 - val_loss: 1.3102\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3116 - val_loss: 1.3101\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3110 - val_loss: 1.3101\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3106 - val_loss: 1.3100\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3105 - val_loss: 1.3100\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3103 - val_loss: 1.3099\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3106 - val_loss: 1.3100\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3103 - val_loss: 1.3102\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3102 - val_loss: 1.3100\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3101 - val_loss: 1.3099\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3100 - val_loss: 1.3098\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3100 - val_loss: 1.3099\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3100 - val_loss: 1.3098\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3104 - val_loss: 1.3118\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3103 - val_loss: 1.3097\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3100 - val_loss: 1.3097\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3099 - val_loss: 1.3097\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3098 - val_loss: 1.3097\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3100 - val_loss: 1.3100\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 327us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3106\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3103\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 328us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3096 - val_loss: 1.3096\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 603 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7752715349197388\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6211260000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 1ms/step - loss: 1.5023 - val_loss: 1.5959\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3609 - val_loss: 2.0038\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3280 - val_loss: 1.3659\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3223 - val_loss: 1.3381\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3161 - val_loss: 1.3088\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3122 - val_loss: 1.3087\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3098 - val_loss: 1.3076\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3089 - val_loss: 1.3072\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3108 - val_loss: 1.3196\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3081 - val_loss: 1.3076\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3071 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3066 - val_loss: 1.3061\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3065 - val_loss: 1.3059\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3061 - val_loss: 1.3059\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3061 - val_loss: 1.3058\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3060 - val_loss: 1.3058\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3061 - val_loss: 1.3058\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3058 - val_loss: 1.3068\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3057 - val_loss: 1.3062\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 341us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 160 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7774516344070435\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6334135\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.4886 - val_loss: 1.8522\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3511 - val_loss: 1.7661\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3237 - val_loss: 1.3480\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3154 - val_loss: 1.3118\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3136 - val_loss: 1.3159\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3114 - val_loss: 1.3176\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3097 - val_loss: 1.3126\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3082 - val_loss: 1.3095\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3072 - val_loss: 1.3098\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3067 - val_loss: 1.3085\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3065 - val_loss: 1.3073\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3065 - val_loss: 1.3079\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3080 - val_loss: 1.3194\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3073 - val_loss: 1.3072\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3065 - val_loss: 1.3067\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3063 - val_loss: 1.3080\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3063 - val_loss: 1.3077\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3064 - val_loss: 1.3063\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3069 - val_loss: 1.3181\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3064 - val_loss: 1.3096\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3062 - val_loss: 1.3081\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3061 - val_loss: 1.3080\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3060 - val_loss: 1.3081\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3060 - val_loss: 1.3082\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3060 - val_loss: 1.3088\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3060 - val_loss: 1.3149\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3059 - val_loss: 1.3141\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3058 - val_loss: 1.3155\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3057 - val_loss: 1.3141\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3056 - val_loss: 1.3142\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3056 - val_loss: 1.3140\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3191\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3056 - val_loss: 1.3322\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3109\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3092\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3098\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3092\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3082\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3084\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3079\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3079\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3078\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3079\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3078\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3076\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3080\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3078\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3079\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3079\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3076\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3079\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3055 - val_loss: 1.3085\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3076\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3080\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3068\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3070\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3077\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3074\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3055 - val_loss: 1.3075\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3055 - val_loss: 1.3078\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3055 - val_loss: 1.3078\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3076\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3079\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3080\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 341us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3068\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3068\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 330us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 331us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3068\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 329us/step - loss: 1.3054 - val_loss: 1.3076\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3076\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3054 - val_loss: 1.3066\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 6835 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.819008469581604\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6596056666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.5345 - val_loss: 1.8458\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3667 - val_loss: 1.7617\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3294 - val_loss: 1.7485\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3168 - val_loss: 1.3267\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3135 - val_loss: 1.3217\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3106 - val_loss: 1.3074\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3098 - val_loss: 1.3161\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3115 - val_loss: 1.3344\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3088 - val_loss: 1.3109\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3073 - val_loss: 1.3083\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3070 - val_loss: 1.3076\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3066 - val_loss: 1.3086\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3063 - val_loss: 1.3081\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3057 - val_loss: 1.3068\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3058 - val_loss: 1.3085\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 332us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3068\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3080\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3080\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 333us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3075\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3072\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 344us/step - loss: 1.3053 - val_loss: 1.3073\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3078\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3071\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3073\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3074\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3076\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 341us/step - loss: 1.3053 - val_loss: 1.3078\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3076\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3078\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 343us/step - loss: 1.3053 - val_loss: 1.3065\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3070\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3067\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3071\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3070\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3074\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3072\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3072\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3073\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3077\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3068\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3077\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3065\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3072\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3068\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 342us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 343us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3071\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3067\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3069\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 342us/step - loss: 1.3053 - val_loss: 1.3069\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 342us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3065\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 344us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 342us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 341us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3064\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3064\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3065\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3065\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 338us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 342us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 334us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 340us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 335us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 339us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 336us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 337us/step - loss: 1.3053 - val_loss: 1.3060\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4249 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7948446273803711\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6266646666666666\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6310293333333332, 0.7121816666666667, 0.6137783333333333, 0.5793148333333333, 0.7209375, 0.690616, 0.6211260000000001, 0.6334135, 0.6596056666666666, 0.6266646666666666]\n","AUROC ===== 0.64886675 +/- 0.04353093160591866\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f3eb9ef80f0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"k0zRbyxXyjhR","colab_type":"text"},"cell_type":"markdown","source":["## Horse Vs All"]},{"metadata":{"id":"KI8KS6woyjRx","colab_type":"code","outputId":"820691f8-1990-4962-f6f5-e31ff2eb256c","executionInfo":{"status":"ok","timestamp":1541653518580,"user_tz":-660,"elapsed":974924,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":20813}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","# RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (5500, 32, 32, 3)\n","Train Label Shape:  (5500,)\n","Validation Data Shape:  (5500, 32, 32, 3)\n","Validation Label Shape:  (5500,)\n","Test Data Shape:  (5500, 32, 32, 3)\n","Test Label Shape:  (5500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 4950 samples, validate on 550 samples\n","Epoch 1/500\n","4950/4950 [==============================] - 5s 983us/step - loss: 1.5386 - val_loss: 2.0201\n","Epoch 2/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3847 - val_loss: 1.7554\n","Epoch 3/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3394 - val_loss: 1.6781\n","Epoch 4/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3235 - val_loss: 1.4846\n","Epoch 5/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3167 - val_loss: 1.3361\n","Epoch 6/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3138 - val_loss: 1.3093\n","Epoch 7/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3105 - val_loss: 1.3073\n","Epoch 8/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3091 - val_loss: 1.3071\n","Epoch 9/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3083 - val_loss: 1.3069\n","Epoch 10/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3078 - val_loss: 1.3069\n","Epoch 11/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3077 - val_loss: 1.3070\n","Epoch 12/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3074 - val_loss: 1.3071\n","Epoch 13/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3071 - val_loss: 1.3069\n","Epoch 14/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3069 - val_loss: 1.3068\n","Epoch 15/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3068\n","Epoch 16/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3068 - val_loss: 1.3068\n","Epoch 17/500\n","4950/4950 [==============================] - 2s 391us/step - loss: 1.3068 - val_loss: 1.3067\n","Epoch 18/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3067 - val_loss: 1.3067\n","Epoch 19/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3068 - val_loss: 1.3067\n","Epoch 20/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3067 - val_loss: 1.3066\n","Epoch 21/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3067 - val_loss: 1.3066\n","Epoch 22/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3067 - val_loss: 1.3066\n","Epoch 23/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3066 - val_loss: 1.3068\n","Epoch 24/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3066 - val_loss: 1.3067\n","Epoch 25/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3066 - val_loss: 1.3067\n","Epoch 26/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3066 - val_loss: 1.3067\n","Epoch 27/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3066 - val_loss: 1.3066\n","Epoch 28/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3067 - val_loss: 1.3069\n","Epoch 29/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3066 - val_loss: 1.3068\n","Epoch 30/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3066 - val_loss: 1.3070\n","Epoch 31/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3066 - val_loss: 1.3066\n","Epoch 32/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 33/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 34/500\n","4950/4950 [==============================] - 2s 372us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 35/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 36/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 37/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 38/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3065 - val_loss: 1.3064\n","Epoch 39/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 40/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 41/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 42/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 43/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3065 - val_loss: 1.3064\n","Epoch 44/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 45/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 46/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 47/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 48/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 49/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 50/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 51/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 52/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 53/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 54/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 55/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 56/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 57/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 58/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 59/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 60/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 61/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 62/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 63/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 64/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 65/500\n","4950/4950 [==============================] - 2s 391us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 66/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 67/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 68/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 69/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 70/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 71/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 72/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 73/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 74/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 75/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 76/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 77/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 78/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 79/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 80/500\n","4950/4950 [==============================] - 2s 391us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 81/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 82/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 83/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 84/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 85/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 86/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 87/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 88/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 89/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 90/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 91/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 92/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 93/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 94/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 95/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 96/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 97/500\n","4950/4950 [==============================] - 2s 392us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 98/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 99/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 100/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 101/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 102/500\n","4950/4950 [==============================] - 2s 392us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 103/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 104/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 105/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 106/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 107/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 108/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 109/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 110/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 111/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 112/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 113/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 114/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 115/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 116/500\n","4950/4950 [==============================] - 2s 371us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 117/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 118/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 119/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 120/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 121/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 122/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 123/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 124/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 125/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 126/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 127/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 128/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 129/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 130/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 131/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 132/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 133/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 134/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 135/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 136/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 137/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 138/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 139/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 140/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 141/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 142/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 143/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 144/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 145/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 146/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 147/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 148/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 149/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 150/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 151/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 152/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 153/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 154/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 155/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 156/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 157/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 158/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 159/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 160/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 161/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 162/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 163/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 164/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 165/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 166/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 167/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 168/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 169/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 170/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 171/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 172/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 173/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 174/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 175/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 176/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 177/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 178/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 179/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 180/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 181/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 182/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 183/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 184/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 185/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 186/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 187/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 188/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 189/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 190/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 191/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 192/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 193/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 194/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 195/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 196/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 197/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 198/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 199/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 200/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 201/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 202/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 203/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 204/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 205/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 206/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 207/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 208/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 209/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 210/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 211/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 212/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 213/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 214/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 215/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 216/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 217/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 218/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 219/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 220/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 221/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 222/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 223/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 224/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 225/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 226/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 227/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 228/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 229/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 230/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 231/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 232/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 233/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 234/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 235/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 236/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 237/500\n","4950/4950 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 238/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 239/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 240/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 241/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 242/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 243/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 244/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 245/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 246/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 247/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 248/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 249/500\n","4950/4950 [==============================] - 2s 371us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 250/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 251/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 252/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 253/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 254/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 255/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 256/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 257/500\n","4950/4950 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 258/500\n","4950/4950 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 259/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 260/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 261/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 262/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 263/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 264/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 265/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 266/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 267/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 268/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 269/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 270/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 271/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3072\n","Epoch 272/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.4370\n","Epoch 273/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3064 - val_loss: 1.3127\n","Epoch 274/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3064 - val_loss: 1.3073\n","Epoch 275/500\n","4950/4950 [==============================] - 2s 393us/step - loss: 1.3064 - val_loss: 1.3067\n","Epoch 276/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 277/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 278/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 279/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 280/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 281/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 282/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 283/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 284/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 285/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 286/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 287/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 288/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 289/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 290/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 291/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 292/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 293/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 294/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 295/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 296/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 297/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 298/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 299/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 300/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 301/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 302/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 303/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 304/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 305/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 306/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 307/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 308/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 309/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 310/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 311/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 312/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 313/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 314/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 315/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 316/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 317/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 318/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 319/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 320/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 321/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 322/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 323/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 324/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 325/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 326/500\n","4950/4950 [==============================] - 2s 391us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 327/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 328/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 329/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 330/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 331/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 332/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 333/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 334/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 335/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 336/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 337/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 338/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 339/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 340/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 341/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 342/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 343/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 344/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 345/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 346/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 347/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 348/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 349/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 350/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 351/500\n","4950/4950 [==============================] - 2s 392us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 352/500\n","4950/4950 [==============================] - 2s 392us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 353/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 354/500\n","4950/4950 [==============================] - 2s 391us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 355/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 356/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 357/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 358/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 359/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 360/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 361/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 362/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 363/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 364/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 365/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 366/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 367/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 368/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 369/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 370/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 371/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 372/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 373/500\n","4950/4950 [==============================] - 2s 391us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 374/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 375/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 376/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 377/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 378/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 379/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 380/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 381/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 382/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 383/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 384/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 385/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 386/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 387/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 388/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 389/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 390/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 391/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 392/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 393/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 394/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 395/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 396/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 397/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 398/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 399/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 400/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 401/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 402/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 403/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 404/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 405/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 406/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 407/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 408/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 409/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 410/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 411/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 412/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 413/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 414/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 415/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 416/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 417/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 418/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 419/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 420/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 421/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 422/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 423/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 424/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 425/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 426/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 427/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 428/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 429/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 430/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 431/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 432/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 433/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 434/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 435/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 436/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 437/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 438/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 439/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 440/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 441/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 442/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 443/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 444/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 445/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 446/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 447/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 448/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 449/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 450/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 451/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 452/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 453/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 454/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 455/500\n","4950/4950 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 456/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 457/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 458/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 459/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 460/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 461/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 462/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 463/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 464/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 465/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 466/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 467/500\n","4950/4950 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 468/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 469/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 470/500\n","4950/4950 [==============================] - 2s 372us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 471/500\n","4950/4950 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3066\n","Epoch 472/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3065\n","Epoch 473/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 474/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 475/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 476/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 477/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 478/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 479/500\n","4950/4950 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 480/500\n","4950/4950 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 481/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 482/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 483/500\n","4950/4950 [==============================] - 2s 392us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 484/500\n","4950/4950 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 485/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 486/500\n","4950/4950 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 487/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 488/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 489/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 490/500\n","4950/4950 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 491/500\n","4950/4950 [==============================] - 2s 385us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 492/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 493/500\n","4950/4950 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 494/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 495/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 496/500\n","4950/4950 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 497/500\n","4950/4950 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 498/500\n","4950/4950 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 499/500\n","4950/4950 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 500/500\n","4950/4950 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3063\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 14 0.5\n","The shape of N (5500, 3072)\n","The minimum value of N  -0.06450676918029785\n","The max value of N 0.0\n","[INFO:] The shape of input data   (5500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (5500, 32, 32, 3)\n","[INFO:] The shape of N  data   (5500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.6334259999999999\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6334259999999999]\n","AUROC ===== 0.6334259999999999 +/- 0.0\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f7f36bfcc18>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"9-iiTmKmiskH","colab_type":"text"},"cell_type":"markdown","source":["## Truck Vs All "]},{"metadata":{"id":"-Qv1ivg3iqsg","colab_type":"code","outputId":"3ebdf66c-e582-4975-dbb1-5745f8578ff3","executionInfo":{"status":"ok","timestamp":1541761311732,"user_tz":-660,"elapsed":3645780,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":64319}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4818 - val_loss: 2.0460\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3497 - val_loss: 1.7627\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 399us/step - loss: 1.3262 - val_loss: 1.3169\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3179 - val_loss: 1.3122\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3179 - val_loss: 1.3094\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3114 - val_loss: 1.3067\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3091 - val_loss: 1.3064\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3091 - val_loss: 1.3064\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3099 - val_loss: 1.3085\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3085 - val_loss: 1.3071\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3078 - val_loss: 1.3069\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3072 - val_loss: 1.3070\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3073 - val_loss: 1.3078\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3067 - val_loss: 1.3074\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3065 - val_loss: 1.3069\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3066\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3065 - val_loss: 1.3068\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3074 - val_loss: 1.3104\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3069 - val_loss: 1.3073\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3065 - val_loss: 1.3067\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3065\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3065\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3062 - val_loss: 1.3064\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3083\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3062 - val_loss: 1.3073\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3070\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3063 - val_loss: 1.3073\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3063 - val_loss: 1.3065\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3068\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3082\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3075\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3075\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3071\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3078\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3070\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3064\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1732 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7940351963043213\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7096633333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 869us/step - loss: 1.4750 - val_loss: 1.8106\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3482 - val_loss: 1.5018\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3251 - val_loss: 1.3359\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3180 - val_loss: 1.3151\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3158 - val_loss: 1.3207\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3114 - val_loss: 1.3137\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3117 - val_loss: 1.3114\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3091 - val_loss: 1.3072\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3078 - val_loss: 1.3064\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3070 - val_loss: 1.3063\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3066 - val_loss: 1.3063\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3071 - val_loss: 1.3091\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3065 - val_loss: 1.3079\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3064 - val_loss: 1.3082\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3087\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3071\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3068\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3094\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3075\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 3 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.3883408308029175\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7766016666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 988us/step - loss: 1.4890 - val_loss: 1.8326\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 439us/step - loss: 1.3494 - val_loss: 1.5595\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3266 - val_loss: 1.3085\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3231 - val_loss: 1.3127\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3143 - val_loss: 1.3045\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3099 - val_loss: 1.3045\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3095 - val_loss: 1.3132\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3083 - val_loss: 1.3057\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3070 - val_loss: 1.3044\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3055\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3057 - val_loss: 1.3041\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3050 - val_loss: 1.3045\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3052 - val_loss: 1.3051\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3044 - val_loss: 1.3044\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3044 - val_loss: 1.3046\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3040 - val_loss: 1.3045\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3037 - val_loss: 1.3054\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3040 - val_loss: 1.3040\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3051\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3038\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 512 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7800296545028687\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7483256666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.5322 - val_loss: 2.2264\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 410us/step - loss: 1.3682 - val_loss: 1.7194\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3332 - val_loss: 1.4707\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3233 - val_loss: 1.5118\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3179 - val_loss: 1.3405\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3184 - val_loss: 1.3200\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3124 - val_loss: 1.3116\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3100 - val_loss: 1.3107\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3092 - val_loss: 1.3102\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3082 - val_loss: 1.3087\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3075 - val_loss: 1.3084\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3074 - val_loss: 1.3080\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3070 - val_loss: 1.3078\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3068 - val_loss: 1.3073\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3067 - val_loss: 1.3071\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3065 - val_loss: 1.3068\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3067\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3066\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3066\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3065\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3062 - val_loss: 1.3064\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3067\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3068 - val_loss: 1.3079\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3064 - val_loss: 1.3067\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3239\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3062 - val_loss: 1.3075\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3061\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 531 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7730573415756226\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7430285000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4903 - val_loss: 2.0602\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 444us/step - loss: 1.3494 - val_loss: 1.6118\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3220 - val_loss: 1.3796\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3121 - val_loss: 1.3051\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3077 - val_loss: 1.3349\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3017\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3036 - val_loss: 1.3011\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3028 - val_loss: 1.3010\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3023 - val_loss: 1.3008\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3041 - val_loss: 1.3048\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3023 - val_loss: 1.3020\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3015 - val_loss: 1.3014\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3014 - val_loss: 1.3024\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3012 - val_loss: 1.3012\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3009 - val_loss: 1.3008\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3008 - val_loss: 1.3008\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3008 - val_loss: 1.3008\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3007 - val_loss: 1.3009\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3006 - val_loss: 1.3008\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3010\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 101 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7617722749710083\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7395413333333335\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4710 - val_loss: 1.9069\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3489 - val_loss: 1.7211\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3277 - val_loss: 1.4638\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3176 - val_loss: 1.3553\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3148 - val_loss: 1.3105\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3113 - val_loss: 1.3062\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3071\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3089 - val_loss: 1.3062\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3088 - val_loss: 1.3141\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3087 - val_loss: 1.3141\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3077 - val_loss: 1.3113\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3070 - val_loss: 1.3087\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3068 - val_loss: 1.3098\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3072\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3072\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3066\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3073\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3077\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3072\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3210\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3131\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3092\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3085\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 333 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7732479572296143\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7620476666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 1ms/step - loss: 1.5246 - val_loss: 2.1390\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 440us/step - loss: 1.3655 - val_loss: 1.7530\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3305 - val_loss: 1.3519\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3222 - val_loss: 1.3596\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3191 - val_loss: 1.3218\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3149 - val_loss: 1.3128\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3128 - val_loss: 1.3127\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3118 - val_loss: 1.3109\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3113 - val_loss: 1.3109\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3109 - val_loss: 1.3119\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3116 - val_loss: 1.3253\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3108 - val_loss: 1.3137\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3104 - val_loss: 1.3121\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3103 - val_loss: 1.3129\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3101 - val_loss: 1.3116\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3099 - val_loss: 1.3113\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3099 - val_loss: 1.3122\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3099 - val_loss: 1.3113\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3098 - val_loss: 1.3131\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3098 - val_loss: 1.3108\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3098 - val_loss: 1.3116\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3097 - val_loss: 1.3109\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3097 - val_loss: 1.3114\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3111\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3103\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3105\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3106\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3106\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3103\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3104\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3103\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3139\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3101 - val_loss: 1.3131\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3098 - val_loss: 1.3113\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3101\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3112\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3106\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3103\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3101\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3104\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3109\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3095 - val_loss: 1.3099\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3095 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3095 - val_loss: 1.3099\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3095 - val_loss: 1.3098\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3095 - val_loss: 1.3100\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3095 - val_loss: 1.3099\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3095 - val_loss: 1.3098\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 1097 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7705496549606323\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7680956666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.4932 - val_loss: 1.9513\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 429us/step - loss: 1.3562 - val_loss: 1.6082\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 405us/step - loss: 1.3289 - val_loss: 1.3897\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3188 - val_loss: 1.3159\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3137 - val_loss: 1.3066\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3105 - val_loss: 1.3098\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3090 - val_loss: 1.3080\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3075 - val_loss: 1.3074\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3071 - val_loss: 1.3103\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3067 - val_loss: 1.3064\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3061\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3056\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 0 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  0.0\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7282931666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.4731 - val_loss: 1.9868\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3472 - val_loss: 1.4655\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 429us/step - loss: 1.3246 - val_loss: 1.4965\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 421us/step - loss: 1.3199 - val_loss: 1.3077\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3164 - val_loss: 1.3111\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3123 - val_loss: 1.3068\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3099 - val_loss: 1.3061\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3080 - val_loss: 1.3083\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3069 - val_loss: 1.3078\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3065 - val_loss: 1.3086\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3070\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3055\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3057 - val_loss: 1.3055\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3072\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3065 - val_loss: 1.3086\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3059\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 2930 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7525659799575806\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7376199999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.5171 - val_loss: 2.2343\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3571 - val_loss: 1.6721\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3266 - val_loss: 1.5009\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3196 - val_loss: 1.3230\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3139 - val_loss: 1.3316\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3109 - val_loss: 1.3172\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3092 - val_loss: 1.3161\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3084 - val_loss: 1.3080\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3077 - val_loss: 1.3124\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3069 - val_loss: 1.3110\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3068 - val_loss: 1.3124\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3092\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3080\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3078\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3071\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3087\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3084\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3079\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3071\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3073 - val_loss: 1.3063\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","(lamda,Threshold) 0.5 0.25\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 174 0.5\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7819720506668091\n","The max value of N 0.0\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.5 0.7343603333333332\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.7096633333333333, 0.7766016666666666, 0.7483256666666667, 0.7430285000000001, 0.7395413333333335, 0.7620476666666667, 0.7680956666666667, 0.7282931666666667, 0.7376199999999999, 0.7343603333333332]\n","AUROC ===== 0.7447577333333333 +/- 0.018874109973747416\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7ffb4b72b630>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"0yV2B7vdigsL","colab_type":"text"},"cell_type":"markdown","source":["##**SHIP vs All **##"]},{"metadata":{"id":"AEX-Ew14SsBs","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lv-Y19P4ShpX","colab_type":"text"},"cell_type":"markdown","source":["##** Bird vs All **##"]},{"metadata":{"id":"pwPagZNWSfOT","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"MBX8bnXdYH9U","colab_type":"text"},"cell_type":"markdown","source":["##** Truck vs All **##"]},{"metadata":{"id":"Ob-cXiYH-jPJ","colab_type":"code","colab":{}},"cell_type":"code","source":["## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","\n","from src.models.RCAE import RCAE_AD\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","\n","rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n","\n","print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n","rcae.fit_and_predict()\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"2dPb3wdU-lHb","colab_type":"text"},"cell_type":"markdown","source":["##** AIRPLANE vs All **##"]},{"metadata":{"id":"u45TrAvEYH9V","colab_type":"code","colab":{}},"cell_type":"code","source":["## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","\n","from src.models.RCAE import RCAE_AD\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","\n","rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n","\n","print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n","rcae.fit_and_predict()\n","print(\"========================================================================\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"gXZLPWvN-990"},"cell_type":"markdown","source":["##** Deer vs All **##"]},{"metadata":{"colab_type":"code","id":"Jc_XoHhM-994","colab":{}},"cell_type":"code","source":["## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","\n","from src.models.RCAE import RCAE_AD\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","\n","rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n","\n","print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n","rcae.fit_and_predict()\n","print(\"========================================================================\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"vuHSXno-_AY8"},"cell_type":"markdown","source":["##** DOG vs All **##"]},{"metadata":{"colab_type":"code","id":"3u98_paZ-5mP","colab":{}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"Z95yT0XT_IUF"},"cell_type":"markdown","source":["##** FROG vs All **##"]},{"metadata":{"colab_type":"code","id":"30la3ggt_AY-","colab":{}},"cell_type":"code","source":["## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","\n","from src.models.RCAE import RCAE_AD\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","\n","rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n","\n","print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n","rcae.fit_and_predict()\n","print(\"========================================================================\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"yQZOt1vR_F0E"},"cell_type":"markdown","source":["##** AUTOMOBILE vs All **##"]},{"metadata":{"colab_type":"code","id":"L55Po1BP_IU3","colab":{}},"cell_type":"code","source":["## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","\n","from src.models.RCAE import RCAE_AD\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","\n","rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n","\n","print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n","rcae.fit_and_predict()\n","print(\"========================================================================\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"Zxpe0vjr_Ms8"},"cell_type":"markdown","source":["##** Cat vs All **##"]},{"metadata":{"colab_type":"code","id":"m_sPbQnR_F0F","colab":{}},"cell_type":"code","source":["## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","\n","from src.models.RCAE import RCAE_AD\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","\n","rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n","\n","print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n","rcae.fit_and_predict()\n","print(\"========================================================================\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"colab_type":"text","id":"9beMvjos-5mN"},"cell_type":"markdown","source":["##** HORSE vs All **##"]},{"metadata":{"colab_type":"code","id":"bvxkoKOM_Ms-","colab":{}},"cell_type":"code","source":["## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","\n","from src.models.RCAE import RCAE_AD\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","\n","rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH)\n","\n","print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","print(\"===========TRAINING AND PREDICTING WITH RCAE============================\")\n","rcae.fit_and_predict()\n","print(\"========================================================================\")\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EviuCbjUI9cW","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}