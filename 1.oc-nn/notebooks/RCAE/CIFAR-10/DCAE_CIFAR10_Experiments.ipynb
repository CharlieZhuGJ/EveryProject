{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCAE_CIFAR10_Experiments.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"6TKSizAjYUU1","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":836},"outputId":"aa595e0f-2a91-49eb-8098-16fd1f743395","executionInfo":{"status":"ok","timestamp":1541753892935,"user_tz":-660,"elapsed":45853,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install picklable_itertools\n","!pip install fuel"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n","Collecting picklable_itertools\n","  Downloading https://files.pythonhosted.org/packages/75/bc/cda9191f0c92960ede6aa95e364443965246385faf62775cc30749931c2c/picklable-itertools-0.1.1.tar.gz\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from picklable_itertools) (1.11.0)\n","Building wheels for collected packages: picklable-itertools\n","  Running setup.py bdist_wheel for picklable-itertools ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/36/dd/e2/ec30ef7c475e1d9fb966735984ba05f8710c67d7de5358c326\n","Successfully built picklable-itertools\n","Installing collected packages: picklable-itertools\n","Successfully installed picklable-itertools-0.1.1\n","Collecting fuel\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c2/b5fb651c90e908f79769b7dd3643982b6a9b1bac9449b8ab16f72612d4f5/fuel-0.2.0.tar.gz (184kB)\n","\u001b[K    100% |████████████████████████████████| 194kB 13.7MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.14.6)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fuel) (1.11.0)\n","Requirement already satisfied: picklable_itertools in /usr/local/lib/python3.6/dist-packages (from fuel) (0.1.1)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fuel) (3.13)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fuel) (2.8.0)\n","Collecting tables (from fuel)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/1b/21f4c7f296b718575c17ef25e61c05742a283c45077b4c8d5a190b3e0b59/tables-3.4.4-cp36-cp36m-manylinux1_x86_64.whl (3.8MB)\n","\u001b[K    100% |████████████████████████████████| 3.8MB 10.2MB/s \n","\u001b[?25hCollecting progressbar2 (from fuel)\n","  Downloading https://files.pythonhosted.org/packages/4f/6f/acb2dd76f2c77527584bd3a4c2509782bb35c481c610521fc3656de5a9e0/progressbar2-3.38.0-py2.py3-none-any.whl\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fuel) (17.0.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.1.0)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fuel) (4.0.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fuel) (2.18.4)\n","Collecting numexpr>=2.5.2 (from tables->fuel)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/ea/efd9e16283637eb5b6c0042b6cc3521f1b9a5b47767ac463c88bbd37670c/numexpr-2.6.8-cp36-cp36m-manylinux1_x86_64.whl (162kB)\n","\u001b[K    100% |████████████████████████████████| 163kB 28.4MB/s \n","\u001b[?25hCollecting python-utils>=2.3.0 (from progressbar2->fuel)\n","  Downloading https://files.pythonhosted.org/packages/eb/a0/19119d8b7c05be49baf6c593f11c432d571b70d805f2fe94c0585e55e4c8/python_utils-2.3.0-py2.py3-none-any.whl\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->fuel) (0.46)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (1.22)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2018.10.15)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2.6)\n","Building wheels for collected packages: fuel\n","  Running setup.py bdist_wheel for fuel ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /root/.cache/pip/wheels/96/ff/1b/f3708731cf77a6d759cd12e68c0a27700c299fbe2bffd34fa3\n","Successfully built fuel\n","Installing collected packages: numexpr, tables, python-utils, progressbar2, fuel\n","Successfully installed fuel-0.2.0 numexpr-2.6.8 progressbar2-3.38.0 python-utils-2.3.0 tables-3.4.4\n"],"name":"stdout"}]},{"metadata":{"id":"qO-ewRVzYH9O","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","# np.random.seed(42)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"EX7QNOkjYH9S","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","\n","PROJECT_DIR = \"/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/\"\n","import sys,os\n","import numpy as np\n","sys.path.append(PROJECT_DIR)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"m_JeOeMfIwbt","colab_type":"text"},"cell_type":"markdown","source":["## Aeroplane Vs All "]},{"metadata":{"id":"dd-Ed9G23C5_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":62619},"outputId":"caf31d9b-6b2b-4b36-e6c1-4068b712cff3","executionInfo":{"status":"ok","timestamp":1541563189106,"user_tz":-660,"elapsed":3638933,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/DCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/DCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 874us/step - loss: 1.3641 - val_loss: 1.4034\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3359 - val_loss: 1.3566\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3314 - val_loss: 1.3468\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3292 - val_loss: 1.3415\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3275 - val_loss: 1.3372\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3265 - val_loss: 1.3353\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3256 - val_loss: 1.3353\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3246 - val_loss: 1.3337\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3242 - val_loss: 1.3335\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3237 - val_loss: 1.3352\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3233 - val_loss: 1.3322\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3228 - val_loss: 1.3300\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3225 - val_loss: 1.3308\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3223 - val_loss: 1.3325\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3221 - val_loss: 1.3294\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3218 - val_loss: 1.3285\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3215 - val_loss: 1.3275\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3212 - val_loss: 1.3274\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3210 - val_loss: 1.3268\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3208 - val_loss: 1.3277\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3207 - val_loss: 1.3264\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3204 - val_loss: 1.3276\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3202 - val_loss: 1.3259\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3201 - val_loss: 1.3249\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3201 - val_loss: 1.3252\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3198 - val_loss: 1.3254\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3196 - val_loss: 1.3246\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3195 - val_loss: 1.3261\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3194 - val_loss: 1.3248\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3195 - val_loss: 1.3248\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3194 - val_loss: 1.3241\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3191 - val_loss: 1.3244\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3190 - val_loss: 1.3240\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3188 - val_loss: 1.3235\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3188 - val_loss: 1.3234\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3186 - val_loss: 1.3232\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3187 - val_loss: 1.3229\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3186 - val_loss: 1.3228\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3184 - val_loss: 1.3227\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3184 - val_loss: 1.3227\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3184 - val_loss: 1.3230\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3182 - val_loss: 1.3240\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3182 - val_loss: 1.3229\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3180 - val_loss: 1.3226\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3181 - val_loss: 1.3224\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3178 - val_loss: 1.3231\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3177 - val_loss: 1.3224\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3178 - val_loss: 1.3221\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3178 - val_loss: 1.3219\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3176 - val_loss: 1.3221\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3176 - val_loss: 1.3215\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3174 - val_loss: 1.3213\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3173 - val_loss: 1.3216\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3173 - val_loss: 1.3208\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3173 - val_loss: 1.3211\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3173 - val_loss: 1.3214\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3172 - val_loss: 1.3215\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3170 - val_loss: 1.3209\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3170 - val_loss: 1.3216\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3170 - val_loss: 1.3215\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3168 - val_loss: 1.3205\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3167 - val_loss: 1.3205\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3168 - val_loss: 1.3202\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3169 - val_loss: 1.3205\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3166 - val_loss: 1.3205\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3166 - val_loss: 1.3201\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3166 - val_loss: 1.3204\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3164 - val_loss: 1.3201\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3165 - val_loss: 1.3201\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3165 - val_loss: 1.3200\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3164 - val_loss: 1.3208\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3163 - val_loss: 1.3204\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3163 - val_loss: 1.3210\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3162 - val_loss: 1.3206\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3162 - val_loss: 1.3209\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3162 - val_loss: 1.3205\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3163 - val_loss: 1.3200\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3160 - val_loss: 1.3200\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3160 - val_loss: 1.3199\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3161 - val_loss: 1.3196\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3160 - val_loss: 1.3193\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3159 - val_loss: 1.3194\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3159 - val_loss: 1.3197\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3156 - val_loss: 1.3195\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3156 - val_loss: 1.3196\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3157 - val_loss: 1.3193\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3155 - val_loss: 1.3192\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3155 - val_loss: 1.3191\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3154 - val_loss: 1.3188\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3154 - val_loss: 1.3189\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3153 - val_loss: 1.3184\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3153 - val_loss: 1.3185\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3152 - val_loss: 1.3189\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3152 - val_loss: 1.3190\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3152 - val_loss: 1.3185\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3152 - val_loss: 1.3191\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3152 - val_loss: 1.3185\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3151 - val_loss: 1.3185\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3150 - val_loss: 1.3184\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3150 - val_loss: 1.3185\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3149 - val_loss: 1.3185\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3149 - val_loss: 1.3183\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3148 - val_loss: 1.3182\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3148 - val_loss: 1.3181\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3148 - val_loss: 1.3186\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3148 - val_loss: 1.3176\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3148 - val_loss: 1.3178\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3148 - val_loss: 1.3179\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3148 - val_loss: 1.3180\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3145 - val_loss: 1.3177\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3145 - val_loss: 1.3175\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3145 - val_loss: 1.3177\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3146 - val_loss: 1.3178\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3143 - val_loss: 1.3173\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3144 - val_loss: 1.3174\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3144 - val_loss: 1.3178\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3143 - val_loss: 1.3177\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3145 - val_loss: 1.3184\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3147 - val_loss: 1.3186\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3144 - val_loss: 1.3176\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3142 - val_loss: 1.3174\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3142 - val_loss: 1.3176\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3142 - val_loss: 1.3173\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3141 - val_loss: 1.3171\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3142 - val_loss: 1.3171\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3142 - val_loss: 1.3173\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3140 - val_loss: 1.3170\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3140 - val_loss: 1.3173\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3168\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3140 - val_loss: 1.3180\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3166\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3138 - val_loss: 1.3167\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3138 - val_loss: 1.3166\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3138 - val_loss: 1.3166\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3138 - val_loss: 1.3170\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3138 - val_loss: 1.3166\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3138 - val_loss: 1.3165\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3137 - val_loss: 1.3165\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3137 - val_loss: 1.3164\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3137 - val_loss: 1.3169\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3137 - val_loss: 1.3170\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3140 - val_loss: 1.3193\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3139 - val_loss: 1.3179\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3137 - val_loss: 1.3171\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3136 - val_loss: 1.3173\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3136 - val_loss: 1.3166\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3135 - val_loss: 1.3165\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3135 - val_loss: 1.3165\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3134 - val_loss: 1.3162\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3134 - val_loss: 1.3163\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967991 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9095251\n","The max value of N 0.8571721\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.712608\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 865us/step - loss: 1.3652 - val_loss: 1.3927\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3347 - val_loss: 1.3515\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3305 - val_loss: 1.3409\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3284 - val_loss: 1.3384\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3271 - val_loss: 1.3343\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3261 - val_loss: 1.3332\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3253 - val_loss: 1.3306\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3245 - val_loss: 1.3304\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3239 - val_loss: 1.3295\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3234 - val_loss: 1.3283\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3229 - val_loss: 1.3280\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3226 - val_loss: 1.3284\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3224 - val_loss: 1.3274\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3221 - val_loss: 1.3275\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3218 - val_loss: 1.3282\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3213 - val_loss: 1.3265\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3211 - val_loss: 1.3260\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3209 - val_loss: 1.3251\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3207 - val_loss: 1.3257\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3205 - val_loss: 1.3249\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3203 - val_loss: 1.3258\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3200 - val_loss: 1.3245\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3199 - val_loss: 1.3254\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3197 - val_loss: 1.3243\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3196 - val_loss: 1.3245\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3194 - val_loss: 1.3244\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3194 - val_loss: 1.3240\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3191 - val_loss: 1.3232\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3190 - val_loss: 1.3241\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3189 - val_loss: 1.3230\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3188 - val_loss: 1.3229\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3186 - val_loss: 1.3227\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3185 - val_loss: 1.3226\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3184 - val_loss: 1.3226\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3183 - val_loss: 1.3223\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3182 - val_loss: 1.3222\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3180 - val_loss: 1.3220\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3180 - val_loss: 1.3221\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3179 - val_loss: 1.3223\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3176 - val_loss: 1.3215\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3178 - val_loss: 1.3222\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3177 - val_loss: 1.3220\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3175 - val_loss: 1.3213\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3175 - val_loss: 1.3228\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3173 - val_loss: 1.3209\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3174 - val_loss: 1.3214\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3173 - val_loss: 1.3207\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3171 - val_loss: 1.3207\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3171 - val_loss: 1.3212\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3171 - val_loss: 1.3212\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3171 - val_loss: 1.3206\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3168 - val_loss: 1.3205\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3169 - val_loss: 1.3201\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3167 - val_loss: 1.3207\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3168 - val_loss: 1.3205\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3165 - val_loss: 1.3198\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3165 - val_loss: 1.3196\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3166 - val_loss: 1.3199\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3164 - val_loss: 1.3201\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3163 - val_loss: 1.3199\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3163 - val_loss: 1.3200\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3162 - val_loss: 1.3197\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3162 - val_loss: 1.3194\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3161 - val_loss: 1.3194\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3160 - val_loss: 1.3191\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3161 - val_loss: 1.3201\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3159 - val_loss: 1.3198\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3159 - val_loss: 1.3190\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3158 - val_loss: 1.3195\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3158 - val_loss: 1.3187\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3157 - val_loss: 1.3187\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3157 - val_loss: 1.3193\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3155 - val_loss: 1.3188\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3157 - val_loss: 1.3187\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3156 - val_loss: 1.3193\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3157 - val_loss: 1.3186\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3157 - val_loss: 1.3196\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3155 - val_loss: 1.3188\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3154 - val_loss: 1.3194\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3153 - val_loss: 1.3199\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3152 - val_loss: 1.3183\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3152 - val_loss: 1.3195\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3151 - val_loss: 1.3187\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3151 - val_loss: 1.3180\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3153 - val_loss: 1.3181\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3150 - val_loss: 1.3183\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3150 - val_loss: 1.3191\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3150 - val_loss: 1.3181\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3149 - val_loss: 1.3178\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3150 - val_loss: 1.3179\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3148 - val_loss: 1.3187\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3148 - val_loss: 1.3176\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3148 - val_loss: 1.3178\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3147 - val_loss: 1.3179\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3146 - val_loss: 1.3176\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3148 - val_loss: 1.3189\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3145 - val_loss: 1.3178\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3146 - val_loss: 1.3177\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3146 - val_loss: 1.3179\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3144 - val_loss: 1.3177\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3143 - val_loss: 1.3174\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3144 - val_loss: 1.3171\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3143 - val_loss: 1.3180\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3143 - val_loss: 1.3177\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3143 - val_loss: 1.3177\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3143 - val_loss: 1.3170\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3141 - val_loss: 1.3170\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3142 - val_loss: 1.3171\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3141 - val_loss: 1.3170\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3142 - val_loss: 1.3180\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3140 - val_loss: 1.3172\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3140 - val_loss: 1.3172\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3139 - val_loss: 1.3170\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3167\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3139 - val_loss: 1.3166\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3139 - val_loss: 1.3167\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3164\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3139 - val_loss: 1.3168\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3139 - val_loss: 1.3166\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3138 - val_loss: 1.3166\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3136 - val_loss: 1.3168\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3136 - val_loss: 1.3162\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3136 - val_loss: 1.3169\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3136 - val_loss: 1.3163\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3136 - val_loss: 1.3162\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3136 - val_loss: 1.3166\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3136 - val_loss: 1.3163\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3135 - val_loss: 1.3168\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3136 - val_loss: 1.3168\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3135 - val_loss: 1.3162\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3135 - val_loss: 1.3164\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3135 - val_loss: 1.3165\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3134 - val_loss: 1.3160\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3133 - val_loss: 1.3158\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3132 - val_loss: 1.3158\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3133 - val_loss: 1.3162\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3133 - val_loss: 1.3159\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3132 - val_loss: 1.3165\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3138 - val_loss: 1.3226\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3133 - val_loss: 1.3178\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3132 - val_loss: 1.3167\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3131 - val_loss: 1.3161\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3131 - val_loss: 1.3160\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3130 - val_loss: 1.3155\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3130 - val_loss: 1.3154\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3130 - val_loss: 1.3155\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967995 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9018548\n","The max value of N 0.84494114\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.6943535000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 965us/step - loss: 1.3600 - val_loss: 1.3955\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3321 - val_loss: 1.3561\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3280 - val_loss: 1.3474\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3258 - val_loss: 1.3412\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3243 - val_loss: 1.3333\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3233 - val_loss: 1.3316\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3223 - val_loss: 1.3305\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3217 - val_loss: 1.3284\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3213 - val_loss: 1.3287\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3207 - val_loss: 1.3280\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3202 - val_loss: 1.3274\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3200 - val_loss: 1.3252\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3197 - val_loss: 1.3266\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3194 - val_loss: 1.3246\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3192 - val_loss: 1.3237\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3189 - val_loss: 1.3233\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3186 - val_loss: 1.3228\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3184 - val_loss: 1.3224\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3181 - val_loss: 1.3227\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3179 - val_loss: 1.3239\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3178 - val_loss: 1.3229\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3176 - val_loss: 1.3221\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3176 - val_loss: 1.3225\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3173 - val_loss: 1.3218\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3171 - val_loss: 1.3216\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3169 - val_loss: 1.3215\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3169 - val_loss: 1.3218\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3168 - val_loss: 1.3213\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3167 - val_loss: 1.3224\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3165 - val_loss: 1.3207\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3166 - val_loss: 1.3206\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3164 - val_loss: 1.3212\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3163 - val_loss: 1.3206\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3161 - val_loss: 1.3199\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3160 - val_loss: 1.3199\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3158 - val_loss: 1.3198\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3156 - val_loss: 1.3195\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3156 - val_loss: 1.3193\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3157 - val_loss: 1.3197\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3155 - val_loss: 1.3196\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3153 - val_loss: 1.3194\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3153 - val_loss: 1.3189\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3153 - val_loss: 1.3187\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3151 - val_loss: 1.3193\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3150 - val_loss: 1.3186\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3149 - val_loss: 1.3186\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3148 - val_loss: 1.3188\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3148 - val_loss: 1.3181\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3147 - val_loss: 1.3183\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3147 - val_loss: 1.3181\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3146 - val_loss: 1.3184\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3145 - val_loss: 1.3181\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3144 - val_loss: 1.3181\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3143 - val_loss: 1.3181\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3143 - val_loss: 1.3176\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3143 - val_loss: 1.3187\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3143 - val_loss: 1.3182\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3140 - val_loss: 1.3178\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3140 - val_loss: 1.3177\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3140 - val_loss: 1.3174\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3140 - val_loss: 1.3174\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3138 - val_loss: 1.3175\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3138 - val_loss: 1.3171\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3138 - val_loss: 1.3171\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3137 - val_loss: 1.3170\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3136 - val_loss: 1.3169\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3137 - val_loss: 1.3171\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3136 - val_loss: 1.3168\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3135 - val_loss: 1.3172\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3135 - val_loss: 1.3181\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3132 - val_loss: 1.3165\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3131 - val_loss: 1.3169\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3130 - val_loss: 1.3163\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3131 - val_loss: 1.3165\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3129 - val_loss: 1.3159\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3128 - val_loss: 1.3161\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3128 - val_loss: 1.3165\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3127 - val_loss: 1.3156\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3128 - val_loss: 1.3161\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3128 - val_loss: 1.3162\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3125 - val_loss: 1.3156\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3126 - val_loss: 1.3165\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3125 - val_loss: 1.3160\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3125 - val_loss: 1.3159\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3124 - val_loss: 1.3152\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3123 - val_loss: 1.3154\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3155\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3124 - val_loss: 1.3158\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3122 - val_loss: 1.3158\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3122 - val_loss: 1.3156\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3121 - val_loss: 1.3152\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3120 - val_loss: 1.3154\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3120 - val_loss: 1.3148\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3120 - val_loss: 1.3153\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3120 - val_loss: 1.3157\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3119 - val_loss: 1.3148\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3119 - val_loss: 1.3150\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3117 - val_loss: 1.3147\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3117 - val_loss: 1.3147\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3118 - val_loss: 1.3149\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3118 - val_loss: 1.3155\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3118 - val_loss: 1.3150\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3117 - val_loss: 1.3147\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3116 - val_loss: 1.3148\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3117 - val_loss: 1.3156\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3116 - val_loss: 1.3146\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3115 - val_loss: 1.3146\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3114 - val_loss: 1.3143\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3115 - val_loss: 1.3144\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3115 - val_loss: 1.3144\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3115 - val_loss: 1.3148\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3113 - val_loss: 1.3147\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3113 - val_loss: 1.3140\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3113 - val_loss: 1.3143\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3114 - val_loss: 1.3141\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3112 - val_loss: 1.3142\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3113 - val_loss: 1.3141\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3112 - val_loss: 1.3141\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3112 - val_loss: 1.3148\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3111 - val_loss: 1.3140\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3112 - val_loss: 1.3139\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3111 - val_loss: 1.3140\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3110 - val_loss: 1.3138\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3110 - val_loss: 1.3137\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3110 - val_loss: 1.3142\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3109 - val_loss: 1.3138\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3110 - val_loss: 1.3137\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3111 - val_loss: 1.3138\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3110 - val_loss: 1.3135\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3109 - val_loss: 1.3136\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3109 - val_loss: 1.3142\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3108 - val_loss: 1.3138\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3108 - val_loss: 1.3134\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3108 - val_loss: 1.3143\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3107 - val_loss: 1.3133\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3108 - val_loss: 1.3136\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3109 - val_loss: 1.3137\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3108 - val_loss: 1.3135\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3107 - val_loss: 1.3141\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3107 - val_loss: 1.3133\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3107 - val_loss: 1.3134\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3106 - val_loss: 1.3134\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3106 - val_loss: 1.3133\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967992 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.90658295\n","The max value of N 0.8399116\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7105566666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.3661 - val_loss: 1.4030\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3369 - val_loss: 1.3651\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3313 - val_loss: 1.3472\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3289 - val_loss: 1.3409\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3272 - val_loss: 1.3390\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3263 - val_loss: 1.3399\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3255 - val_loss: 1.3347\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3246 - val_loss: 1.3326\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3241 - val_loss: 1.3318\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3235 - val_loss: 1.3311\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3230 - val_loss: 1.3306\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3227 - val_loss: 1.3290\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3224 - val_loss: 1.3281\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3220 - val_loss: 1.3286\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3217 - val_loss: 1.3271\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3217 - val_loss: 1.3269\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3215 - val_loss: 1.3265\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3209 - val_loss: 1.3263\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3209 - val_loss: 1.3272\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3205 - val_loss: 1.3261\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3205 - val_loss: 1.3257\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3205 - val_loss: 1.3267\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3203 - val_loss: 1.3260\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3200 - val_loss: 1.3250\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3197 - val_loss: 1.3250\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3196 - val_loss: 1.3258\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3195 - val_loss: 1.3244\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3193 - val_loss: 1.3245\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3192 - val_loss: 1.3235\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3191 - val_loss: 1.3278\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3192 - val_loss: 1.3240\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3190 - val_loss: 1.3248\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3189 - val_loss: 1.3242\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3186 - val_loss: 1.3236\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3186 - val_loss: 1.3234\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3184 - val_loss: 1.3233\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3183 - val_loss: 1.3229\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3183 - val_loss: 1.3230\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3182 - val_loss: 1.3230\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3181 - val_loss: 1.3228\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3179 - val_loss: 1.3220\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3178 - val_loss: 1.3222\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3180 - val_loss: 1.3218\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3179 - val_loss: 1.3229\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3179 - val_loss: 1.3215\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3176 - val_loss: 1.3220\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3175 - val_loss: 1.3217\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3175 - val_loss: 1.3212\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3174 - val_loss: 1.3215\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3174 - val_loss: 1.3212\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3173 - val_loss: 1.3208\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3171 - val_loss: 1.3211\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3171 - val_loss: 1.3211\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3215\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3170 - val_loss: 1.3208\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3169 - val_loss: 1.3210\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3168 - val_loss: 1.3208\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3169 - val_loss: 1.3208\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3167 - val_loss: 1.3208\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3167 - val_loss: 1.3211\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3166 - val_loss: 1.3206\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3166 - val_loss: 1.3205\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3165 - val_loss: 1.3213\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3166 - val_loss: 1.3211\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3164 - val_loss: 1.3204\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3164 - val_loss: 1.3202\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3162 - val_loss: 1.3204\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3163 - val_loss: 1.3199\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3162 - val_loss: 1.3196\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3161 - val_loss: 1.3198\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3161 - val_loss: 1.3200\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3160 - val_loss: 1.3195\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3159 - val_loss: 1.3194\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3159 - val_loss: 1.3198\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3159 - val_loss: 1.3201\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3157 - val_loss: 1.3195\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3156 - val_loss: 1.3196\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3157 - val_loss: 1.3196\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3157 - val_loss: 1.3192\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3155 - val_loss: 1.3193\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3156 - val_loss: 1.3193\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3154 - val_loss: 1.3195\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3154 - val_loss: 1.3187\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3154 - val_loss: 1.3187\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3153 - val_loss: 1.3186\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3153 - val_loss: 1.3187\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3152 - val_loss: 1.3185\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3151 - val_loss: 1.3186\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3151 - val_loss: 1.3191\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3151 - val_loss: 1.3195\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3150 - val_loss: 1.3183\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3149 - val_loss: 1.3183\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3150 - val_loss: 1.3183\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3149 - val_loss: 1.3183\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3149 - val_loss: 1.3182\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3148 - val_loss: 1.3180\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3148 - val_loss: 1.3181\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3149 - val_loss: 1.3180\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3147 - val_loss: 1.3178\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3147 - val_loss: 1.3177\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3147 - val_loss: 1.3182\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3147 - val_loss: 1.3179\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3146 - val_loss: 1.3177\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3146 - val_loss: 1.3179\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3145 - val_loss: 1.3178\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3145 - val_loss: 1.3183\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3145 - val_loss: 1.3175\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3146 - val_loss: 1.3175\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3144 - val_loss: 1.3181\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3143 - val_loss: 1.3182\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3144 - val_loss: 1.3178\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3143 - val_loss: 1.3174\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3143 - val_loss: 1.3174\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3143 - val_loss: 1.3173\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3142 - val_loss: 1.3171\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3142 - val_loss: 1.3172\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3142 - val_loss: 1.3172\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3141 - val_loss: 1.3169\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3140 - val_loss: 1.3173\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3141 - val_loss: 1.3172\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3141 - val_loss: 1.3169\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3140 - val_loss: 1.3171\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3140 - val_loss: 1.3169\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3140 - val_loss: 1.3172\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3140 - val_loss: 1.3168\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3138 - val_loss: 1.3167\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3139 - val_loss: 1.3178\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3139 - val_loss: 1.3169\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3139 - val_loss: 1.3167\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3139 - val_loss: 1.3167\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3139 - val_loss: 1.3166\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3137 - val_loss: 1.3167\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3137 - val_loss: 1.3165\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3137 - val_loss: 1.3164\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3136 - val_loss: 1.3164\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3138 - val_loss: 1.3162\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3136 - val_loss: 1.3165\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3136 - val_loss: 1.3163\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3136 - val_loss: 1.3163\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3136 - val_loss: 1.3164\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3135 - val_loss: 1.3161\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3136 - val_loss: 1.3163\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3135 - val_loss: 1.3162\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3134 - val_loss: 1.3162\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3134 - val_loss: 1.3162\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3134 - val_loss: 1.3161\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3135 - val_loss: 1.3163\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3135 - val_loss: 1.3162\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967996 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9180686\n","The max value of N 0.8230108\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7093200000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.3566 - val_loss: 1.3855\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 433us/step - loss: 1.3297 - val_loss: 1.3519\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3251 - val_loss: 1.3395\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3229 - val_loss: 1.3347\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3217 - val_loss: 1.3313\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3207 - val_loss: 1.3285\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3198 - val_loss: 1.3265\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3190 - val_loss: 1.3255\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3185 - val_loss: 1.3254\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3181 - val_loss: 1.3260\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3177 - val_loss: 1.3232\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3173 - val_loss: 1.3228\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3170 - val_loss: 1.3258\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3167 - val_loss: 1.3226\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3163 - val_loss: 1.3210\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3161 - val_loss: 1.3224\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3158 - val_loss: 1.3219\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3159 - val_loss: 1.3208\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3155 - val_loss: 1.3205\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3153 - val_loss: 1.3209\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3151 - val_loss: 1.3203\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3148 - val_loss: 1.3200\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3147 - val_loss: 1.3194\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3146 - val_loss: 1.3189\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3144 - val_loss: 1.3188\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3142 - val_loss: 1.3188\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3141 - val_loss: 1.3190\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3141 - val_loss: 1.3184\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3141 - val_loss: 1.3186\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3137 - val_loss: 1.3177\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3136 - val_loss: 1.3180\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3134 - val_loss: 1.3177\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3135 - val_loss: 1.3180\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3135 - val_loss: 1.3179\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3132 - val_loss: 1.3179\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3173\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3128 - val_loss: 1.3172\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3128 - val_loss: 1.3167\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3126 - val_loss: 1.3167\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3126 - val_loss: 1.3169\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3125 - val_loss: 1.3162\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3125 - val_loss: 1.3163\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3164\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3124 - val_loss: 1.3164\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3123 - val_loss: 1.3159\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3122 - val_loss: 1.3159\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3121 - val_loss: 1.3167\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3121 - val_loss: 1.3161\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3120 - val_loss: 1.3156\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3119 - val_loss: 1.3166\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3118 - val_loss: 1.3153\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3117 - val_loss: 1.3167\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3118 - val_loss: 1.3154\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3117 - val_loss: 1.3153\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3115 - val_loss: 1.3153\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3115 - val_loss: 1.3152\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3113 - val_loss: 1.3151\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3112 - val_loss: 1.3151\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3113 - val_loss: 1.3149\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3113 - val_loss: 1.3147\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3111 - val_loss: 1.3145\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3111 - val_loss: 1.3143\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3111 - val_loss: 1.3145\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3110 - val_loss: 1.3145\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3109 - val_loss: 1.3148\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3107 - val_loss: 1.3144\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3107 - val_loss: 1.3146\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3108 - val_loss: 1.3142\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3106 - val_loss: 1.3141\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3107 - val_loss: 1.3139\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3106 - val_loss: 1.3144\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3106 - val_loss: 1.3160\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3105 - val_loss: 1.3144\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3105 - val_loss: 1.3139\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3105 - val_loss: 1.3142\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3103 - val_loss: 1.3140\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3102 - val_loss: 1.3135\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3102 - val_loss: 1.3142\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3102 - val_loss: 1.3137\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3102 - val_loss: 1.3133\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3101 - val_loss: 1.3133\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3100 - val_loss: 1.3135\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3101 - val_loss: 1.3134\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3099 - val_loss: 1.3136\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3099 - val_loss: 1.3135\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3099 - val_loss: 1.3134\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3098 - val_loss: 1.3128\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3098 - val_loss: 1.3128\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3097 - val_loss: 1.3130\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3098 - val_loss: 1.3128\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3096 - val_loss: 1.3128\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3095 - val_loss: 1.3132\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3096 - val_loss: 1.3127\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3141\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3125\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3095 - val_loss: 1.3128\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3095 - val_loss: 1.3133\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3095 - val_loss: 1.3127\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3094 - val_loss: 1.3127\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3093 - val_loss: 1.3129\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3092 - val_loss: 1.3128\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3092 - val_loss: 1.3123\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3092 - val_loss: 1.3124\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3091 - val_loss: 1.3121\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3090 - val_loss: 1.3122\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3090 - val_loss: 1.3124\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3089 - val_loss: 1.3121\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3089 - val_loss: 1.3119\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3090 - val_loss: 1.3132\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3090 - val_loss: 1.3122\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3088 - val_loss: 1.3123\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3087 - val_loss: 1.3117\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3088 - val_loss: 1.3120\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3087 - val_loss: 1.3115\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3088 - val_loss: 1.3159\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3090 - val_loss: 1.3131\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3087 - val_loss: 1.3120\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3086 - val_loss: 1.3116\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3085 - val_loss: 1.3115\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3086 - val_loss: 1.3117\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3087 - val_loss: 1.3117\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3084 - val_loss: 1.3118\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3085 - val_loss: 1.3115\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3084 - val_loss: 1.3114\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3083 - val_loss: 1.3111\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3083 - val_loss: 1.3112\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3082 - val_loss: 1.3109\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3082 - val_loss: 1.3111\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3085 - val_loss: 1.3109\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3084 - val_loss: 1.3110\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3082 - val_loss: 1.3109\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3082 - val_loss: 1.3115\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3081 - val_loss: 1.3111\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3082 - val_loss: 1.3108\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3080 - val_loss: 1.3109\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3081 - val_loss: 1.3108\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3081 - val_loss: 1.3111\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3080 - val_loss: 1.3106\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3083 - val_loss: 1.3121\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3080 - val_loss: 1.3112\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3079 - val_loss: 1.3107\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3079 - val_loss: 1.3109\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3080 - val_loss: 1.3106\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3078 - val_loss: 1.3104\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3078 - val_loss: 1.3108\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3079 - val_loss: 1.3107\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3079 - val_loss: 1.3107\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3078 - val_loss: 1.3110\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3078 - val_loss: 1.3106\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3078 - val_loss: 1.3105\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967993 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.86011106\n","The max value of N 0.87046385\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7021589999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 1ms/step - loss: 1.3639 - val_loss: 1.4011\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3360 - val_loss: 1.3606\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3309 - val_loss: 1.3481\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3284 - val_loss: 1.3408\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3267 - val_loss: 1.3363\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3256 - val_loss: 1.3335\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3246 - val_loss: 1.3322\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3240 - val_loss: 1.3310\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3234 - val_loss: 1.3303\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3229 - val_loss: 1.3284\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3225 - val_loss: 1.3295\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3222 - val_loss: 1.3287\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3219 - val_loss: 1.3284\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3216 - val_loss: 1.3283\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3213 - val_loss: 1.3269\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3212 - val_loss: 1.3271\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3210 - val_loss: 1.3268\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3206 - val_loss: 1.3251\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3204 - val_loss: 1.3258\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3201 - val_loss: 1.3268\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3198 - val_loss: 1.3260\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3196 - val_loss: 1.3255\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3196 - val_loss: 1.3248\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3194 - val_loss: 1.3248\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3193 - val_loss: 1.3247\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3192 - val_loss: 1.3251\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3192 - val_loss: 1.3244\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3191 - val_loss: 1.3254\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3191 - val_loss: 1.3235\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3190 - val_loss: 1.3247\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3184 - val_loss: 1.3231\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3184 - val_loss: 1.3238\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3182 - val_loss: 1.3226\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3181 - val_loss: 1.3226\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3182 - val_loss: 1.3219\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3179 - val_loss: 1.3228\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3178 - val_loss: 1.3224\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3178 - val_loss: 1.3217\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3178 - val_loss: 1.3224\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3176 - val_loss: 1.3219\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3176 - val_loss: 1.3223\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3176 - val_loss: 1.3220\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3175 - val_loss: 1.3219\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3173 - val_loss: 1.3218\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3172 - val_loss: 1.3216\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3172 - val_loss: 1.3210\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3171 - val_loss: 1.3213\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3170 - val_loss: 1.3207\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3171 - val_loss: 1.3213\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3169 - val_loss: 1.3219\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3169 - val_loss: 1.3206\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3167 - val_loss: 1.3212\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3165 - val_loss: 1.3206\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3165 - val_loss: 1.3202\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3165 - val_loss: 1.3205\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3165 - val_loss: 1.3200\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3163 - val_loss: 1.3203\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3164 - val_loss: 1.3200\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3162 - val_loss: 1.3208\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3160 - val_loss: 1.3200\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3161 - val_loss: 1.3195\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3161 - val_loss: 1.3193\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3159 - val_loss: 1.3192\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3160 - val_loss: 1.3197\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3159 - val_loss: 1.3196\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3158 - val_loss: 1.3193\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3158 - val_loss: 1.3193\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3157 - val_loss: 1.3194\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3157 - val_loss: 1.3189\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3156 - val_loss: 1.3190\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3157 - val_loss: 1.3190\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3157 - val_loss: 1.3188\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3156 - val_loss: 1.3195\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3155 - val_loss: 1.3206\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3155 - val_loss: 1.3192\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3153 - val_loss: 1.3188\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3152 - val_loss: 1.3184\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3153 - val_loss: 1.3191\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3153 - val_loss: 1.3183\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3152 - val_loss: 1.3188\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3151 - val_loss: 1.3183\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3150 - val_loss: 1.3185\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3149 - val_loss: 1.3183\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3149 - val_loss: 1.3193\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3151 - val_loss: 1.3186\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3148 - val_loss: 1.3183\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3149 - val_loss: 1.3186\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3148 - val_loss: 1.3178\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3147 - val_loss: 1.3177\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3146 - val_loss: 1.3178\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3146 - val_loss: 1.3183\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3147 - val_loss: 1.3179\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3146 - val_loss: 1.3178\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3145 - val_loss: 1.3177\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3145 - val_loss: 1.3175\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3145 - val_loss: 1.3179\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3145 - val_loss: 1.3173\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3144 - val_loss: 1.3178\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3143 - val_loss: 1.3172\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3142 - val_loss: 1.3191\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3144 - val_loss: 1.3178\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3141 - val_loss: 1.3173\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3142 - val_loss: 1.3172\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3142 - val_loss: 1.3177\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3141 - val_loss: 1.3176\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3141 - val_loss: 1.3170\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3140 - val_loss: 1.3168\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3140 - val_loss: 1.3167\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3139 - val_loss: 1.3171\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3140 - val_loss: 1.3169\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3139 - val_loss: 1.3169\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3139 - val_loss: 1.3170\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3139 - val_loss: 1.3170\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3138 - val_loss: 1.3164\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3137 - val_loss: 1.3168\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3136 - val_loss: 1.3165\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3136 - val_loss: 1.3165\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3136 - val_loss: 1.3165\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3136 - val_loss: 1.3166\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3137 - val_loss: 1.3169\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3135 - val_loss: 1.3167\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3136 - val_loss: 1.3161\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3135 - val_loss: 1.3163\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3137 - val_loss: 1.3168\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3134 - val_loss: 1.3167\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3134 - val_loss: 1.3164\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3134 - val_loss: 1.3163\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3135 - val_loss: 1.3169\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3134 - val_loss: 1.3165\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3133 - val_loss: 1.3161\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3134 - val_loss: 1.3170\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3132 - val_loss: 1.3161\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3158\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3132 - val_loss: 1.3157\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3131 - val_loss: 1.3155\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3131 - val_loss: 1.3161\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3131 - val_loss: 1.3159\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3131 - val_loss: 1.3159\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3130 - val_loss: 1.3156\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3130 - val_loss: 1.3156\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3130 - val_loss: 1.3156\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967992 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.91899073\n","The max value of N 0.858511\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7224945\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.3691 - val_loss: 1.4093\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 440us/step - loss: 1.3398 - val_loss: 1.3654\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3347 - val_loss: 1.3512\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3322 - val_loss: 1.3456\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3306 - val_loss: 1.3417\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3296 - val_loss: 1.3420\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3289 - val_loss: 1.3386\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3281 - val_loss: 1.3366\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3276 - val_loss: 1.3356\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3270 - val_loss: 1.3361\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3266 - val_loss: 1.3348\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3263 - val_loss: 1.3337\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3260 - val_loss: 1.3331\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3258 - val_loss: 1.3340\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3255 - val_loss: 1.3317\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3252 - val_loss: 1.3322\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3250 - val_loss: 1.3305\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3246 - val_loss: 1.3317\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3243 - val_loss: 1.3300\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3244 - val_loss: 1.3315\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3242 - val_loss: 1.3312\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3241 - val_loss: 1.3307\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3238 - val_loss: 1.3296\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3236 - val_loss: 1.3304\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3235 - val_loss: 1.3294\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3234 - val_loss: 1.3291\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3232 - val_loss: 1.3284\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3230 - val_loss: 1.3279\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3228 - val_loss: 1.3277\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3228 - val_loss: 1.3282\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3226 - val_loss: 1.3276\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3225 - val_loss: 1.3285\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3225 - val_loss: 1.3273\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3222 - val_loss: 1.3276\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3223 - val_loss: 1.3281\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3221 - val_loss: 1.3276\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3220 - val_loss: 1.3276\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3222 - val_loss: 1.3274\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3219 - val_loss: 1.3265\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3217 - val_loss: 1.3287\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3218 - val_loss: 1.3268\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3215 - val_loss: 1.3266\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3214 - val_loss: 1.3265\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3214 - val_loss: 1.3258\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3213 - val_loss: 1.3259\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3212 - val_loss: 1.3259\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3213 - val_loss: 1.3257\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3211 - val_loss: 1.3260\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3210 - val_loss: 1.3256\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3210 - val_loss: 1.3256\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3210 - val_loss: 1.3256\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3209 - val_loss: 1.3258\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3209 - val_loss: 1.3251\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3208 - val_loss: 1.3257\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3207 - val_loss: 1.3252\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3206 - val_loss: 1.3257\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3205 - val_loss: 1.3247\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3204 - val_loss: 1.3248\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3203 - val_loss: 1.3246\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3204 - val_loss: 1.3245\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3202 - val_loss: 1.3245\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3202 - val_loss: 1.3244\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3201 - val_loss: 1.3242\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3202 - val_loss: 1.3245\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3201 - val_loss: 1.3245\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3200 - val_loss: 1.3244\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3199 - val_loss: 1.3238\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3199 - val_loss: 1.3245\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3198 - val_loss: 1.3242\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3198 - val_loss: 1.3243\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3197 - val_loss: 1.3239\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3197 - val_loss: 1.3242\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3196 - val_loss: 1.3235\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3196 - val_loss: 1.3241\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3196 - val_loss: 1.3243\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3196 - val_loss: 1.3236\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3195 - val_loss: 1.3241\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3194 - val_loss: 1.3235\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3194 - val_loss: 1.3236\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3194 - val_loss: 1.3234\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3193 - val_loss: 1.3244\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3192 - val_loss: 1.3233\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3191 - val_loss: 1.3229\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3193 - val_loss: 1.3232\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3192 - val_loss: 1.3229\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3192 - val_loss: 1.3232\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3190 - val_loss: 1.3231\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3190 - val_loss: 1.3228\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3189 - val_loss: 1.3228\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3188 - val_loss: 1.3229\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3189 - val_loss: 1.3229\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3188 - val_loss: 1.3230\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3188 - val_loss: 1.3226\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3188 - val_loss: 1.3227\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3187 - val_loss: 1.3222\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3186 - val_loss: 1.3230\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3186 - val_loss: 1.3226\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3185 - val_loss: 1.3231\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3185 - val_loss: 1.3221\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3185 - val_loss: 1.3221\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3185 - val_loss: 1.3222\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3185 - val_loss: 1.3221\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3184 - val_loss: 1.3221\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3183 - val_loss: 1.3218\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3183 - val_loss: 1.3219\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3185 - val_loss: 1.3221\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3182 - val_loss: 1.3224\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3183 - val_loss: 1.3219\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3181 - val_loss: 1.3220\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3181 - val_loss: 1.3216\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3180 - val_loss: 1.3216\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3181 - val_loss: 1.3217\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3180 - val_loss: 1.3216\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3180 - val_loss: 1.3218\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3179 - val_loss: 1.3216\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3181 - val_loss: 1.3214\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3179 - val_loss: 1.3214\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3178 - val_loss: 1.3216\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3178 - val_loss: 1.3211\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3178 - val_loss: 1.3215\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3178 - val_loss: 1.3224\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3178 - val_loss: 1.3216\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3177 - val_loss: 1.3214\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3177 - val_loss: 1.3211\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3177 - val_loss: 1.3212\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3177 - val_loss: 1.3212\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3176 - val_loss: 1.3210\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3175 - val_loss: 1.3206\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3174 - val_loss: 1.3208\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3176 - val_loss: 1.3209\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3176 - val_loss: 1.3214\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3176 - val_loss: 1.3209\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3175 - val_loss: 1.3211\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3174 - val_loss: 1.3212\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3173 - val_loss: 1.3208\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3173 - val_loss: 1.3206\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3172 - val_loss: 1.3211\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3172 - val_loss: 1.3207\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3172 - val_loss: 1.3207\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3172 - val_loss: 1.3204\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3173 - val_loss: 1.3203\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3172 - val_loss: 1.3203\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3171 - val_loss: 1.3203\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3204\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3172 - val_loss: 1.3207\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3171 - val_loss: 1.3205\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3170 - val_loss: 1.3203\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3170 - val_loss: 1.3202\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3170 - val_loss: 1.3203\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3171 - val_loss: 1.3206\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967990 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.85542846\n","The max value of N 0.9390134\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7365636666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.3625 - val_loss: 1.3937\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 469us/step - loss: 1.3356 - val_loss: 1.3542\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3309 - val_loss: 1.3435\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 409us/step - loss: 1.3284 - val_loss: 1.3403\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3271 - val_loss: 1.3370\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3259 - val_loss: 1.3342\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3248 - val_loss: 1.3318\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3242 - val_loss: 1.3338\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3236 - val_loss: 1.3312\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3234 - val_loss: 1.3291\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3227 - val_loss: 1.3305\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3222 - val_loss: 1.3291\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3218 - val_loss: 1.3298\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3219 - val_loss: 1.3275\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3216 - val_loss: 1.3292\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3211 - val_loss: 1.3266\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3208 - val_loss: 1.3269\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3206 - val_loss: 1.3252\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3205 - val_loss: 1.3253\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3207 - val_loss: 1.3273\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3202 - val_loss: 1.3255\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3198 - val_loss: 1.3252\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3198 - val_loss: 1.3250\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3197 - val_loss: 1.3282\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3195 - val_loss: 1.3238\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3194 - val_loss: 1.3243\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3192 - val_loss: 1.3261\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3192 - val_loss: 1.3246\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3190 - val_loss: 1.3243\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3188 - val_loss: 1.3230\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3187 - val_loss: 1.3228\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3184 - val_loss: 1.3223\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3184 - val_loss: 1.3226\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3183 - val_loss: 1.3225\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3182 - val_loss: 1.3218\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3181 - val_loss: 1.3224\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3180 - val_loss: 1.3236\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3179 - val_loss: 1.3216\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3180 - val_loss: 1.3220\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3178 - val_loss: 1.3219\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3177 - val_loss: 1.3216\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3176 - val_loss: 1.3217\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3174 - val_loss: 1.3216\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3173 - val_loss: 1.3231\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3176 - val_loss: 1.3211\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3173 - val_loss: 1.3238\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3172 - val_loss: 1.3228\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3172 - val_loss: 1.3211\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3170 - val_loss: 1.3211\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3169 - val_loss: 1.3212\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3169 - val_loss: 1.3200\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3168 - val_loss: 1.3202\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3167 - val_loss: 1.3209\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3166 - val_loss: 1.3197\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3168 - val_loss: 1.3230\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3167 - val_loss: 1.3214\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3165 - val_loss: 1.3202\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3165 - val_loss: 1.3194\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3163 - val_loss: 1.3197\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3163 - val_loss: 1.3197\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3161 - val_loss: 1.3192\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3162 - val_loss: 1.3188\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3160 - val_loss: 1.3189\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3161 - val_loss: 1.3190\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3160 - val_loss: 1.3199\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3159 - val_loss: 1.3193\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3160 - val_loss: 1.3194\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3159 - val_loss: 1.3190\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3158 - val_loss: 1.3190\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3158 - val_loss: 1.3186\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3157 - val_loss: 1.3187\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3156 - val_loss: 1.3184\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3155 - val_loss: 1.3187\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3154 - val_loss: 1.3184\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3154 - val_loss: 1.3189\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3153 - val_loss: 1.3184\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3154 - val_loss: 1.3181\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3153 - val_loss: 1.3186\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3155 - val_loss: 1.3183\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3152 - val_loss: 1.3184\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3152 - val_loss: 1.3185\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3151 - val_loss: 1.3184\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3151 - val_loss: 1.3180\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3151 - val_loss: 1.3187\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3151 - val_loss: 1.3178\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3151 - val_loss: 1.3180\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3150 - val_loss: 1.3176\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3150 - val_loss: 1.3176\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3149 - val_loss: 1.3176\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3149 - val_loss: 1.3177\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3147 - val_loss: 1.3174\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3148 - val_loss: 1.3175\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3148 - val_loss: 1.3178\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3148 - val_loss: 1.3178\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3146 - val_loss: 1.3174\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3145 - val_loss: 1.3173\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3146 - val_loss: 1.3172\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3144 - val_loss: 1.3173\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3145 - val_loss: 1.3173\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3144 - val_loss: 1.3176\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3145 - val_loss: 1.3175\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3144 - val_loss: 1.3168\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3142 - val_loss: 1.3175\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3142 - val_loss: 1.3167\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3142 - val_loss: 1.3168\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3141 - val_loss: 1.3167\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3142 - val_loss: 1.3170\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3141 - val_loss: 1.3166\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3141 - val_loss: 1.3168\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3139 - val_loss: 1.3167\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3140 - val_loss: 1.3167\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3139 - val_loss: 1.3165\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3138 - val_loss: 1.3170\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3139 - val_loss: 1.3166\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3138 - val_loss: 1.3164\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3139 - val_loss: 1.3164\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3138 - val_loss: 1.3162\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3138 - val_loss: 1.3164\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3136 - val_loss: 1.3173\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3137 - val_loss: 1.3161\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3135 - val_loss: 1.3165\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3135 - val_loss: 1.3162\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3135 - val_loss: 1.3164\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3135 - val_loss: 1.3161\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3134 - val_loss: 1.3160\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3134 - val_loss: 1.3157\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3133 - val_loss: 1.3157\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3133 - val_loss: 1.3159\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3135 - val_loss: 1.3162\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3134 - val_loss: 1.3161\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3158\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3132 - val_loss: 1.3158\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3132 - val_loss: 1.3158\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3157\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3131 - val_loss: 1.3160\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3130 - val_loss: 1.3157\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3131 - val_loss: 1.3155\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3131 - val_loss: 1.3154\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3130 - val_loss: 1.3156\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3130 - val_loss: 1.3157\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3129 - val_loss: 1.3155\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3128 - val_loss: 1.3154\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3128 - val_loss: 1.3154\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3130 - val_loss: 1.3154\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3129 - val_loss: 1.3157\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967993 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8776515\n","The max value of N 0.877817\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7077393333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 12s 2ms/step - loss: 1.3639 - val_loss: 1.3800\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3359 - val_loss: 1.3572\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 434us/step - loss: 1.3302 - val_loss: 1.3429\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 400us/step - loss: 1.3278 - val_loss: 1.3373\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3266 - val_loss: 1.3383\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3252 - val_loss: 1.3372\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3243 - val_loss: 1.3328\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3237 - val_loss: 1.3338\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3233 - val_loss: 1.3311\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3231 - val_loss: 1.3301\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3226 - val_loss: 1.3320\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3221 - val_loss: 1.3294\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3219 - val_loss: 1.3290\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3218 - val_loss: 1.3289\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3212 - val_loss: 1.3269\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3208 - val_loss: 1.3267\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3208 - val_loss: 1.3263\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3205 - val_loss: 1.3268\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3203 - val_loss: 1.3266\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3199 - val_loss: 1.3255\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3197 - val_loss: 1.3259\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3197 - val_loss: 1.3245\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3194 - val_loss: 1.3241\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3193 - val_loss: 1.3245\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3191 - val_loss: 1.3254\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3192 - val_loss: 1.3246\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3190 - val_loss: 1.3242\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3186 - val_loss: 1.3249\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3186 - val_loss: 1.3233\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3185 - val_loss: 1.3248\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3185 - val_loss: 1.3245\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3184 - val_loss: 1.3241\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3181 - val_loss: 1.3233\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3182 - val_loss: 1.3229\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3179 - val_loss: 1.3235\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3178 - val_loss: 1.3224\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3178 - val_loss: 1.3225\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3179 - val_loss: 1.3224\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3177 - val_loss: 1.3228\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3175 - val_loss: 1.3221\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3175 - val_loss: 1.3220\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3173 - val_loss: 1.3220\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3172 - val_loss: 1.3213\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3172 - val_loss: 1.3212\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3174 - val_loss: 1.3216\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3169 - val_loss: 1.3215\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3169 - val_loss: 1.3217\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3168 - val_loss: 1.3204\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3168 - val_loss: 1.3210\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3167 - val_loss: 1.3203\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3166 - val_loss: 1.3206\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3166 - val_loss: 1.3206\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3167 - val_loss: 1.3211\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3164 - val_loss: 1.3211\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3165 - val_loss: 1.3205\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3163 - val_loss: 1.3200\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3162 - val_loss: 1.3200\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3162 - val_loss: 1.3198\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3160 - val_loss: 1.3210\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3160 - val_loss: 1.3196\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3161 - val_loss: 1.3199\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3160 - val_loss: 1.3193\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3158 - val_loss: 1.3199\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3157 - val_loss: 1.3199\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3157 - val_loss: 1.3193\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3156 - val_loss: 1.3190\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3157 - val_loss: 1.3192\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3156 - val_loss: 1.3196\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3155 - val_loss: 1.3201\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3153 - val_loss: 1.3189\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3154 - val_loss: 1.3214\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3160 - val_loss: 1.3227\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3154 - val_loss: 1.3221\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3153 - val_loss: 1.3200\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3153 - val_loss: 1.3190\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3153 - val_loss: 1.3191\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3152 - val_loss: 1.3189\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3151 - val_loss: 1.3191\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3150 - val_loss: 1.3187\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3149 - val_loss: 1.3182\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3149 - val_loss: 1.3182\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3151 - val_loss: 1.3181\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3149 - val_loss: 1.3183\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3148 - val_loss: 1.3179\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3147 - val_loss: 1.3189\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3147 - val_loss: 1.3180\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3145 - val_loss: 1.3177\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3145 - val_loss: 1.3181\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3146 - val_loss: 1.3179\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3146 - val_loss: 1.3180\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3144 - val_loss: 1.3177\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3144 - val_loss: 1.3175\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3143 - val_loss: 1.3177\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3142 - val_loss: 1.3173\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3142 - val_loss: 1.3178\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3142 - val_loss: 1.3176\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3142 - val_loss: 1.3179\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3141 - val_loss: 1.3173\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3140 - val_loss: 1.3173\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3142 - val_loss: 1.3178\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3141 - val_loss: 1.3174\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3140 - val_loss: 1.3168\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3139 - val_loss: 1.3168\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3139 - val_loss: 1.3170\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3138 - val_loss: 1.3171\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3139 - val_loss: 1.3173\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3138 - val_loss: 1.3167\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3138 - val_loss: 1.3172\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3137 - val_loss: 1.3167\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3138 - val_loss: 1.3187\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3137 - val_loss: 1.3175\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3137 - val_loss: 1.3167\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3136 - val_loss: 1.3168\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3136 - val_loss: 1.3173\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3135 - val_loss: 1.3163\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3135 - val_loss: 1.3162\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3134 - val_loss: 1.3162\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3134 - val_loss: 1.3164\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3134 - val_loss: 1.3161\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3133 - val_loss: 1.3162\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3133 - val_loss: 1.3162\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3136 - val_loss: 1.3166\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3133 - val_loss: 1.3165\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3134 - val_loss: 1.3160\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3133 - val_loss: 1.3165\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3132 - val_loss: 1.3161\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3131 - val_loss: 1.3165\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3132 - val_loss: 1.3167\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3131 - val_loss: 1.3158\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3131 - val_loss: 1.3161\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3131 - val_loss: 1.3159\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3131 - val_loss: 1.3160\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3130 - val_loss: 1.3157\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 398us/step - loss: 1.3129 - val_loss: 1.3155\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3129 - val_loss: 1.3157\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3128 - val_loss: 1.3155\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3128 - val_loss: 1.3156\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3127 - val_loss: 1.3155\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3128 - val_loss: 1.3157\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3127 - val_loss: 1.3156\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3128 - val_loss: 1.3156\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3127 - val_loss: 1.3153\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 398us/step - loss: 1.3127 - val_loss: 1.3153\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3127 - val_loss: 1.3152\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3127 - val_loss: 1.3154\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3126 - val_loss: 1.3153\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3126 - val_loss: 1.3154\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3126 - val_loss: 1.3158\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967990 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.99680436\n","The max value of N 0.84160894\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7114320000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 0\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 13s 2ms/step - loss: 1.3637 - val_loss: 1.3906\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 516us/step - loss: 1.3341 - val_loss: 1.3637\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 485us/step - loss: 1.3296 - val_loss: 1.3435\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 473us/step - loss: 1.3274 - val_loss: 1.3377\n","Epoch 5/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3260 - val_loss: 1.3345\n","Epoch 6/150\n","5850/5850 [==============================] - 3s 439us/step - loss: 1.3249 - val_loss: 1.3324\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3240 - val_loss: 1.3312\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 406us/step - loss: 1.3233 - val_loss: 1.3312\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3228 - val_loss: 1.3291\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3223 - val_loss: 1.3281\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3219 - val_loss: 1.3292\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3215 - val_loss: 1.3275\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3211 - val_loss: 1.3268\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3209 - val_loss: 1.3271\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3205 - val_loss: 1.3256\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3203 - val_loss: 1.3258\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3202 - val_loss: 1.3251\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3199 - val_loss: 1.3258\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3198 - val_loss: 1.3258\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3195 - val_loss: 1.3249\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3195 - val_loss: 1.3242\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3192 - val_loss: 1.3235\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3190 - val_loss: 1.3237\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3190 - val_loss: 1.3239\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3188 - val_loss: 1.3242\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3186 - val_loss: 1.3246\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3184 - val_loss: 1.3225\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3184 - val_loss: 1.3229\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3181 - val_loss: 1.3220\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3180 - val_loss: 1.3224\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3180 - val_loss: 1.3230\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3179 - val_loss: 1.3226\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3178 - val_loss: 1.3222\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3178 - val_loss: 1.3221\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3176 - val_loss: 1.3214\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3175 - val_loss: 1.3219\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 398us/step - loss: 1.3174 - val_loss: 1.3213\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3173 - val_loss: 1.3212\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3173 - val_loss: 1.3220\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3171 - val_loss: 1.3212\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3170 - val_loss: 1.3210\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3171 - val_loss: 1.3213\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3170 - val_loss: 1.3213\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 399us/step - loss: 1.3168 - val_loss: 1.3205\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3168 - val_loss: 1.3207\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3168 - val_loss: 1.3206\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3167 - val_loss: 1.3210\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3166 - val_loss: 1.3214\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3166 - val_loss: 1.3207\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3165 - val_loss: 1.3202\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3163 - val_loss: 1.3221\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 398us/step - loss: 1.3164 - val_loss: 1.3203\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3162 - val_loss: 1.3198\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3160 - val_loss: 1.3197\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3160 - val_loss: 1.3206\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3159 - val_loss: 1.3196\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3160 - val_loss: 1.3192\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3158 - val_loss: 1.3201\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3158 - val_loss: 1.3193\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3157 - val_loss: 1.3191\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3156 - val_loss: 1.3190\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3155 - val_loss: 1.3197\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3157 - val_loss: 1.3189\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3155 - val_loss: 1.3188\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3154 - val_loss: 1.3192\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3153 - val_loss: 1.3187\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3153 - val_loss: 1.3186\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3152 - val_loss: 1.3187\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3153 - val_loss: 1.3184\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3151 - val_loss: 1.3187\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3151 - val_loss: 1.3188\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3150 - val_loss: 1.3189\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3149 - val_loss: 1.3180\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3149 - val_loss: 1.3181\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3148 - val_loss: 1.3184\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3148 - val_loss: 1.3180\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3148 - val_loss: 1.3192\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3149 - val_loss: 1.3182\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3149 - val_loss: 1.3181\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3147 - val_loss: 1.3181\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3147 - val_loss: 1.3190\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3145 - val_loss: 1.3178\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3145 - val_loss: 1.3179\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3144 - val_loss: 1.3182\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3144 - val_loss: 1.3176\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3144 - val_loss: 1.3178\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3143 - val_loss: 1.3173\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3143 - val_loss: 1.3173\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3143 - val_loss: 1.3175\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3143 - val_loss: 1.3175\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3141 - val_loss: 1.3175\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3141 - val_loss: 1.3175\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3141 - val_loss: 1.3177\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3141 - val_loss: 1.3172\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3140 - val_loss: 1.3176\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3140 - val_loss: 1.3170\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3140 - val_loss: 1.3173\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3138 - val_loss: 1.3182\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3138 - val_loss: 1.3168\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3138 - val_loss: 1.3179\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3136 - val_loss: 1.3171\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3138 - val_loss: 1.3172\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3136 - val_loss: 1.3171\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3136 - val_loss: 1.3167\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3136 - val_loss: 1.3172\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3135 - val_loss: 1.3167\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3135 - val_loss: 1.3165\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3134 - val_loss: 1.3163\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3136 - val_loss: 1.3163\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3135 - val_loss: 1.3172\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3134 - val_loss: 1.3164\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3132 - val_loss: 1.3162\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3132 - val_loss: 1.3161\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3132 - val_loss: 1.3163\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3133 - val_loss: 1.3160\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3132 - val_loss: 1.3160\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3131 - val_loss: 1.3163\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3131 - val_loss: 1.3163\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3132 - val_loss: 1.3161\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3131 - val_loss: 1.3157\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 398us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3131 - val_loss: 1.3166\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3131 - val_loss: 1.3160\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3129 - val_loss: 1.3156\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3129 - val_loss: 1.3156\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3128 - val_loss: 1.3154\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3128 - val_loss: 1.3157\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3129 - val_loss: 1.3156\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3128 - val_loss: 1.3155\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3128 - val_loss: 1.3156\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3128 - val_loss: 1.3156\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3128 - val_loss: 1.3156\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3127 - val_loss: 1.3155\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3127 - val_loss: 1.3154\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3126 - val_loss: 1.3155\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3126 - val_loss: 1.3153\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3126 - val_loss: 1.3152\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3126 - val_loss: 1.3154\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3125 - val_loss: 1.3151\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3126 - val_loss: 1.3154\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3127 - val_loss: 1.3152\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3125 - val_loss: 1.3149\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3124 - val_loss: 1.3154\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967991 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.90453506\n","The max value of N 0.81644994\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7141545\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.712608, 0.6943535000000001, 0.7105566666666666, 0.7093200000000001, 0.7021589999999999, 0.7224945, 0.7365636666666666, 0.7077393333333334, 0.7114320000000001, 0.7141545]\n","AUROC ===== 0.7121381166666667 +/- 0.010743904231146859\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAIinAEA8BQAiCgBEFACIKAAQUQAgF2RyX0Q2irs8AAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7f8e7a60ef98>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"b7YYU9jHWLRx","colab_type":"text"},"cell_type":"markdown","source":["##**Automobile  vs All **##"]},{"metadata":{"id":"hX7lqgyjWKyO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":64319},"outputId":"5c611b2c-7ace-423c-c29b-61691bc03b63","executionInfo":{"status":"ok","timestamp":1541671373525,"user_tz":-660,"elapsed":3740384,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/DCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/DCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 992us/step - loss: 1.4866 - val_loss: 2.1300\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 520us/step - loss: 1.3471 - val_loss: 2.1412\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 511us/step - loss: 1.3290 - val_loss: 1.3459\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 510us/step - loss: 1.3196 - val_loss: 1.3099\n","Epoch 5/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3158 - val_loss: 1.3114\n","Epoch 6/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3119 - val_loss: 1.3085\n","Epoch 7/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3097 - val_loss: 1.3080\n","Epoch 8/150\n","5850/5850 [==============================] - 3s 502us/step - loss: 1.3095 - val_loss: 1.3072\n","Epoch 9/150\n","5850/5850 [==============================] - 3s 501us/step - loss: 1.3085 - val_loss: 1.3069\n","Epoch 10/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3076 - val_loss: 1.3066\n","Epoch 11/150\n","5850/5850 [==============================] - 3s 499us/step - loss: 1.3072 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 3s 494us/step - loss: 1.3068 - val_loss: 1.3063\n","Epoch 13/150\n","5850/5850 [==============================] - 3s 501us/step - loss: 1.3069 - val_loss: 1.3062\n","Epoch 14/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3083 - val_loss: 1.3085\n","Epoch 15/150\n","5850/5850 [==============================] - 3s 499us/step - loss: 1.3071 - val_loss: 1.3067\n","Epoch 16/150\n","5850/5850 [==============================] - 3s 504us/step - loss: 1.3066 - val_loss: 1.3064\n","Epoch 17/150\n","5850/5850 [==============================] - 3s 486us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 18/150\n","5850/5850 [==============================] - 3s 494us/step - loss: 1.3064 - val_loss: 1.3093\n","Epoch 19/150\n","5850/5850 [==============================] - 3s 493us/step - loss: 1.3062 - val_loss: 1.3075\n","Epoch 20/150\n","5850/5850 [==============================] - 3s 491us/step - loss: 1.3061 - val_loss: 1.3073\n","Epoch 21/150\n","5850/5850 [==============================] - 3s 493us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 22/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 23/150\n","5850/5850 [==============================] - 3s 497us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 24/150\n","5850/5850 [==============================] - 3s 491us/step - loss: 1.3078 - val_loss: 1.3074\n","Epoch 25/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3074 - val_loss: 1.3127\n","Epoch 26/150\n","5850/5850 [==============================] - 3s 497us/step - loss: 1.3068 - val_loss: 1.3087\n","Epoch 27/150\n","5850/5850 [==============================] - 3s 504us/step - loss: 1.3064 - val_loss: 1.3076\n","Epoch 28/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3063 - val_loss: 1.3071\n","Epoch 29/150\n","5850/5850 [==============================] - 3s 499us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 30/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3064 - val_loss: 1.3153\n","Epoch 31/150\n","5850/5850 [==============================] - 3s 503us/step - loss: 1.3062 - val_loss: 1.3136\n","Epoch 32/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3061 - val_loss: 1.3076\n","Epoch 33/150\n","5850/5850 [==============================] - 3s 485us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 34/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3062 - val_loss: 1.3067\n","Epoch 35/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3063 - val_loss: 1.3066\n","Epoch 36/150\n","5850/5850 [==============================] - 3s 489us/step - loss: 1.3062 - val_loss: 1.3077\n","Epoch 37/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 38/150\n","5850/5850 [==============================] - 3s 490us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 39/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 40/150\n","5850/5850 [==============================] - 3s 490us/step - loss: 1.3060 - val_loss: 1.3070\n","Epoch 41/150\n","5850/5850 [==============================] - 3s 494us/step - loss: 1.3060 - val_loss: 1.3098\n","Epoch 42/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3060 - val_loss: 1.3113\n","Epoch 43/150\n","5850/5850 [==============================] - 3s 488us/step - loss: 1.3060 - val_loss: 1.3114\n","Epoch 44/150\n","5850/5850 [==============================] - 3s 489us/step - loss: 1.3060 - val_loss: 1.3116\n","Epoch 45/150\n","5850/5850 [==============================] - 3s 495us/step - loss: 1.3060 - val_loss: 1.3091\n","Epoch 46/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3060 - val_loss: 1.3089\n","Epoch 47/150\n","5850/5850 [==============================] - 3s 492us/step - loss: 1.3060 - val_loss: 1.3099\n","Epoch 48/150\n","5850/5850 [==============================] - 3s 494us/step - loss: 1.3060 - val_loss: 1.3114\n","Epoch 49/150\n","5850/5850 [==============================] - 3s 493us/step - loss: 1.3059 - val_loss: 1.3096\n","Epoch 50/150\n","5850/5850 [==============================] - 3s 494us/step - loss: 1.3059 - val_loss: 1.3105\n","Epoch 51/150\n","5850/5850 [==============================] - 3s 494us/step - loss: 1.3059 - val_loss: 1.3111\n","Epoch 52/150\n","5850/5850 [==============================] - 3s 493us/step - loss: 1.3059 - val_loss: 1.3120\n","Epoch 53/150\n","5850/5850 [==============================] - 3s 492us/step - loss: 1.3059 - val_loss: 1.3124\n","Epoch 54/150\n","5850/5850 [==============================] - 3s 501us/step - loss: 1.3059 - val_loss: 1.3126\n","Epoch 55/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3059 - val_loss: 1.3105\n","Epoch 56/150\n","5850/5850 [==============================] - 3s 495us/step - loss: 1.3059 - val_loss: 1.3099\n","Epoch 57/150\n","5850/5850 [==============================] - 3s 494us/step - loss: 1.3059 - val_loss: 1.3105\n","Epoch 58/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3059 - val_loss: 1.3107\n","Epoch 59/150\n","5850/5850 [==============================] - 3s 502us/step - loss: 1.3059 - val_loss: 1.3109\n","Epoch 60/150\n","5850/5850 [==============================] - 3s 515us/step - loss: 1.3059 - val_loss: 1.3107\n","Epoch 61/150\n","5850/5850 [==============================] - 3s 521us/step - loss: 1.3059 - val_loss: 1.3107\n","Epoch 62/150\n","5850/5850 [==============================] - 3s 507us/step - loss: 1.3059 - val_loss: 1.3114\n","Epoch 63/150\n","5850/5850 [==============================] - 3s 508us/step - loss: 1.3059 - val_loss: 1.3113\n","Epoch 64/150\n","5850/5850 [==============================] - 3s 520us/step - loss: 1.3059 - val_loss: 1.3105\n","Epoch 65/150\n","5850/5850 [==============================] - 3s 510us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 66/150\n","5850/5850 [==============================] - 3s 513us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 67/150\n","5850/5850 [==============================] - 3s 512us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 68/150\n","5850/5850 [==============================] - 3s 510us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 69/150\n","5850/5850 [==============================] - 3s 511us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 70/150\n","5850/5850 [==============================] - 3s 520us/step - loss: 1.3059 - val_loss: 1.3072\n","Epoch 71/150\n","5850/5850 [==============================] - 3s 545us/step - loss: 1.3059 - val_loss: 1.3075\n","Epoch 72/150\n","5850/5850 [==============================] - 3s 531us/step - loss: 1.3059 - val_loss: 1.3079\n","Epoch 73/150\n","5850/5850 [==============================] - 3s 532us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 74/150\n","5850/5850 [==============================] - 3s 531us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 75/150\n","5850/5850 [==============================] - 3s 527us/step - loss: 1.3059 - val_loss: 1.3107\n","Epoch 76/150\n","5850/5850 [==============================] - 3s 533us/step - loss: 1.3059 - val_loss: 1.3073\n","Epoch 77/150\n","5850/5850 [==============================] - 3s 531us/step - loss: 1.3059 - val_loss: 1.3071\n","Epoch 78/150\n","5850/5850 [==============================] - 3s 524us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 79/150\n","5850/5850 [==============================] - 3s 525us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 80/150\n","5850/5850 [==============================] - 3s 525us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 81/150\n","5850/5850 [==============================] - 3s 515us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 82/150\n","5850/5850 [==============================] - 3s 513us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 83/150\n","5850/5850 [==============================] - 3s 510us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 84/150\n","5850/5850 [==============================] - 3s 516us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 85/150\n","5850/5850 [==============================] - 3s 508us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 86/150\n","5850/5850 [==============================] - 3s 511us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 87/150\n","5850/5850 [==============================] - 3s 512us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 88/150\n","5850/5850 [==============================] - 3s 515us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 89/150\n","5850/5850 [==============================] - 3s 513us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 90/150\n","5850/5850 [==============================] - 3s 511us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 91/150\n","5850/5850 [==============================] - 3s 512us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 92/150\n","5850/5850 [==============================] - 3s 521us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 93/150\n","5850/5850 [==============================] - 3s 511us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 94/150\n","5850/5850 [==============================] - 3s 508us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 95/150\n","5850/5850 [==============================] - 3s 509us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 96/150\n","5850/5850 [==============================] - 3s 513us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 97/150\n","5850/5850 [==============================] - 3s 512us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 98/150\n","5850/5850 [==============================] - 3s 512us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 99/150\n","5850/5850 [==============================] - 3s 507us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 100/150\n","5850/5850 [==============================] - 3s 507us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 101/150\n","5850/5850 [==============================] - 3s 514us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 102/150\n","5850/5850 [==============================] - 3s 509us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 103/150\n","5850/5850 [==============================] - 3s 508us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 104/150\n","5850/5850 [==============================] - 3s 512us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 105/150\n","5850/5850 [==============================] - 3s 509us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 106/150\n","5850/5850 [==============================] - 3s 508us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 107/150\n","5850/5850 [==============================] - 3s 508us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 108/150\n","5850/5850 [==============================] - 3s 511us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 109/150\n","5850/5850 [==============================] - 3s 513us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 110/150\n","5850/5850 [==============================] - 3s 510us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 111/150\n","5850/5850 [==============================] - 3s 512us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 112/150\n","5850/5850 [==============================] - 3s 497us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 113/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 114/150\n","5850/5850 [==============================] - 3s 501us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 115/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 116/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 117/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 118/150\n","5850/5850 [==============================] - 3s 501us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 119/150\n","5850/5850 [==============================] - 3s 498us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 120/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 121/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 122/150\n","5850/5850 [==============================] - 3s 502us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 123/150\n","5850/5850 [==============================] - 3s 496us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 124/150\n","5850/5850 [==============================] - 3s 502us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 125/150\n","5850/5850 [==============================] - 3s 506us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 126/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 127/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 128/150\n","5850/5850 [==============================] - 3s 504us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 129/150\n","5850/5850 [==============================] - 3s 511us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 130/150\n","5850/5850 [==============================] - 3s 502us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 131/150\n","5850/5850 [==============================] - 3s 500us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 132/150\n","5850/5850 [==============================] - 3s 505us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 133/150\n","5850/5850 [==============================] - 3s 505us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 134/150\n","5850/5850 [==============================] - 3s 503us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 135/150\n","5850/5850 [==============================] - 3s 510us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 136/150\n","5850/5850 [==============================] - 3s 510us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 137/150\n","5850/5850 [==============================] - 3s 504us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 138/150\n","5850/5850 [==============================] - 3s 506us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 139/150\n","5850/5850 [==============================] - 3s 504us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 140/150\n","5850/5850 [==============================] - 3s 501us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 141/150\n","5850/5850 [==============================] - 3s 513us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 142/150\n","5850/5850 [==============================] - 3s 527us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 143/150\n","5850/5850 [==============================] - 3s 525us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 144/150\n","5850/5850 [==============================] - 3s 527us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 145/150\n","5850/5850 [==============================] - 3s 526us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 146/150\n","5850/5850 [==============================] - 3s 529us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 147/150\n","5850/5850 [==============================] - 3s 525us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 148/150\n","5850/5850 [==============================] - 3s 528us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 149/150\n","5850/5850 [==============================] - 3s 523us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 150/150\n","5850/5850 [==============================] - 3s 523us/step - loss: 1.3059 - val_loss: 1.3059\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0471354\n","The max value of N 0.12041207\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6382789999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 839us/step - loss: 1.4829 - val_loss: 1.6562\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3491 - val_loss: 1.9065\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3247 - val_loss: 1.5362\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3210 - val_loss: 1.4326\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3134 - val_loss: 1.3225\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3107 - val_loss: 1.3079\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3121 - val_loss: 1.3737\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3061\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3089 - val_loss: 1.3064\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3080 - val_loss: 1.3064\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3075 - val_loss: 1.3068\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3075 - val_loss: 1.3067\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3065\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3064 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3067\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3071 - val_loss: 1.3099\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3065 - val_loss: 1.3083\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3062 - val_loss: 1.3072\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3071\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3072\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3071\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3076\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3073\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3083\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3089\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3068\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3071\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3071\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3057 - val_loss: 1.3080\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3070\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3083\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3077\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3076\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.029014\n","The max value of N 0.110171154\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6589713333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 937us/step - loss: 1.4998 - val_loss: 1.6985\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 425us/step - loss: 1.3549 - val_loss: 1.6464\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3261 - val_loss: 1.3193\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3194 - val_loss: 1.3130\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3124 - val_loss: 1.3049\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3094 - val_loss: 1.3042\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3078 - val_loss: 1.3040\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3085 - val_loss: 1.3057\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3067 - val_loss: 1.3046\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3042\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3048 - val_loss: 1.3038\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3045 - val_loss: 1.3039\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3044 - val_loss: 1.3039\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3052 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3046 - val_loss: 1.3049\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3042 - val_loss: 1.3041\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3044 - val_loss: 1.3052\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3041 - val_loss: 1.3040\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3040 - val_loss: 1.3038\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3043 - val_loss: 1.3069\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3041 - val_loss: 1.3054\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3040 - val_loss: 1.3046\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3039 - val_loss: 1.3049\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3039 - val_loss: 1.3045\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3038 - val_loss: 1.3039\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3039 - val_loss: 1.3051\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3038 - val_loss: 1.3040\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3038\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3037 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3036 - val_loss: 1.3049\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3045\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3048\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3039\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3039\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0155929\n","The max value of N 0.107942544\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6502976666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.5250 - val_loss: 2.0305\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 430us/step - loss: 1.3635 - val_loss: 1.8988\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3297 - val_loss: 1.5299\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3221 - val_loss: 1.3121\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3163 - val_loss: 1.3081\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3082\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3150 - val_loss: 1.3095\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3110 - val_loss: 1.3080\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3097 - val_loss: 1.3069\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3086 - val_loss: 1.3066\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3079 - val_loss: 1.3066\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3074 - val_loss: 1.3065\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3071 - val_loss: 1.3064\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3070 - val_loss: 1.3064\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3067 - val_loss: 1.3064\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3067 - val_loss: 1.3064\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3070 - val_loss: 1.3096\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3089 - val_loss: 1.3581\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3076 - val_loss: 1.3125\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3070 - val_loss: 1.3075\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3068 - val_loss: 1.3070\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3067 - val_loss: 1.3073\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3067 - val_loss: 1.3078\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3067 - val_loss: 1.3072\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3068\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3069\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3068 - val_loss: 1.3082\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3065 - val_loss: 1.3072\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3065 - val_loss: 1.3067\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3066\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3069\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3062 - val_loss: 1.3068\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3067\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3062 - val_loss: 1.3068\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3069\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3061 - val_loss: 1.3069\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3061 - val_loss: 1.3070\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0395365\n","The max value of N 0.13664559\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5999641666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4930 - val_loss: 1.6522\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3495 - val_loss: 1.4261\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3215 - val_loss: 1.3156\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3116 - val_loss: 1.3066\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3104 - val_loss: 1.3042\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3012\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3039 - val_loss: 1.3014\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3026 - val_loss: 1.3011\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3025 - val_loss: 1.3022\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3020 - val_loss: 1.3017\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3015 - val_loss: 1.3011\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3024 - val_loss: 1.3008\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3013 - val_loss: 1.3006\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3010 - val_loss: 1.3006\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3009 - val_loss: 1.3006\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3009 - val_loss: 1.3007\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3008 - val_loss: 1.3007\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3007 - val_loss: 1.3006\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3007 - val_loss: 1.3006\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3007 - val_loss: 1.3007\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3007 - val_loss: 1.3006\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3007 - val_loss: 1.3010\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3013 - val_loss: 1.3016\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3008 - val_loss: 1.3006\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3007 - val_loss: 1.3005\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.47389722\n","The max value of N 0.10831204\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6133811666666668\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4726 - val_loss: 1.8029\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 423us/step - loss: 1.3489 - val_loss: 2.1273\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3260 - val_loss: 1.4910\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3190 - val_loss: 1.4093\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3142 - val_loss: 1.3079\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3127 - val_loss: 1.3078\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3132 - val_loss: 1.3124\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3098 - val_loss: 1.3061\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3117 - val_loss: 1.3080\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3087 - val_loss: 1.3065\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3079 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3074 - val_loss: 1.3060\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3069 - val_loss: 1.3065\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3069 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3071 - val_loss: 1.3117\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3065 - val_loss: 1.3099\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3095\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3083\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3084\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3065 - val_loss: 1.3134\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3168\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3152\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3267\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3205\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3189\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3251\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3203\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3158\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3092\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3082\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3108\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3121\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3125\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3101\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3089\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3105\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3106\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3123\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3124\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3070\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3084\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3096\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3113\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3113\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3068\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3071\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3073\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3072\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3076\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3072\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3064 - val_loss: 1.3113\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3264\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.4747\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3253\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3082\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0188725\n","The max value of N 0.10768678\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6713598333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.5125 - val_loss: 1.6644\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3593 - val_loss: 1.6895\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 409us/step - loss: 1.3298 - val_loss: 1.7665\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3209 - val_loss: 1.3225\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3170 - val_loss: 1.3131\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3148 - val_loss: 1.3125\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3137 - val_loss: 1.3112\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3124 - val_loss: 1.3105\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3114 - val_loss: 1.3101\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3111 - val_loss: 1.3102\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3113 - val_loss: 1.3109\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3114 - val_loss: 1.3103\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3106 - val_loss: 1.3101\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3103 - val_loss: 1.3100\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3101 - val_loss: 1.3099\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3107 - val_loss: 1.3101\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3109 - val_loss: 1.3292\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3104 - val_loss: 1.3132\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3101 - val_loss: 1.3111\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3101 - val_loss: 1.3106\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3100 - val_loss: 1.3109\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3100 - val_loss: 1.3106\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3099 - val_loss: 1.3104\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3099 - val_loss: 1.3101\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3098 - val_loss: 1.3101\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3099 - val_loss: 1.3101\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3098 - val_loss: 1.3100\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3098 - val_loss: 1.3100\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3098 - val_loss: 1.3200\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3098 - val_loss: 1.3121\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3098 - val_loss: 1.3114\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3098 - val_loss: 1.3121\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3098 - val_loss: 1.3115\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3098 - val_loss: 1.3102\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3096\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0504013\n","The max value of N 0.10767607\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6143056666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.4870 - val_loss: 2.0190\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3512 - val_loss: 1.8737\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3323 - val_loss: 1.4785\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3216 - val_loss: 1.3781\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3147 - val_loss: 1.3069\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3112 - val_loss: 1.3077\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3107 - val_loss: 1.3072\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3105 - val_loss: 1.3067\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3092 - val_loss: 1.3065\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3081 - val_loss: 1.3062\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3073 - val_loss: 1.3058\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3068 - val_loss: 1.3057\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3056\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3062 - val_loss: 1.3056\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3062 - val_loss: 1.3057\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3072 - val_loss: 1.3091\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3062 - val_loss: 1.3058\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3058\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3085\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3101\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3069\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3086\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3087\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3084\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3074\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3078\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3091\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3090\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3077\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3072\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3071\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3069\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0372249\n","The max value of N 0.10861182\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5938356666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.4833 - val_loss: 1.8733\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 403us/step - loss: 1.3471 - val_loss: 2.0226\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3291 - val_loss: 1.4754\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3198 - val_loss: 1.3063\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3154 - val_loss: 1.3062\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3148 - val_loss: 1.3066\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3109 - val_loss: 1.3061\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3095 - val_loss: 1.3059\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3080 - val_loss: 1.3058\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3077 - val_loss: 1.3057\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3099 - val_loss: 1.3137\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3081 - val_loss: 1.3094\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3073 - val_loss: 1.3073\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3073 - val_loss: 1.3081\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3068 - val_loss: 1.3076\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3066 - val_loss: 1.3072\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3069 - val_loss: 1.3114\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3074 - val_loss: 1.3202\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3070 - val_loss: 1.3087\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3061 - val_loss: 1.3068\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3070\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3057 - val_loss: 1.3063\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3057 - val_loss: 1.3074\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3057 - val_loss: 1.3076\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3079\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3077\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3069\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3102\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3068 - val_loss: 1.3342\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3068\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3058\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3057 - val_loss: 1.3080\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0285052\n","The max value of N 0.12765376\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6362540000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 1\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.5217 - val_loss: 2.0567\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3594 - val_loss: 1.6076\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3260 - val_loss: 1.4099\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3178 - val_loss: 1.3082\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3126 - val_loss: 1.3073\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3121 - val_loss: 1.3244\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3095 - val_loss: 1.3068\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3081 - val_loss: 1.3068\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3091 - val_loss: 1.3229\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3078 - val_loss: 1.3070\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3068 - val_loss: 1.3062\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3064\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3060\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3070 - val_loss: 1.3110\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3065 - val_loss: 1.3078\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3070\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0188879\n","The max value of N 0.108311266\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6321486666666667\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6382789999999999, 0.6589713333333334, 0.6502976666666667, 0.5999641666666667, 0.6133811666666668, 0.6713598333333334, 0.6143056666666666, 0.5938356666666667, 0.6362540000000001, 0.6321486666666667]\n","AUROC ===== 0.6308797166666666 +/- 0.024081132368521992\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f6abc212ef0>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"0yV2B7vdigsL","colab_type":"text"},"cell_type":"markdown","source":["##**SHIP vs All **##"]},{"metadata":{"id":"AEX-Ew14SsBs","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":62687},"outputId":"429e4b4d-f93f-4f7f-f606-595b4db3282a","executionInfo":{"status":"ok","timestamp":1541399012391,"user_tz":-660,"elapsed":3614818,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 950us/step - loss: 1.3616 - val_loss: 1.3957\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 466us/step - loss: 1.3332 - val_loss: 1.3617\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3294 - val_loss: 1.3447\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3278 - val_loss: 1.3398\n","Epoch 5/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3268 - val_loss: 1.3392\n","Epoch 6/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3259 - val_loss: 1.3354\n","Epoch 7/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3250 - val_loss: 1.3339\n","Epoch 8/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3248 - val_loss: 1.3349\n","Epoch 9/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3244 - val_loss: 1.3327\n","Epoch 10/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3239 - val_loss: 1.3313\n","Epoch 11/150\n","5850/5850 [==============================] - 3s 446us/step - loss: 1.3235 - val_loss: 1.3320\n","Epoch 12/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3232 - val_loss: 1.3306\n","Epoch 13/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3228 - val_loss: 1.3304\n","Epoch 14/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3225 - val_loss: 1.3293\n","Epoch 15/150\n","5850/5850 [==============================] - 3s 439us/step - loss: 1.3224 - val_loss: 1.3288\n","Epoch 16/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3217 - val_loss: 1.3274\n","Epoch 17/150\n","5850/5850 [==============================] - 3s 441us/step - loss: 1.3212 - val_loss: 1.3275\n","Epoch 18/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3209 - val_loss: 1.3278\n","Epoch 19/150\n","5850/5850 [==============================] - 3s 437us/step - loss: 1.3205 - val_loss: 1.3270\n","Epoch 20/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3202 - val_loss: 1.3272\n","Epoch 21/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3199 - val_loss: 1.3269\n","Epoch 22/150\n","5850/5850 [==============================] - 3s 431us/step - loss: 1.3197 - val_loss: 1.3265\n","Epoch 23/150\n","5850/5850 [==============================] - 3s 431us/step - loss: 1.3196 - val_loss: 1.3260\n","Epoch 24/150\n","5850/5850 [==============================] - 3s 434us/step - loss: 1.3193 - val_loss: 1.3260\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 427us/step - loss: 1.3192 - val_loss: 1.3276\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 422us/step - loss: 1.3189 - val_loss: 1.3260\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 427us/step - loss: 1.3188 - val_loss: 1.3260\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 425us/step - loss: 1.3187 - val_loss: 1.3267\n","Epoch 29/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3185 - val_loss: 1.3245\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 427us/step - loss: 1.3184 - val_loss: 1.3246\n","Epoch 31/150\n","5850/5850 [==============================] - 3s 430us/step - loss: 1.3183 - val_loss: 1.3245\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 424us/step - loss: 1.3181 - val_loss: 1.3261\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3179 - val_loss: 1.3237\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3178 - val_loss: 1.3241\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3178 - val_loss: 1.3237\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3178 - val_loss: 1.3243\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3179 - val_loss: 1.3236\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3175 - val_loss: 1.3246\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3175 - val_loss: 1.3244\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3173 - val_loss: 1.3237\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3173 - val_loss: 1.3241\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3172 - val_loss: 1.3231\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3172 - val_loss: 1.3235\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 412us/step - loss: 1.3170 - val_loss: 1.3235\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3170 - val_loss: 1.3238\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3168 - val_loss: 1.3217\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3168 - val_loss: 1.3224\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3168 - val_loss: 1.3222\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3167 - val_loss: 1.3228\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3168 - val_loss: 1.3222\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3166 - val_loss: 1.3220\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3164 - val_loss: 1.3228\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3164 - val_loss: 1.3215\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3164 - val_loss: 1.3213\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3165 - val_loss: 1.3213\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3163 - val_loss: 1.3216\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3161 - val_loss: 1.3219\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3160 - val_loss: 1.3212\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3160 - val_loss: 1.3207\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3160 - val_loss: 1.3211\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3161 - val_loss: 1.3212\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 412us/step - loss: 1.3160 - val_loss: 1.3205\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3160 - val_loss: 1.3215\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3158 - val_loss: 1.3213\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3157 - val_loss: 1.3215\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3157 - val_loss: 1.3211\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3157 - val_loss: 1.3212\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3156 - val_loss: 1.3208\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 421us/step - loss: 1.3155 - val_loss: 1.3204\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3155 - val_loss: 1.3201\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3154 - val_loss: 1.3202\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3154 - val_loss: 1.3200\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3154 - val_loss: 1.3200\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3154 - val_loss: 1.3202\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3154 - val_loss: 1.3201\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3152 - val_loss: 1.3195\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3153 - val_loss: 1.3194\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3151 - val_loss: 1.3201\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3152 - val_loss: 1.3201\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3150 - val_loss: 1.3196\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 410us/step - loss: 1.3149 - val_loss: 1.3196\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3149 - val_loss: 1.3191\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3149 - val_loss: 1.3189\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3148 - val_loss: 1.3191\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3148 - val_loss: 1.3192\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3149 - val_loss: 1.3199\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3147 - val_loss: 1.3196\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3148 - val_loss: 1.3189\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3146 - val_loss: 1.3196\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3146 - val_loss: 1.3187\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 421us/step - loss: 1.3145 - val_loss: 1.3189\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3145 - val_loss: 1.3188\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3144 - val_loss: 1.3186\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3144 - val_loss: 1.3193\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3144 - val_loss: 1.3187\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3144 - val_loss: 1.3184\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3142 - val_loss: 1.3186\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3142 - val_loss: 1.3185\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3142 - val_loss: 1.3183\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3141 - val_loss: 1.3180\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3141 - val_loss: 1.3184\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3141 - val_loss: 1.3180\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3141 - val_loss: 1.3196\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3141 - val_loss: 1.3183\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3140 - val_loss: 1.3191\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3141 - val_loss: 1.3184\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3140 - val_loss: 1.3180\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3139 - val_loss: 1.3182\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3139 - val_loss: 1.3176\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3139 - val_loss: 1.3182\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 412us/step - loss: 1.3139 - val_loss: 1.3182\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 423us/step - loss: 1.3139 - val_loss: 1.3179\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3138 - val_loss: 1.3179\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3137 - val_loss: 1.3182\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3137 - val_loss: 1.3177\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3136 - val_loss: 1.3178\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3136 - val_loss: 1.3181\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3135 - val_loss: 1.3176\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3136 - val_loss: 1.3175\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3136 - val_loss: 1.3177\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3135 - val_loss: 1.3172\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3135 - val_loss: 1.3176\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3135 - val_loss: 1.3176\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3135 - val_loss: 1.3171\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 410us/step - loss: 1.3133 - val_loss: 1.3171\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3133 - val_loss: 1.3169\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3134 - val_loss: 1.3173\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 422us/step - loss: 1.3134 - val_loss: 1.3169\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3133 - val_loss: 1.3173\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3134 - val_loss: 1.3172\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 423us/step - loss: 1.3133 - val_loss: 1.3173\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3134 - val_loss: 1.3174\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3132 - val_loss: 1.3173\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 415us/step - loss: 1.3132 - val_loss: 1.3170\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3132 - val_loss: 1.3168\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 400us/step - loss: 1.3132 - val_loss: 1.3173\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3132 - val_loss: 1.3171\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3131 - val_loss: 1.3169\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3131 - val_loss: 1.3168\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 421us/step - loss: 1.3131 - val_loss: 1.3168\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3130 - val_loss: 1.3168\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3130 - val_loss: 1.3165\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3130 - val_loss: 1.3167\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3130 - val_loss: 1.3165\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3129 - val_loss: 1.3164\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3129 - val_loss: 1.3167\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3131 - val_loss: 1.3168\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3129 - val_loss: 1.3164\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967990 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9249648\n","The max value of N 0.9307065\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7487409999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 838us/step - loss: 1.3618 - val_loss: 1.3962\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3332 - val_loss: 1.3558\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3295 - val_loss: 1.3449\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3278 - val_loss: 1.3405\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3266 - val_loss: 1.3383\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3257 - val_loss: 1.3360\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3247 - val_loss: 1.3331\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3238 - val_loss: 1.3344\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3231 - val_loss: 1.3315\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3227 - val_loss: 1.3315\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3223 - val_loss: 1.3310\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3218 - val_loss: 1.3298\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3215 - val_loss: 1.3284\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3210 - val_loss: 1.3284\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3207 - val_loss: 1.3278\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3205 - val_loss: 1.3283\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3202 - val_loss: 1.3271\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3200 - val_loss: 1.3267\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3198 - val_loss: 1.3260\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3195 - val_loss: 1.3264\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3193 - val_loss: 1.3255\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3192 - val_loss: 1.3256\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3190 - val_loss: 1.3253\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3190 - val_loss: 1.3250\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3187 - val_loss: 1.3243\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3185 - val_loss: 1.3245\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3183 - val_loss: 1.3243\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3183 - val_loss: 1.3245\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3182 - val_loss: 1.3239\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3180 - val_loss: 1.3246\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3180 - val_loss: 1.3249\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3178 - val_loss: 1.3231\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3176 - val_loss: 1.3233\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3175 - val_loss: 1.3229\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3174 - val_loss: 1.3231\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3175 - val_loss: 1.3230\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3173 - val_loss: 1.3229\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3171 - val_loss: 1.3231\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3170 - val_loss: 1.3234\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3170 - val_loss: 1.3223\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3169 - val_loss: 1.3234\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3167 - val_loss: 1.3221\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3168 - val_loss: 1.3223\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3167 - val_loss: 1.3222\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3165 - val_loss: 1.3228\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3164 - val_loss: 1.3222\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3165 - val_loss: 1.3220\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3163 - val_loss: 1.3211\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3163 - val_loss: 1.3220\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3163 - val_loss: 1.3220\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3161 - val_loss: 1.3218\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3161 - val_loss: 1.3214\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3159 - val_loss: 1.3216\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3159 - val_loss: 1.3209\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3158 - val_loss: 1.3208\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3157 - val_loss: 1.3223\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3157 - val_loss: 1.3210\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3156 - val_loss: 1.3209\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3155 - val_loss: 1.3205\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3155 - val_loss: 1.3203\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3155 - val_loss: 1.3203\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3154 - val_loss: 1.3206\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3153 - val_loss: 1.3202\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3152 - val_loss: 1.3202\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3151 - val_loss: 1.3199\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3152 - val_loss: 1.3199\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3152 - val_loss: 1.3200\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3151 - val_loss: 1.3199\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3150 - val_loss: 1.3196\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3149 - val_loss: 1.3198\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3149 - val_loss: 1.3200\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3149 - val_loss: 1.3191\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3147 - val_loss: 1.3195\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3147 - val_loss: 1.3194\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3147 - val_loss: 1.3195\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3146 - val_loss: 1.3188\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3147 - val_loss: 1.3190\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3146 - val_loss: 1.3190\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3146 - val_loss: 1.3188\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3145 - val_loss: 1.3193\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3145 - val_loss: 1.3186\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3144 - val_loss: 1.3197\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3143 - val_loss: 1.3198\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3143 - val_loss: 1.3185\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3142 - val_loss: 1.3185\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3142 - val_loss: 1.3181\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3141 - val_loss: 1.3184\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3141 - val_loss: 1.3180\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3140 - val_loss: 1.3182\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3139 - val_loss: 1.3181\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3140 - val_loss: 1.3181\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3139 - val_loss: 1.3178\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3138 - val_loss: 1.3175\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3137 - val_loss: 1.3175\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3137 - val_loss: 1.3180\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3137 - val_loss: 1.3178\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3137 - val_loss: 1.3177\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3136 - val_loss: 1.3179\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3136 - val_loss: 1.3175\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3135 - val_loss: 1.3173\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3135 - val_loss: 1.3175\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3134 - val_loss: 1.3169\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3134 - val_loss: 1.3175\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3135 - val_loss: 1.3173\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3133 - val_loss: 1.3173\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3133 - val_loss: 1.3171\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3132 - val_loss: 1.3171\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3132 - val_loss: 1.3169\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3132 - val_loss: 1.3168\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3132 - val_loss: 1.3179\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3132 - val_loss: 1.3171\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3130 - val_loss: 1.3166\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3129 - val_loss: 1.3167\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3129 - val_loss: 1.3167\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3129 - val_loss: 1.3165\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3129 - val_loss: 1.3165\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3128 - val_loss: 1.3169\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3128 - val_loss: 1.3163\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3127 - val_loss: 1.3160\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3127 - val_loss: 1.3161\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3126 - val_loss: 1.3163\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3128 - val_loss: 1.3165\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3163\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3127 - val_loss: 1.3160\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3126 - val_loss: 1.3160\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3125 - val_loss: 1.3160\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3161\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3125 - val_loss: 1.3159\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3124 - val_loss: 1.3158\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3124 - val_loss: 1.3158\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3124 - val_loss: 1.3162\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3124 - val_loss: 1.3160\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3123 - val_loss: 1.3160\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3123 - val_loss: 1.3160\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3123 - val_loss: 1.3159\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3122 - val_loss: 1.3157\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3157\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3121 - val_loss: 1.3157\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3121 - val_loss: 1.3157\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3121 - val_loss: 1.3155\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3121 - val_loss: 1.3152\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3122 - val_loss: 1.3154\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3121 - val_loss: 1.3154\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3121 - val_loss: 1.3154\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3120 - val_loss: 1.3156\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967995 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8528018\n","The max value of N 0.7940382\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7448013333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 934us/step - loss: 1.3575 - val_loss: 1.3908\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 404us/step - loss: 1.3301 - val_loss: 1.3642\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3267 - val_loss: 1.3468\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3251 - val_loss: 1.3411\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3241 - val_loss: 1.3366\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3233 - val_loss: 1.3327\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3223 - val_loss: 1.3316\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3213 - val_loss: 1.3298\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3207 - val_loss: 1.3278\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3201 - val_loss: 1.3281\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3196 - val_loss: 1.3271\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3192 - val_loss: 1.3267\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3189 - val_loss: 1.3276\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3185 - val_loss: 1.3257\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3183 - val_loss: 1.3267\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3180 - val_loss: 1.3248\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3178 - val_loss: 1.3257\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3178 - val_loss: 1.3250\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3173 - val_loss: 1.3245\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3171 - val_loss: 1.3239\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3169 - val_loss: 1.3230\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3167 - val_loss: 1.3229\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3166 - val_loss: 1.3228\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3164 - val_loss: 1.3223\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3163 - val_loss: 1.3227\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3161 - val_loss: 1.3222\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3161 - val_loss: 1.3220\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3160 - val_loss: 1.3228\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3158 - val_loss: 1.3218\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3156 - val_loss: 1.3214\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3156 - val_loss: 1.3208\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3154 - val_loss: 1.3210\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3153 - val_loss: 1.3213\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3152 - val_loss: 1.3206\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3151 - val_loss: 1.3217\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3151 - val_loss: 1.3205\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3148 - val_loss: 1.3204\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3148 - val_loss: 1.3197\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3147 - val_loss: 1.3200\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3146 - val_loss: 1.3198\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3144 - val_loss: 1.3195\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3145 - val_loss: 1.3193\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3143 - val_loss: 1.3212\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3143 - val_loss: 1.3196\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3141 - val_loss: 1.3203\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3141 - val_loss: 1.3192\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3141 - val_loss: 1.3191\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3193\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3139 - val_loss: 1.3194\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3138 - val_loss: 1.3192\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3137 - val_loss: 1.3191\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3137 - val_loss: 1.3187\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3136 - val_loss: 1.3187\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3136 - val_loss: 1.3187\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3136 - val_loss: 1.3182\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3134 - val_loss: 1.3183\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3133 - val_loss: 1.3181\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3133 - val_loss: 1.3184\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3132 - val_loss: 1.3179\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3132 - val_loss: 1.3179\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3132 - val_loss: 1.3179\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3131 - val_loss: 1.3177\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3181\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3129 - val_loss: 1.3179\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3129 - val_loss: 1.3176\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3128 - val_loss: 1.3179\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3128 - val_loss: 1.3176\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3176\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3126 - val_loss: 1.3170\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3126 - val_loss: 1.3181\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3126 - val_loss: 1.3168\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3126 - val_loss: 1.3171\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3180\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3124 - val_loss: 1.3168\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3123 - val_loss: 1.3174\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3123 - val_loss: 1.3169\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3123 - val_loss: 1.3167\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3122 - val_loss: 1.3162\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3121 - val_loss: 1.3163\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3120 - val_loss: 1.3164\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3120 - val_loss: 1.3158\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3120 - val_loss: 1.3159\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3120 - val_loss: 1.3157\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3119 - val_loss: 1.3169\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3119 - val_loss: 1.3164\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3117 - val_loss: 1.3157\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3117 - val_loss: 1.3157\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3116 - val_loss: 1.3157\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3116 - val_loss: 1.3161\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3116 - val_loss: 1.3162\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3116 - val_loss: 1.3160\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3116 - val_loss: 1.3154\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3116 - val_loss: 1.3163\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3116 - val_loss: 1.3155\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3114 - val_loss: 1.3151\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3114 - val_loss: 1.3156\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3115 - val_loss: 1.3158\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3114 - val_loss: 1.3148\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3113 - val_loss: 1.3152\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3113 - val_loss: 1.3159\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3112 - val_loss: 1.3148\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3112 - val_loss: 1.3149\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3111 - val_loss: 1.3154\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3110 - val_loss: 1.3147\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3111 - val_loss: 1.3150\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3110 - val_loss: 1.3149\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3110 - val_loss: 1.3146\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3109 - val_loss: 1.3146\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3109 - val_loss: 1.3148\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3109 - val_loss: 1.3143\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3109 - val_loss: 1.3148\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3108 - val_loss: 1.3146\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3109 - val_loss: 1.3146\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3109 - val_loss: 1.3144\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3108 - val_loss: 1.3145\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3108 - val_loss: 1.3144\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3108 - val_loss: 1.3144\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3107 - val_loss: 1.3144\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3107 - val_loss: 1.3147\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3106 - val_loss: 1.3141\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3105 - val_loss: 1.3139\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3106 - val_loss: 1.3140\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3105 - val_loss: 1.3138\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3104 - val_loss: 1.3142\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3106 - val_loss: 1.3141\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3105 - val_loss: 1.3142\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3105 - val_loss: 1.3138\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3103 - val_loss: 1.3138\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3105 - val_loss: 1.3139\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3103 - val_loss: 1.3136\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3103 - val_loss: 1.3140\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3103 - val_loss: 1.3143\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3102 - val_loss: 1.3138\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3102 - val_loss: 1.3135\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3101 - val_loss: 1.3135\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3101 - val_loss: 1.3134\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3101 - val_loss: 1.3135\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3102 - val_loss: 1.3141\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3102 - val_loss: 1.3138\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3101 - val_loss: 1.3136\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3101 - val_loss: 1.3133\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3102 - val_loss: 1.3137\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3101 - val_loss: 1.3135\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3100 - val_loss: 1.3135\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3100 - val_loss: 1.3138\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3100 - val_loss: 1.3133\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3100 - val_loss: 1.3136\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3099 - val_loss: 1.3134\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3099 - val_loss: 1.3133\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3099 - val_loss: 1.3133\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967996 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.78933847\n","The max value of N 0.7865684\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7648995000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.3631 - val_loss: 1.4068\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 427us/step - loss: 1.3340 - val_loss: 1.3657\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3296 - val_loss: 1.3543\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3277 - val_loss: 1.3451\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3264 - val_loss: 1.3396\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3255 - val_loss: 1.3398\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3249 - val_loss: 1.3366\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3243 - val_loss: 1.3363\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3237 - val_loss: 1.3329\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3232 - val_loss: 1.3320\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3224 - val_loss: 1.3302\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3219 - val_loss: 1.3293\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3216 - val_loss: 1.3295\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3214 - val_loss: 1.3291\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3208 - val_loss: 1.3290\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3207 - val_loss: 1.3282\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3204 - val_loss: 1.3270\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3202 - val_loss: 1.3282\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3200 - val_loss: 1.3288\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3199 - val_loss: 1.3268\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3196 - val_loss: 1.3258\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3194 - val_loss: 1.3258\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3192 - val_loss: 1.3253\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3190 - val_loss: 1.3246\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3189 - val_loss: 1.3249\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3187 - val_loss: 1.3246\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3186 - val_loss: 1.3252\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3186 - val_loss: 1.3251\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3183 - val_loss: 1.3244\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3181 - val_loss: 1.3250\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3180 - val_loss: 1.3240\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3179 - val_loss: 1.3234\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3179 - val_loss: 1.3249\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3178 - val_loss: 1.3244\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3177 - val_loss: 1.3226\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3176 - val_loss: 1.3228\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3175 - val_loss: 1.3231\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3175 - val_loss: 1.3231\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3174 - val_loss: 1.3235\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3173 - val_loss: 1.3230\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3171 - val_loss: 1.3222\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3171 - val_loss: 1.3226\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3171 - val_loss: 1.3226\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3170 - val_loss: 1.3219\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3169 - val_loss: 1.3226\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3168 - val_loss: 1.3221\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3167 - val_loss: 1.3233\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3167 - val_loss: 1.3217\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3166 - val_loss: 1.3217\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3165 - val_loss: 1.3218\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3163 - val_loss: 1.3211\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3163 - val_loss: 1.3216\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3163 - val_loss: 1.3217\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3161 - val_loss: 1.3204\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3162 - val_loss: 1.3209\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3161 - val_loss: 1.3213\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3160 - val_loss: 1.3202\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3159 - val_loss: 1.3204\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3159 - val_loss: 1.3212\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3159 - val_loss: 1.3205\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3158 - val_loss: 1.3200\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3159 - val_loss: 1.3204\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3157 - val_loss: 1.3215\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3155 - val_loss: 1.3205\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3155 - val_loss: 1.3201\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3155 - val_loss: 1.3204\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3154 - val_loss: 1.3196\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3153 - val_loss: 1.3199\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3153 - val_loss: 1.3201\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3152 - val_loss: 1.3201\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3152 - val_loss: 1.3195\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3152 - val_loss: 1.3198\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3153 - val_loss: 1.3192\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3152 - val_loss: 1.3193\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3150 - val_loss: 1.3190\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3150 - val_loss: 1.3191\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3149 - val_loss: 1.3189\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3149 - val_loss: 1.3188\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3149 - val_loss: 1.3186\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3148 - val_loss: 1.3189\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3147 - val_loss: 1.3187\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3147 - val_loss: 1.3187\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3146 - val_loss: 1.3192\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3146 - val_loss: 1.3186\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3145 - val_loss: 1.3184\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3145 - val_loss: 1.3182\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3146 - val_loss: 1.3187\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3144 - val_loss: 1.3182\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3144 - val_loss: 1.3184\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3145 - val_loss: 1.3186\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3143 - val_loss: 1.3180\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3143 - val_loss: 1.3180\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3143 - val_loss: 1.3184\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3142 - val_loss: 1.3181\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3141 - val_loss: 1.3182\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3141 - val_loss: 1.3177\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3141 - val_loss: 1.3180\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3174\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3175\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3140 - val_loss: 1.3178\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3140 - val_loss: 1.3177\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3139 - val_loss: 1.3175\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3138 - val_loss: 1.3174\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3137 - val_loss: 1.3174\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3138 - val_loss: 1.3174\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3137 - val_loss: 1.3176\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3138 - val_loss: 1.3175\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3137 - val_loss: 1.3175\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3136 - val_loss: 1.3173\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3136 - val_loss: 1.3174\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3135 - val_loss: 1.3170\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3135 - val_loss: 1.3169\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3134 - val_loss: 1.3167\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3134 - val_loss: 1.3169\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3134 - val_loss: 1.3168\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3134 - val_loss: 1.3168\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3133 - val_loss: 1.3167\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3133 - val_loss: 1.3167\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3133 - val_loss: 1.3167\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3134 - val_loss: 1.3167\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3132 - val_loss: 1.3169\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3132 - val_loss: 1.3170\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3132 - val_loss: 1.3167\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3130 - val_loss: 1.3162\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3162\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3164\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3129 - val_loss: 1.3160\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3129 - val_loss: 1.3165\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3130 - val_loss: 1.3164\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3129 - val_loss: 1.3162\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3129 - val_loss: 1.3159\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3128 - val_loss: 1.3159\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3128 - val_loss: 1.3159\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3128 - val_loss: 1.3170\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3129 - val_loss: 1.3162\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3162\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3127 - val_loss: 1.3159\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3126 - val_loss: 1.3158\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3127 - val_loss: 1.3155\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3126 - val_loss: 1.3160\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3155\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3126 - val_loss: 1.3156\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967990 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8257169\n","The max value of N 0.7967404\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7229093333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.3563 - val_loss: 1.3831\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3279 - val_loss: 1.3574\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3239 - val_loss: 1.3422\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3221 - val_loss: 1.3354\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3207 - val_loss: 1.3325\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3197 - val_loss: 1.3312\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3185 - val_loss: 1.3281\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3178 - val_loss: 1.3280\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3174 - val_loss: 1.3272\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3169 - val_loss: 1.3273\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3164 - val_loss: 1.3241\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3162 - val_loss: 1.3254\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3158 - val_loss: 1.3256\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3155 - val_loss: 1.3253\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3153 - val_loss: 1.3252\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3150 - val_loss: 1.3227\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3146 - val_loss: 1.3233\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3145 - val_loss: 1.3233\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3142 - val_loss: 1.3216\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3140 - val_loss: 1.3217\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3140 - val_loss: 1.3211\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3139 - val_loss: 1.3208\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3137 - val_loss: 1.3223\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3135 - val_loss: 1.3198\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3132 - val_loss: 1.3211\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3131 - val_loss: 1.3201\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3130 - val_loss: 1.3211\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3128 - val_loss: 1.3215\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3127 - val_loss: 1.3191\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3125 - val_loss: 1.3190\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3188\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3123 - val_loss: 1.3188\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3123 - val_loss: 1.3189\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3121 - val_loss: 1.3189\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3120 - val_loss: 1.3178\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3120 - val_loss: 1.3187\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3118 - val_loss: 1.3175\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3117 - val_loss: 1.3181\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3116 - val_loss: 1.3174\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3116 - val_loss: 1.3172\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3114 - val_loss: 1.3177\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3113 - val_loss: 1.3168\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3112 - val_loss: 1.3177\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3113 - val_loss: 1.3175\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3111 - val_loss: 1.3169\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3110 - val_loss: 1.3169\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3109 - val_loss: 1.3167\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3108 - val_loss: 1.3164\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3108 - val_loss: 1.3168\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3108 - val_loss: 1.3159\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3107 - val_loss: 1.3158\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3107 - val_loss: 1.3174\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3108 - val_loss: 1.3164\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3105 - val_loss: 1.3153\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3104 - val_loss: 1.3154\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3104 - val_loss: 1.3158\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3102 - val_loss: 1.3159\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3102 - val_loss: 1.3152\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3101 - val_loss: 1.3152\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3101 - val_loss: 1.3153\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3100 - val_loss: 1.3147\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3099 - val_loss: 1.3153\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3099 - val_loss: 1.3145\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3098 - val_loss: 1.3144\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3098 - val_loss: 1.3142\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3097 - val_loss: 1.3140\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3141\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3141\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3095 - val_loss: 1.3140\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3140\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3095 - val_loss: 1.3148\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3094 - val_loss: 1.3144\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3094 - val_loss: 1.3141\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3093 - val_loss: 1.3139\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3093 - val_loss: 1.3139\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3093 - val_loss: 1.3143\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3092 - val_loss: 1.3137\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3091 - val_loss: 1.3136\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3091 - val_loss: 1.3136\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3090 - val_loss: 1.3136\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3089 - val_loss: 1.3135\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3089 - val_loss: 1.3133\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3088 - val_loss: 1.3133\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3088 - val_loss: 1.3134\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3087 - val_loss: 1.3132\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3086 - val_loss: 1.3130\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3086 - val_loss: 1.3133\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3087 - val_loss: 1.3134\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3086 - val_loss: 1.3132\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3084 - val_loss: 1.3133\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3085 - val_loss: 1.3127\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3083 - val_loss: 1.3125\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3084 - val_loss: 1.3125\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3084 - val_loss: 1.3131\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3083 - val_loss: 1.3126\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3081 - val_loss: 1.3121\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3081 - val_loss: 1.3127\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3081 - val_loss: 1.3125\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3081 - val_loss: 1.3120\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3080 - val_loss: 1.3122\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3080 - val_loss: 1.3117\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3080 - val_loss: 1.3118\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3079 - val_loss: 1.3121\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3080 - val_loss: 1.3120\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3078 - val_loss: 1.3117\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3078 - val_loss: 1.3118\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3079 - val_loss: 1.3120\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3077 - val_loss: 1.3119\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3077 - val_loss: 1.3115\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3077 - val_loss: 1.3116\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3077 - val_loss: 1.3116\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3077 - val_loss: 1.3117\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3076 - val_loss: 1.3112\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3075 - val_loss: 1.3112\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3075 - val_loss: 1.3111\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3075 - val_loss: 1.3115\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3074 - val_loss: 1.3109\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3074 - val_loss: 1.3113\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3073 - val_loss: 1.3114\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3073 - val_loss: 1.3111\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3073 - val_loss: 1.3108\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3073 - val_loss: 1.3112\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3073 - val_loss: 1.3108\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3072 - val_loss: 1.3110\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3072 - val_loss: 1.3106\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3072 - val_loss: 1.3106\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3072 - val_loss: 1.3106\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3071 - val_loss: 1.3108\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3072 - val_loss: 1.3105\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3070 - val_loss: 1.3104\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3070 - val_loss: 1.3104\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3104\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3070 - val_loss: 1.3105\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3069 - val_loss: 1.3102\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3069 - val_loss: 1.3103\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3103\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3069 - val_loss: 1.3104\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3068 - val_loss: 1.3115\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3068 - val_loss: 1.3101\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3068 - val_loss: 1.3100\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3068 - val_loss: 1.3106\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3067 - val_loss: 1.3101\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3067 - val_loss: 1.3101\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3068 - val_loss: 1.3099\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3067 - val_loss: 1.3099\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3067 - val_loss: 1.3102\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3066 - val_loss: 1.3099\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3066 - val_loss: 1.3099\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3065 - val_loss: 1.3099\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3066 - val_loss: 1.3097\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967997 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8257468\n","The max value of N 0.78449893\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7503966666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.3599 - val_loss: 1.3934\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3331 - val_loss: 1.3572\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3293 - val_loss: 1.3456\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3276 - val_loss: 1.3416\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3265 - val_loss: 1.3374\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3254 - val_loss: 1.3345\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3248 - val_loss: 1.3338\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3242 - val_loss: 1.3322\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3236 - val_loss: 1.3303\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3227 - val_loss: 1.3298\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3221 - val_loss: 1.3294\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3217 - val_loss: 1.3296\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3213 - val_loss: 1.3299\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3209 - val_loss: 1.3282\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3206 - val_loss: 1.3285\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3203 - val_loss: 1.3280\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3201 - val_loss: 1.3338\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3203 - val_loss: 1.3283\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3196 - val_loss: 1.3270\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3194 - val_loss: 1.3265\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3192 - val_loss: 1.3300\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3191 - val_loss: 1.3260\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3190 - val_loss: 1.3255\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3187 - val_loss: 1.3262\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3186 - val_loss: 1.3249\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3184 - val_loss: 1.3251\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3182 - val_loss: 1.3245\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3181 - val_loss: 1.3246\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3180 - val_loss: 1.3252\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3179 - val_loss: 1.3238\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3177 - val_loss: 1.3247\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3176 - val_loss: 1.3235\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3175 - val_loss: 1.3239\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3175 - val_loss: 1.3225\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3174 - val_loss: 1.3231\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3172 - val_loss: 1.3221\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3171 - val_loss: 1.3221\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3221\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3224\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3169 - val_loss: 1.3223\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3168 - val_loss: 1.3221\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3168 - val_loss: 1.3223\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3167 - val_loss: 1.3221\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3165 - val_loss: 1.3224\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3165 - val_loss: 1.3215\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3163 - val_loss: 1.3216\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3163 - val_loss: 1.3213\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3162 - val_loss: 1.3213\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3162 - val_loss: 1.3210\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3161 - val_loss: 1.3202\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3161 - val_loss: 1.3205\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3159 - val_loss: 1.3204\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3159 - val_loss: 1.3207\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3159 - val_loss: 1.3211\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3158 - val_loss: 1.3203\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3157 - val_loss: 1.3204\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3156 - val_loss: 1.3202\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3156 - val_loss: 1.3205\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3155 - val_loss: 1.3195\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3155 - val_loss: 1.3195\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3154 - val_loss: 1.3197\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3153 - val_loss: 1.3195\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3152 - val_loss: 1.3196\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3152 - val_loss: 1.3197\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3151 - val_loss: 1.3201\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3152 - val_loss: 1.3200\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3151 - val_loss: 1.3197\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3150 - val_loss: 1.3193\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3150 - val_loss: 1.3191\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3149 - val_loss: 1.3191\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3147 - val_loss: 1.3200\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3147 - val_loss: 1.3191\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3150 - val_loss: 1.3213\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3149 - val_loss: 1.3206\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3147 - val_loss: 1.3198\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3145 - val_loss: 1.3190\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3145 - val_loss: 1.3187\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3144 - val_loss: 1.3184\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3144 - val_loss: 1.3187\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3143 - val_loss: 1.3180\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3143 - val_loss: 1.3182\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3142 - val_loss: 1.3180\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3143 - val_loss: 1.3178\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3141 - val_loss: 1.3178\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3141 - val_loss: 1.3176\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3141 - val_loss: 1.3176\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3140 - val_loss: 1.3180\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3140 - val_loss: 1.3179\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3139 - val_loss: 1.3178\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3140 - val_loss: 1.3178\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3139 - val_loss: 1.3176\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3137 - val_loss: 1.3173\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3137 - val_loss: 1.3172\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3137 - val_loss: 1.3177\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3137 - val_loss: 1.3170\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3136 - val_loss: 1.3171\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3135 - val_loss: 1.3172\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3135 - val_loss: 1.3170\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3136 - val_loss: 1.3178\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3135 - val_loss: 1.3173\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3134 - val_loss: 1.3168\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3134 - val_loss: 1.3171\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3134 - val_loss: 1.3167\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3134 - val_loss: 1.3166\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3133 - val_loss: 1.3169\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3134 - val_loss: 1.3172\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3132 - val_loss: 1.3166\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3132 - val_loss: 1.3166\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3132 - val_loss: 1.3165\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3131 - val_loss: 1.3164\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3186\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3132 - val_loss: 1.3183\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3129 - val_loss: 1.3167\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3129 - val_loss: 1.3161\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3129 - val_loss: 1.3161\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3128 - val_loss: 1.3161\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3128 - val_loss: 1.3159\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3129 - val_loss: 1.3165\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3129 - val_loss: 1.3163\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3129 - val_loss: 1.3163\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3128 - val_loss: 1.3162\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3127 - val_loss: 1.3162\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3126 - val_loss: 1.3157\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3126 - val_loss: 1.3158\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3127 - val_loss: 1.3157\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3126 - val_loss: 1.3159\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3125 - val_loss: 1.3159\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3125 - val_loss: 1.3156\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3124 - val_loss: 1.3158\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3125 - val_loss: 1.3154\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3125 - val_loss: 1.3156\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3126 - val_loss: 1.3165\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3124 - val_loss: 1.3156\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3124 - val_loss: 1.3157\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3124 - val_loss: 1.3153\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3123 - val_loss: 1.3154\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3124 - val_loss: 1.3157\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3124 - val_loss: 1.3159\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3124 - val_loss: 1.3153\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3123 - val_loss: 1.3154\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3122 - val_loss: 1.3153\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967993 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.82413125\n","The max value of N 0.80477506\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7379086666666668\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.3660 - val_loss: 1.4007\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3372 - val_loss: 1.3664\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3334 - val_loss: 1.3524\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3314 - val_loss: 1.3469\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3301 - val_loss: 1.3444\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3293 - val_loss: 1.3403\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3286 - val_loss: 1.3391\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3284 - val_loss: 1.3396\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3279 - val_loss: 1.3361\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3270 - val_loss: 1.3347\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3263 - val_loss: 1.3344\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3258 - val_loss: 1.3343\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3254 - val_loss: 1.3340\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3251 - val_loss: 1.3334\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3247 - val_loss: 1.3328\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3244 - val_loss: 1.3319\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3241 - val_loss: 1.3335\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3240 - val_loss: 1.3324\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3238 - val_loss: 1.3323\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3235 - val_loss: 1.3310\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3233 - val_loss: 1.3301\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3230 - val_loss: 1.3301\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3229 - val_loss: 1.3306\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3227 - val_loss: 1.3303\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3226 - val_loss: 1.3295\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3225 - val_loss: 1.3308\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3224 - val_loss: 1.3288\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3221 - val_loss: 1.3289\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3221 - val_loss: 1.3291\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3219 - val_loss: 1.3286\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3219 - val_loss: 1.3290\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3217 - val_loss: 1.3281\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3216 - val_loss: 1.3275\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3214 - val_loss: 1.3281\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3213 - val_loss: 1.3285\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3213 - val_loss: 1.3275\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3211 - val_loss: 1.3264\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3211 - val_loss: 1.3270\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3209 - val_loss: 1.3277\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3210 - val_loss: 1.3270\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3208 - val_loss: 1.3267\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3207 - val_loss: 1.3264\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3206 - val_loss: 1.3264\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3205 - val_loss: 1.3261\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3205 - val_loss: 1.3257\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3204 - val_loss: 1.3261\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3203 - val_loss: 1.3257\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3203 - val_loss: 1.3266\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3202 - val_loss: 1.3261\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3200 - val_loss: 1.3253\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3200 - val_loss: 1.3258\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3200 - val_loss: 1.3256\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3199 - val_loss: 1.3257\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3201 - val_loss: 1.3257\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3198 - val_loss: 1.3253\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3197 - val_loss: 1.3256\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3197 - val_loss: 1.3260\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3196 - val_loss: 1.3254\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3196 - val_loss: 1.3257\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3195 - val_loss: 1.3244\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3194 - val_loss: 1.3249\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3194 - val_loss: 1.3255\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3194 - val_loss: 1.3244\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3192 - val_loss: 1.3247\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3191 - val_loss: 1.3238\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3191 - val_loss: 1.3241\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3190 - val_loss: 1.3238\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3191 - val_loss: 1.3239\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3189 - val_loss: 1.3235\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3189 - val_loss: 1.3239\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3188 - val_loss: 1.3236\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3188 - val_loss: 1.3231\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3187 - val_loss: 1.3231\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3187 - val_loss: 1.3238\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3186 - val_loss: 1.3231\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3186 - val_loss: 1.3227\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3187 - val_loss: 1.3232\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3186 - val_loss: 1.3228\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3185 - val_loss: 1.3226\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3184 - val_loss: 1.3229\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3183 - val_loss: 1.3226\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3183 - val_loss: 1.3227\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3184 - val_loss: 1.3223\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3183 - val_loss: 1.3234\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3183 - val_loss: 1.3228\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3182 - val_loss: 1.3231\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3182 - val_loss: 1.3228\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3181 - val_loss: 1.3226\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3180 - val_loss: 1.3228\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3180 - val_loss: 1.3226\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3180 - val_loss: 1.3227\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3179 - val_loss: 1.3227\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3179 - val_loss: 1.3225\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3177 - val_loss: 1.3221\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3178 - val_loss: 1.3217\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3177 - val_loss: 1.3220\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3178 - val_loss: 1.3219\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3177 - val_loss: 1.3225\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3178 - val_loss: 1.3225\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3176 - val_loss: 1.3218\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3176 - val_loss: 1.3230\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3177 - val_loss: 1.3214\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3176 - val_loss: 1.3220\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3174 - val_loss: 1.3219\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3174 - val_loss: 1.3215\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3174 - val_loss: 1.3213\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3173 - val_loss: 1.3213\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3173 - val_loss: 1.3214\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3173 - val_loss: 1.3212\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3172 - val_loss: 1.3210\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3172 - val_loss: 1.3210\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3172 - val_loss: 1.3211\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3172 - val_loss: 1.3213\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3208\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3171 - val_loss: 1.3208\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3171 - val_loss: 1.3210\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3170 - val_loss: 1.3217\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3170 - val_loss: 1.3208\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3170 - val_loss: 1.3212\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3170 - val_loss: 1.3205\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3169 - val_loss: 1.3206\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3169 - val_loss: 1.3205\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3168 - val_loss: 1.3209\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3168 - val_loss: 1.3205\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3168 - val_loss: 1.3206\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3169 - val_loss: 1.3204\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3168 - val_loss: 1.3208\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3168 - val_loss: 1.3203\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3168 - val_loss: 1.3204\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3168 - val_loss: 1.3206\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3167 - val_loss: 1.3202\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3167 - val_loss: 1.3202\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3167 - val_loss: 1.3203\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3166 - val_loss: 1.3204\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3166 - val_loss: 1.3204\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3166 - val_loss: 1.3201\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3166 - val_loss: 1.3204\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3166 - val_loss: 1.3203\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3167 - val_loss: 1.3201\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3166 - val_loss: 1.3204\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3166 - val_loss: 1.3204\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3165 - val_loss: 1.3201\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3166 - val_loss: 1.3203\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3165 - val_loss: 1.3205\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3165 - val_loss: 1.3200\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3164 - val_loss: 1.3201\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3164 - val_loss: 1.3197\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3163 - val_loss: 1.3198\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3163 - val_loss: 1.3200\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3163 - val_loss: 1.3198\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967990 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8000629\n","The max value of N 0.80923855\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7764223333333332\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.3610 - val_loss: 1.3970\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3326 - val_loss: 1.3601\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3291 - val_loss: 1.3469\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3270 - val_loss: 1.3424\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3256 - val_loss: 1.3373\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3244 - val_loss: 1.3348\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3237 - val_loss: 1.3346\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3230 - val_loss: 1.3313\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3224 - val_loss: 1.3325\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3219 - val_loss: 1.3298\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3216 - val_loss: 1.3284\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3211 - val_loss: 1.3278\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3206 - val_loss: 1.3272\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3204 - val_loss: 1.3275\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3202 - val_loss: 1.3266\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3199 - val_loss: 1.3272\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3196 - val_loss: 1.3290\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3194 - val_loss: 1.3255\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3192 - val_loss: 1.3260\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3191 - val_loss: 1.3287\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3190 - val_loss: 1.3263\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3187 - val_loss: 1.3246\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3185 - val_loss: 1.3242\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3184 - val_loss: 1.3248\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3183 - val_loss: 1.3240\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3180 - val_loss: 1.3245\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3179 - val_loss: 1.3241\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3177 - val_loss: 1.3239\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3177 - val_loss: 1.3233\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3177 - val_loss: 1.3235\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3177 - val_loss: 1.3241\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3175 - val_loss: 1.3240\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3172 - val_loss: 1.3229\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3171 - val_loss: 1.3236\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3171 - val_loss: 1.3225\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3170 - val_loss: 1.3231\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3169 - val_loss: 1.3224\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3168 - val_loss: 1.3220\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3166 - val_loss: 1.3217\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3165 - val_loss: 1.3217\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3165 - val_loss: 1.3210\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3163 - val_loss: 1.3217\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3163 - val_loss: 1.3212\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3162 - val_loss: 1.3211\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3162 - val_loss: 1.3212\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3161 - val_loss: 1.3212\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3161 - val_loss: 1.3222\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3159 - val_loss: 1.3211\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3157 - val_loss: 1.3205\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3157 - val_loss: 1.3203\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3156 - val_loss: 1.3207\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3155 - val_loss: 1.3200\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3154 - val_loss: 1.3203\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3154 - val_loss: 1.3205\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3154 - val_loss: 1.3197\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3152 - val_loss: 1.3195\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3152 - val_loss: 1.3204\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3152 - val_loss: 1.3191\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3150 - val_loss: 1.3196\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3150 - val_loss: 1.3194\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3150 - val_loss: 1.3194\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3149 - val_loss: 1.3195\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3148 - val_loss: 1.3194\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3148 - val_loss: 1.3189\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3146 - val_loss: 1.3190\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3146 - val_loss: 1.3186\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3145 - val_loss: 1.3192\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3144 - val_loss: 1.3186\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3186\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3143 - val_loss: 1.3183\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3143 - val_loss: 1.3181\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3143 - val_loss: 1.3187\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3142 - val_loss: 1.3187\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3141 - val_loss: 1.3180\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3141 - val_loss: 1.3181\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3142 - val_loss: 1.3178\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3140 - val_loss: 1.3182\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3140 - val_loss: 1.3185\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3139 - val_loss: 1.3180\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3139 - val_loss: 1.3179\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3139 - val_loss: 1.3180\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3137 - val_loss: 1.3178\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3137 - val_loss: 1.3183\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3137 - val_loss: 1.3178\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3136 - val_loss: 1.3174\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3136 - val_loss: 1.3176\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3136 - val_loss: 1.3174\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3135 - val_loss: 1.3173\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3135 - val_loss: 1.3174\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3135 - val_loss: 1.3171\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3134 - val_loss: 1.3171\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3133 - val_loss: 1.3169\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3134 - val_loss: 1.3172\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3134 - val_loss: 1.3172\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3134 - val_loss: 1.3180\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3173\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3132 - val_loss: 1.3168\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3132 - val_loss: 1.3171\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3168\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3131 - val_loss: 1.3169\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3130 - val_loss: 1.3164\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3130 - val_loss: 1.3164\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3167\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3129 - val_loss: 1.3164\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3128 - val_loss: 1.3163\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3129 - val_loss: 1.3163\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3128 - val_loss: 1.3164\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3128 - val_loss: 1.3164\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3128 - val_loss: 1.3167\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3129 - val_loss: 1.3195\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3128 - val_loss: 1.3166\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3128 - val_loss: 1.3173\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3127 - val_loss: 1.3168\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3127 - val_loss: 1.3163\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3126 - val_loss: 1.3163\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3126 - val_loss: 1.3165\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3125 - val_loss: 1.3162\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3125 - val_loss: 1.3169\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3125 - val_loss: 1.3166\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3125 - val_loss: 1.3157\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3124 - val_loss: 1.3161\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3157\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3123 - val_loss: 1.3158\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3156\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3123 - val_loss: 1.3163\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3123 - val_loss: 1.3155\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3121 - val_loss: 1.3154\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3122 - val_loss: 1.3159\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3155\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3121 - val_loss: 1.3155\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3154\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3121 - val_loss: 1.3152\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3121 - val_loss: 1.3156\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3121 - val_loss: 1.3155\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3121 - val_loss: 1.3160\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3121 - val_loss: 1.3155\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3121 - val_loss: 1.3154\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3120 - val_loss: 1.3152\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3121 - val_loss: 1.3155\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3119 - val_loss: 1.3153\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3120 - val_loss: 1.3155\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3119 - val_loss: 1.3152\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3149\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3118 - val_loss: 1.3154\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3119 - val_loss: 1.3150\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3119 - val_loss: 1.3155\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3119 - val_loss: 1.3149\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3118 - val_loss: 1.3149\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967994 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7736008\n","The max value of N 0.8164715\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7572363333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.3580 - val_loss: 1.3840\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 406us/step - loss: 1.3322 - val_loss: 1.3559\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3284 - val_loss: 1.3493\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3269 - val_loss: 1.3436\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3258 - val_loss: 1.3387\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3251 - val_loss: 1.3373\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3242 - val_loss: 1.3354\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3235 - val_loss: 1.3317\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3227 - val_loss: 1.3309\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3222 - val_loss: 1.3309\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3217 - val_loss: 1.3306\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3213 - val_loss: 1.3291\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3211 - val_loss: 1.3287\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3206 - val_loss: 1.3283\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3203 - val_loss: 1.3289\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3201 - val_loss: 1.3275\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3198 - val_loss: 1.3270\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3197 - val_loss: 1.3264\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3193 - val_loss: 1.3256\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3193 - val_loss: 1.3275\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3192 - val_loss: 1.3255\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3188 - val_loss: 1.3254\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3190 - val_loss: 1.3274\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3186 - val_loss: 1.3253\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3183 - val_loss: 1.3252\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3181 - val_loss: 1.3254\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3183 - val_loss: 1.3246\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3180 - val_loss: 1.3262\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3177 - val_loss: 1.3244\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3177 - val_loss: 1.3249\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3175 - val_loss: 1.3240\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3174 - val_loss: 1.3249\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3173 - val_loss: 1.3235\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3172 - val_loss: 1.3232\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3171 - val_loss: 1.3232\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3170 - val_loss: 1.3244\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3170 - val_loss: 1.3228\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3168 - val_loss: 1.3223\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3167 - val_loss: 1.3234\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3168 - val_loss: 1.3225\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3167 - val_loss: 1.3239\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3168 - val_loss: 1.3221\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3165 - val_loss: 1.3231\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3164 - val_loss: 1.3216\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3163 - val_loss: 1.3225\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3161 - val_loss: 1.3216\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3160 - val_loss: 1.3219\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3160 - val_loss: 1.3216\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3159 - val_loss: 1.3210\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3158 - val_loss: 1.3213\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3158 - val_loss: 1.3214\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3159 - val_loss: 1.3215\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3159 - val_loss: 1.3215\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3156 - val_loss: 1.3209\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3156 - val_loss: 1.3204\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3155 - val_loss: 1.3210\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3155 - val_loss: 1.3213\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3154 - val_loss: 1.3205\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3153 - val_loss: 1.3201\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3154 - val_loss: 1.3198\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3154 - val_loss: 1.3210\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3152 - val_loss: 1.3206\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3153 - val_loss: 1.3210\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3152 - val_loss: 1.3201\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3150 - val_loss: 1.3197\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3149 - val_loss: 1.3201\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3149 - val_loss: 1.3197\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3149 - val_loss: 1.3195\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3150 - val_loss: 1.3196\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3148 - val_loss: 1.3193\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3148 - val_loss: 1.3194\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3147 - val_loss: 1.3188\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3147 - val_loss: 1.3188\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3146 - val_loss: 1.3196\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3147 - val_loss: 1.3190\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3146 - val_loss: 1.3195\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3146 - val_loss: 1.3193\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3187\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3143 - val_loss: 1.3182\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3143 - val_loss: 1.3188\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3143 - val_loss: 1.3183\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3142 - val_loss: 1.3184\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3142 - val_loss: 1.3183\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3142 - val_loss: 1.3184\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3141 - val_loss: 1.3183\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3141 - val_loss: 1.3179\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3141 - val_loss: 1.3182\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3141 - val_loss: 1.3178\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3140 - val_loss: 1.3188\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3140 - val_loss: 1.3180\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3139 - val_loss: 1.3182\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3138 - val_loss: 1.3177\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3137 - val_loss: 1.3180\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3138 - val_loss: 1.3181\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3137 - val_loss: 1.3177\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3136 - val_loss: 1.3182\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3136 - val_loss: 1.3175\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3135 - val_loss: 1.3174\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3135 - val_loss: 1.3173\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3135 - val_loss: 1.3178\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3134 - val_loss: 1.3171\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3134 - val_loss: 1.3180\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3132 - val_loss: 1.3172\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3134 - val_loss: 1.3176\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3132 - val_loss: 1.3172\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3132 - val_loss: 1.3171\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3132 - val_loss: 1.3176\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3132 - val_loss: 1.3173\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3168\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3130 - val_loss: 1.3168\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3131 - val_loss: 1.3171\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3130 - val_loss: 1.3176\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3130 - val_loss: 1.3172\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3129 - val_loss: 1.3169\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3128 - val_loss: 1.3162\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3129 - val_loss: 1.3172\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3129 - val_loss: 1.3168\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3128 - val_loss: 1.3163\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3127 - val_loss: 1.3167\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3127 - val_loss: 1.3163\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3128 - val_loss: 1.3167\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3127 - val_loss: 1.3165\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3126 - val_loss: 1.3166\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3125 - val_loss: 1.3162\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3125 - val_loss: 1.3163\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3125 - val_loss: 1.3159\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3124 - val_loss: 1.3161\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3125 - val_loss: 1.3160\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3124 - val_loss: 1.3161\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3124 - val_loss: 1.3160\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3124 - val_loss: 1.3162\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3125 - val_loss: 1.3160\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3123 - val_loss: 1.3162\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3123 - val_loss: 1.3163\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3123 - val_loss: 1.3158\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3123 - val_loss: 1.3158\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3123 - val_loss: 1.3156\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3122 - val_loss: 1.3157\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3122 - val_loss: 1.3160\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3122 - val_loss: 1.3154\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3121 - val_loss: 1.3164\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3123 - val_loss: 1.3160\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3121 - val_loss: 1.3154\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3121 - val_loss: 1.3162\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3121 - val_loss: 1.3156\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3121 - val_loss: 1.3157\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3155\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967991 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.7956509\n","The max value of N 0.8252226\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7427530000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 8\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.3622 - val_loss: 1.3929\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3325 - val_loss: 1.3661\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 410us/step - loss: 1.3287 - val_loss: 1.3463\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3268 - val_loss: 1.3419\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3258 - val_loss: 1.3389\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3250 - val_loss: 1.3395\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3243 - val_loss: 1.3349\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3238 - val_loss: 1.3340\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3233 - val_loss: 1.3333\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3230 - val_loss: 1.3315\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3225 - val_loss: 1.3299\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3220 - val_loss: 1.3297\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3214 - val_loss: 1.3291\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3209 - val_loss: 1.3287\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3205 - val_loss: 1.3283\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3202 - val_loss: 1.3277\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3199 - val_loss: 1.3295\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3196 - val_loss: 1.3285\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3193 - val_loss: 1.3287\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3192 - val_loss: 1.3271\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3189 - val_loss: 1.3253\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3189 - val_loss: 1.3260\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3186 - val_loss: 1.3254\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3184 - val_loss: 1.3248\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3183 - val_loss: 1.3251\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3180 - val_loss: 1.3245\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3179 - val_loss: 1.3236\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3177 - val_loss: 1.3235\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3176 - val_loss: 1.3235\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3175 - val_loss: 1.3229\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3174 - val_loss: 1.3232\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3173 - val_loss: 1.3225\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3224\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3170 - val_loss: 1.3222\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3170 - val_loss: 1.3235\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3168 - val_loss: 1.3221\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3167 - val_loss: 1.3220\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3167 - val_loss: 1.3222\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3165 - val_loss: 1.3218\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3165 - val_loss: 1.3227\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3165 - val_loss: 1.3213\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3163 - val_loss: 1.3211\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3161 - val_loss: 1.3214\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3161 - val_loss: 1.3214\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3160 - val_loss: 1.3209\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3159 - val_loss: 1.3206\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3159 - val_loss: 1.3207\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3159 - val_loss: 1.3204\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3157 - val_loss: 1.3209\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3156 - val_loss: 1.3209\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3155 - val_loss: 1.3207\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3154 - val_loss: 1.3207\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3154 - val_loss: 1.3195\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3153 - val_loss: 1.3204\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3153 - val_loss: 1.3205\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3152 - val_loss: 1.3197\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3152 - val_loss: 1.3195\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3153 - val_loss: 1.3196\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3150 - val_loss: 1.3198\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3151 - val_loss: 1.3196\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3151 - val_loss: 1.3196\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3149 - val_loss: 1.3190\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3148 - val_loss: 1.3191\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3148 - val_loss: 1.3189\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3147 - val_loss: 1.3192\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3148 - val_loss: 1.3189\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3146 - val_loss: 1.3187\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3145 - val_loss: 1.3185\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3146 - val_loss: 1.3190\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3145 - val_loss: 1.3194\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3145 - val_loss: 1.3186\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3144 - val_loss: 1.3187\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3142 - val_loss: 1.3181\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3142 - val_loss: 1.3180\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3141 - val_loss: 1.3181\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3141 - val_loss: 1.3181\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3141 - val_loss: 1.3180\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3140 - val_loss: 1.3178\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3139 - val_loss: 1.3179\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3139 - val_loss: 1.3177\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3139 - val_loss: 1.3181\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3139 - val_loss: 1.3179\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3137 - val_loss: 1.3179\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3139 - val_loss: 1.3183\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3136 - val_loss: 1.3183\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3137 - val_loss: 1.3173\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3136 - val_loss: 1.3178\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3135 - val_loss: 1.3177\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3135 - val_loss: 1.3176\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3135 - val_loss: 1.3173\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3135 - val_loss: 1.3171\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3133 - val_loss: 1.3174\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3133 - val_loss: 1.3173\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3175\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3133 - val_loss: 1.3174\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3132 - val_loss: 1.3168\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3132 - val_loss: 1.3170\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3173\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3132 - val_loss: 1.3168\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3131 - val_loss: 1.3174\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3131 - val_loss: 1.3181\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3129 - val_loss: 1.3166\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3130 - val_loss: 1.3164\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3130 - val_loss: 1.3165\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3129 - val_loss: 1.3165\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3129 - val_loss: 1.3167\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3128 - val_loss: 1.3165\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3128 - val_loss: 1.3165\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3128 - val_loss: 1.3165\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3128 - val_loss: 1.3165\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3126 - val_loss: 1.3163\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3126 - val_loss: 1.3159\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3125 - val_loss: 1.3158\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3162\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3126 - val_loss: 1.3158\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3124 - val_loss: 1.3159\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3124 - val_loss: 1.3155\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3124 - val_loss: 1.3163\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3123 - val_loss: 1.3156\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3123 - val_loss: 1.3156\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3123 - val_loss: 1.3157\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3123 - val_loss: 1.3156\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3159\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3123 - val_loss: 1.3160\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3122 - val_loss: 1.3155\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3154\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3121 - val_loss: 1.3152\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3121 - val_loss: 1.3151\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3153\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3121 - val_loss: 1.3153\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3120 - val_loss: 1.3153\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3121 - val_loss: 1.3153\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3120 - val_loss: 1.3151\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3153\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3119 - val_loss: 1.3151\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3120 - val_loss: 1.3158\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3152\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3119 - val_loss: 1.3153\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3119 - val_loss: 1.3154\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3118 - val_loss: 1.3149\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3119 - val_loss: 1.3149\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3118 - val_loss: 1.3150\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3118 - val_loss: 1.3149\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3118 - val_loss: 1.3150\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3118 - val_loss: 1.3148\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3117 - val_loss: 1.3150\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967997 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9255732\n","The max value of N 0.85919756\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","=====================\n","AUROC 0.0 0.7411813333333334\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.7487409999999999, 0.7448013333333333, 0.7648995000000001, 0.7229093333333333, 0.7503966666666666, 0.7379086666666668, 0.7764223333333332, 0.7572363333333334, 0.7427530000000001, 0.7411813333333334]\n","AUROC ===== 0.7487249500000001 +/- 0.01412587274354912\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7fd1a0d81898>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"lv-Y19P4ShpX","colab_type":"text"},"cell_type":"markdown","source":["##** Bird vs All **##"]},{"metadata":{"id":"pwPagZNWSfOT","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":64353},"outputId":"f5f0e620-824a-494e-9a24-dd434645dd2e","executionInfo":{"status":"ok","timestamp":1541713309451,"user_tz":-660,"elapsed":3952378,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/DCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/DCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 273s 2us/step\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.3587 - val_loss: 1.4187\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 477us/step - loss: 1.3321 - val_loss: 1.3721\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 482us/step - loss: 1.3285 - val_loss: 1.3462\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 477us/step - loss: 1.3268 - val_loss: 1.3399\n","Epoch 5/150\n","5850/5850 [==============================] - 3s 472us/step - loss: 1.3257 - val_loss: 1.3363\n","Epoch 6/150\n","5850/5850 [==============================] - 3s 471us/step - loss: 1.3246 - val_loss: 1.3330\n","Epoch 7/150\n","5850/5850 [==============================] - 3s 467us/step - loss: 1.3238 - val_loss: 1.3318\n","Epoch 8/150\n","5850/5850 [==============================] - 3s 467us/step - loss: 1.3233 - val_loss: 1.3335\n","Epoch 9/150\n","5850/5850 [==============================] - 3s 464us/step - loss: 1.3229 - val_loss: 1.3310\n","Epoch 10/150\n","5850/5850 [==============================] - 3s 464us/step - loss: 1.3223 - val_loss: 1.3300\n","Epoch 11/150\n","5850/5850 [==============================] - 3s 469us/step - loss: 1.3219 - val_loss: 1.3294\n","Epoch 12/150\n","5850/5850 [==============================] - 3s 466us/step - loss: 1.3214 - val_loss: 1.3297\n","Epoch 13/150\n","5850/5850 [==============================] - 3s 467us/step - loss: 1.3212 - val_loss: 1.3291\n","Epoch 14/150\n","5850/5850 [==============================] - 3s 466us/step - loss: 1.3208 - val_loss: 1.3296\n","Epoch 15/150\n","5850/5850 [==============================] - 3s 467us/step - loss: 1.3205 - val_loss: 1.3274\n","Epoch 16/150\n","5850/5850 [==============================] - 3s 470us/step - loss: 1.3202 - val_loss: 1.3272\n","Epoch 17/150\n","5850/5850 [==============================] - 3s 463us/step - loss: 1.3200 - val_loss: 1.3268\n","Epoch 18/150\n","5850/5850 [==============================] - 3s 469us/step - loss: 1.3200 - val_loss: 1.3263\n","Epoch 19/150\n","5850/5850 [==============================] - 3s 469us/step - loss: 1.3198 - val_loss: 1.3269\n","Epoch 20/150\n","5850/5850 [==============================] - 3s 475us/step - loss: 1.3195 - val_loss: 1.3273\n","Epoch 21/150\n","5850/5850 [==============================] - 3s 468us/step - loss: 1.3193 - val_loss: 1.3255\n","Epoch 22/150\n","5850/5850 [==============================] - 3s 465us/step - loss: 1.3191 - val_loss: 1.3260\n","Epoch 23/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3189 - val_loss: 1.3251\n","Epoch 24/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3189 - val_loss: 1.3257\n","Epoch 25/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3188 - val_loss: 1.3276\n","Epoch 26/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3186 - val_loss: 1.3249\n","Epoch 27/150\n","5850/5850 [==============================] - 3s 464us/step - loss: 1.3184 - val_loss: 1.3251\n","Epoch 28/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3183 - val_loss: 1.3253\n","Epoch 29/150\n","5850/5850 [==============================] - 3s 465us/step - loss: 1.3182 - val_loss: 1.3248\n","Epoch 30/150\n","5850/5850 [==============================] - 3s 467us/step - loss: 1.3181 - val_loss: 1.3245\n","Epoch 31/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3179 - val_loss: 1.3232\n","Epoch 32/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3178 - val_loss: 1.3234\n","Epoch 33/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3177 - val_loss: 1.3243\n","Epoch 34/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3177 - val_loss: 1.3230\n","Epoch 35/150\n","5850/5850 [==============================] - 3s 465us/step - loss: 1.3175 - val_loss: 1.3229\n","Epoch 36/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3174 - val_loss: 1.3230\n","Epoch 37/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3173 - val_loss: 1.3235\n","Epoch 38/150\n","5850/5850 [==============================] - 3s 463us/step - loss: 1.3173 - val_loss: 1.3221\n","Epoch 39/150\n","5850/5850 [==============================] - 3s 463us/step - loss: 1.3174 - val_loss: 1.3230\n","Epoch 40/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3172 - val_loss: 1.3223\n","Epoch 41/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3169 - val_loss: 1.3220\n","Epoch 42/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3169 - val_loss: 1.3223\n","Epoch 43/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3168 - val_loss: 1.3219\n","Epoch 44/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3167 - val_loss: 1.3219\n","Epoch 45/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3167 - val_loss: 1.3217\n","Epoch 46/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3166 - val_loss: 1.3218\n","Epoch 47/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3165 - val_loss: 1.3223\n","Epoch 48/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3164 - val_loss: 1.3213\n","Epoch 49/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3164 - val_loss: 1.3224\n","Epoch 50/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3163 - val_loss: 1.3213\n","Epoch 51/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3162 - val_loss: 1.3219\n","Epoch 52/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3162 - val_loss: 1.3220\n","Epoch 53/150\n","5850/5850 [==============================] - 3s 444us/step - loss: 1.3161 - val_loss: 1.3215\n","Epoch 54/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3160 - val_loss: 1.3211\n","Epoch 55/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3159 - val_loss: 1.3211\n","Epoch 56/150\n","5850/5850 [==============================] - 3s 444us/step - loss: 1.3159 - val_loss: 1.3219\n","Epoch 57/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3159 - val_loss: 1.3207\n","Epoch 58/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3158 - val_loss: 1.3207\n","Epoch 59/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3157 - val_loss: 1.3203\n","Epoch 60/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3157 - val_loss: 1.3207\n","Epoch 61/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3157 - val_loss: 1.3206\n","Epoch 62/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3155 - val_loss: 1.3206\n","Epoch 63/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3155 - val_loss: 1.3201\n","Epoch 64/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3154 - val_loss: 1.3203\n","Epoch 65/150\n","5850/5850 [==============================] - 3s 440us/step - loss: 1.3153 - val_loss: 1.3198\n","Epoch 66/150\n","5850/5850 [==============================] - 3s 444us/step - loss: 1.3152 - val_loss: 1.3191\n","Epoch 67/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3153 - val_loss: 1.3199\n","Epoch 68/150\n","5850/5850 [==============================] - 3s 435us/step - loss: 1.3150 - val_loss: 1.3192\n","Epoch 69/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3149 - val_loss: 1.3193\n","Epoch 70/150\n","5850/5850 [==============================] - 3s 440us/step - loss: 1.3149 - val_loss: 1.3191\n","Epoch 71/150\n","5850/5850 [==============================] - 3s 443us/step - loss: 1.3149 - val_loss: 1.3192\n","Epoch 72/150\n","5850/5850 [==============================] - 3s 441us/step - loss: 1.3148 - val_loss: 1.3196\n","Epoch 73/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3147 - val_loss: 1.3197\n","Epoch 74/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3147 - val_loss: 1.3200\n","Epoch 75/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3146 - val_loss: 1.3189\n","Epoch 76/150\n","5850/5850 [==============================] - 3s 459us/step - loss: 1.3145 - val_loss: 1.3190\n","Epoch 77/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3144 - val_loss: 1.3189\n","Epoch 78/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3144 - val_loss: 1.3190\n","Epoch 79/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3144 - val_loss: 1.3191\n","Epoch 80/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3145 - val_loss: 1.3188\n","Epoch 81/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3143 - val_loss: 1.3184\n","Epoch 82/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3142 - val_loss: 1.3183\n","Epoch 83/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3143 - val_loss: 1.3193\n","Epoch 84/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3141 - val_loss: 1.3184\n","Epoch 85/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3140 - val_loss: 1.3179\n","Epoch 86/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3140 - val_loss: 1.3185\n","Epoch 87/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3141 - val_loss: 1.3176\n","Epoch 88/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3140 - val_loss: 1.3181\n","Epoch 89/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3141 - val_loss: 1.3184\n","Epoch 90/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3141 - val_loss: 1.3181\n","Epoch 91/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3139 - val_loss: 1.3187\n","Epoch 92/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3139 - val_loss: 1.3176\n","Epoch 93/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3137 - val_loss: 1.3185\n","Epoch 94/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3138 - val_loss: 1.3178\n","Epoch 95/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3138 - val_loss: 1.3181\n","Epoch 96/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3138 - val_loss: 1.3176\n","Epoch 97/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3136 - val_loss: 1.3170\n","Epoch 98/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.3135 - val_loss: 1.3175\n","Epoch 99/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3136 - val_loss: 1.3173\n","Epoch 100/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3135 - val_loss: 1.3170\n","Epoch 101/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3135 - val_loss: 1.3170\n","Epoch 102/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3135 - val_loss: 1.3177\n","Epoch 103/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3135 - val_loss: 1.3167\n","Epoch 104/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3134 - val_loss: 1.3169\n","Epoch 105/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3134 - val_loss: 1.3174\n","Epoch 106/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3135 - val_loss: 1.3173\n","Epoch 107/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3133 - val_loss: 1.3169\n","Epoch 108/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3134 - val_loss: 1.3170\n","Epoch 109/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3133 - val_loss: 1.3170\n","Epoch 110/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3133 - val_loss: 1.3171\n","Epoch 111/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3132 - val_loss: 1.3166\n","Epoch 112/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 113/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3132 - val_loss: 1.3167\n","Epoch 114/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3132 - val_loss: 1.3167\n","Epoch 115/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3133 - val_loss: 1.3166\n","Epoch 116/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 117/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 118/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3130 - val_loss: 1.3162\n","Epoch 119/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3130 - val_loss: 1.3161\n","Epoch 120/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 121/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 122/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3130 - val_loss: 1.3164\n","Epoch 123/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3129 - val_loss: 1.3160\n","Epoch 124/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3129 - val_loss: 1.3159\n","Epoch 125/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3129 - val_loss: 1.3158\n","Epoch 126/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3129 - val_loss: 1.3159\n","Epoch 127/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3128 - val_loss: 1.3162\n","Epoch 128/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3128 - val_loss: 1.3170\n","Epoch 129/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3130 - val_loss: 1.3168\n","Epoch 130/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3128 - val_loss: 1.3162\n","Epoch 131/150\n","5850/5850 [==============================] - 3s 458us/step - loss: 1.3128 - val_loss: 1.3159\n","Epoch 132/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3128 - val_loss: 1.3166\n","Epoch 133/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3128 - val_loss: 1.3160\n","Epoch 134/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3128 - val_loss: 1.3159\n","Epoch 135/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3128 - val_loss: 1.3163\n","Epoch 136/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3127 - val_loss: 1.3163\n","Epoch 137/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3126 - val_loss: 1.3159\n","Epoch 138/150\n","5850/5850 [==============================] - 3s 454us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 139/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3126 - val_loss: 1.3155\n","Epoch 140/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3126 - val_loss: 1.3158\n","Epoch 141/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3125 - val_loss: 1.3157\n","Epoch 142/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3126 - val_loss: 1.3155\n","Epoch 143/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3127 - val_loss: 1.3157\n","Epoch 144/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3125 - val_loss: 1.3158\n","Epoch 145/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3125 - val_loss: 1.3156\n","Epoch 146/150\n","5850/5850 [==============================] - 3s 455us/step - loss: 1.3125 - val_loss: 1.3155\n","Epoch 147/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3124 - val_loss: 1.3160\n","Epoch 148/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3124 - val_loss: 1.3156\n","Epoch 149/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3124 - val_loss: 1.3157\n","Epoch 150/150\n","5850/5850 [==============================] - 3s 456us/step - loss: 1.3125 - val_loss: 1.3158\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967996 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.88267744\n","The max value of N 0.8657019\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7385896666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 777us/step - loss: 1.3591 - val_loss: 1.3880\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3320 - val_loss: 1.3534\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3280 - val_loss: 1.3453\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3263 - val_loss: 1.3396\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3251 - val_loss: 1.3378\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3243 - val_loss: 1.3333\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3234 - val_loss: 1.3316\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3229 - val_loss: 1.3307\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3222 - val_loss: 1.3302\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3217 - val_loss: 1.3308\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3214 - val_loss: 1.3306\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3210 - val_loss: 1.3295\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3207 - val_loss: 1.3284\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3206 - val_loss: 1.3276\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3202 - val_loss: 1.3272\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3199 - val_loss: 1.3275\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3197 - val_loss: 1.3274\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3199 - val_loss: 1.3265\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3194 - val_loss: 1.3272\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3192 - val_loss: 1.3279\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3191 - val_loss: 1.3256\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3188 - val_loss: 1.3251\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3188 - val_loss: 1.3256\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3185 - val_loss: 1.3240\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3183 - val_loss: 1.3239\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3183 - val_loss: 1.3248\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3180 - val_loss: 1.3236\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3180 - val_loss: 1.3238\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3179 - val_loss: 1.3242\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3179 - val_loss: 1.3259\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3178 - val_loss: 1.3235\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3176 - val_loss: 1.3226\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3174 - val_loss: 1.3230\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3173 - val_loss: 1.3224\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3174 - val_loss: 1.3227\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3173 - val_loss: 1.3222\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3171 - val_loss: 1.3228\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3172 - val_loss: 1.3230\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3169 - val_loss: 1.3223\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3167 - val_loss: 1.3217\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3168 - val_loss: 1.3221\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3166 - val_loss: 1.3217\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3165 - val_loss: 1.3223\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3164 - val_loss: 1.3214\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3165 - val_loss: 1.3209\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3163 - val_loss: 1.3208\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3163 - val_loss: 1.3220\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3163 - val_loss: 1.3227\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3162 - val_loss: 1.3219\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3163 - val_loss: 1.3226\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3160 - val_loss: 1.3206\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3160 - val_loss: 1.3205\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3159 - val_loss: 1.3201\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3158 - val_loss: 1.3215\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3159 - val_loss: 1.3206\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3158 - val_loss: 1.3210\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3156 - val_loss: 1.3205\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3156 - val_loss: 1.3200\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3157 - val_loss: 1.3205\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3153 - val_loss: 1.3221\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3153 - val_loss: 1.3203\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3151 - val_loss: 1.3196\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3151 - val_loss: 1.3198\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3152 - val_loss: 1.3198\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3151 - val_loss: 1.3212\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3149 - val_loss: 1.3200\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3149 - val_loss: 1.3192\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3149 - val_loss: 1.3194\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3149 - val_loss: 1.3217\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3147 - val_loss: 1.3192\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3147 - val_loss: 1.3191\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3147 - val_loss: 1.3187\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3146 - val_loss: 1.3193\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3145 - val_loss: 1.3186\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3146 - val_loss: 1.3187\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3143 - val_loss: 1.3184\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3143 - val_loss: 1.3185\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3143 - val_loss: 1.3182\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3143 - val_loss: 1.3182\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3142 - val_loss: 1.3183\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3141 - val_loss: 1.3178\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3142 - val_loss: 1.3182\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3140 - val_loss: 1.3180\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3140 - val_loss: 1.3178\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3140 - val_loss: 1.3175\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3139 - val_loss: 1.3173\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3139 - val_loss: 1.3180\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3139 - val_loss: 1.3177\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3139 - val_loss: 1.3175\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3138 - val_loss: 1.3174\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3138 - val_loss: 1.3171\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3137 - val_loss: 1.3173\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3137 - val_loss: 1.3171\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3137 - val_loss: 1.3172\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3136 - val_loss: 1.3172\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3136 - val_loss: 1.3175\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3134 - val_loss: 1.3169\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3135 - val_loss: 1.3170\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3135 - val_loss: 1.3168\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3134 - val_loss: 1.3166\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3134 - val_loss: 1.3167\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3133 - val_loss: 1.3166\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3134 - val_loss: 1.3168\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3131 - val_loss: 1.3164\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3132 - val_loss: 1.3163\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3131 - val_loss: 1.3161\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3132 - val_loss: 1.3163\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3132 - val_loss: 1.3162\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3165\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3130 - val_loss: 1.3163\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3130 - val_loss: 1.3159\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3131 - val_loss: 1.3163\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3130 - val_loss: 1.3173\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3130 - val_loss: 1.3163\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3129 - val_loss: 1.3160\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3129 - val_loss: 1.3161\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3128 - val_loss: 1.3163\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3128 - val_loss: 1.3160\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3127 - val_loss: 1.3157\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3127 - val_loss: 1.3161\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3127 - val_loss: 1.3154\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3127 - val_loss: 1.3155\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3154\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3125 - val_loss: 1.3155\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3125 - val_loss: 1.3154\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3125 - val_loss: 1.3155\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3126 - val_loss: 1.3157\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3125 - val_loss: 1.3151\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3125 - val_loss: 1.3153\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3125 - val_loss: 1.3152\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3125 - val_loss: 1.3156\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3126 - val_loss: 1.3152\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3124 - val_loss: 1.3155\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3124 - val_loss: 1.3152\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3124 - val_loss: 1.3150\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3123 - val_loss: 1.3151\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3123 - val_loss: 1.3150\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3123 - val_loss: 1.3151\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3124 - val_loss: 1.3152\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3123 - val_loss: 1.3149\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3123 - val_loss: 1.3150\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3123 - val_loss: 1.3150\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967997 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.875166\n","The max value of N 0.86648846\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.720618\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 924us/step - loss: 1.3534 - val_loss: 1.3986\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3289 - val_loss: 1.3645\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3255 - val_loss: 1.3426\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3235 - val_loss: 1.3347\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3224 - val_loss: 1.3337\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3214 - val_loss: 1.3314\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3207 - val_loss: 1.3299\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3202 - val_loss: 1.3311\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3197 - val_loss: 1.3287\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3192 - val_loss: 1.3272\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3190 - val_loss: 1.3264\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3185 - val_loss: 1.3255\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3182 - val_loss: 1.3258\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3179 - val_loss: 1.3243\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3177 - val_loss: 1.3241\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3175 - val_loss: 1.3236\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3173 - val_loss: 1.3236\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3171 - val_loss: 1.3232\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3170 - val_loss: 1.3240\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3169 - val_loss: 1.3232\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3167 - val_loss: 1.3221\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3166 - val_loss: 1.3225\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3162 - val_loss: 1.3216\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3160 - val_loss: 1.3214\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3159 - val_loss: 1.3220\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3159 - val_loss: 1.3218\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3158 - val_loss: 1.3207\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3157 - val_loss: 1.3215\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3156 - val_loss: 1.3216\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3154 - val_loss: 1.3209\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3153 - val_loss: 1.3205\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3152 - val_loss: 1.3219\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3150 - val_loss: 1.3200\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3149 - val_loss: 1.3199\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3149 - val_loss: 1.3193\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3148 - val_loss: 1.3194\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3146 - val_loss: 1.3196\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3145 - val_loss: 1.3194\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3144 - val_loss: 1.3194\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3143 - val_loss: 1.3197\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3143 - val_loss: 1.3197\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3143 - val_loss: 1.3200\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3143 - val_loss: 1.3189\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3141 - val_loss: 1.3187\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3140 - val_loss: 1.3189\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3138 - val_loss: 1.3189\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3137 - val_loss: 1.3186\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3137 - val_loss: 1.3185\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3136 - val_loss: 1.3187\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3137 - val_loss: 1.3180\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3136 - val_loss: 1.3178\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3135 - val_loss: 1.3185\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3133 - val_loss: 1.3192\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3133 - val_loss: 1.3181\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3131 - val_loss: 1.3184\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3130 - val_loss: 1.3181\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3184\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3129 - val_loss: 1.3170\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3127 - val_loss: 1.3183\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3128 - val_loss: 1.3167\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3127 - val_loss: 1.3168\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3186\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3126 - val_loss: 1.3167\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3180\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3126 - val_loss: 1.3171\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3125 - val_loss: 1.3166\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3123 - val_loss: 1.3161\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3123 - val_loss: 1.3165\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3122 - val_loss: 1.3160\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3121 - val_loss: 1.3157\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3121 - val_loss: 1.3160\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3120 - val_loss: 1.3158\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3120 - val_loss: 1.3164\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3120 - val_loss: 1.3157\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3118 - val_loss: 1.3160\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3118 - val_loss: 1.3155\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3118 - val_loss: 1.3152\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3119 - val_loss: 1.3163\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3117 - val_loss: 1.3164\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3117 - val_loss: 1.3155\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3116 - val_loss: 1.3162\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3116 - val_loss: 1.3156\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3115 - val_loss: 1.3150\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3115 - val_loss: 1.3148\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3116 - val_loss: 1.3161\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3114 - val_loss: 1.3148\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3113 - val_loss: 1.3149\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3113 - val_loss: 1.3147\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3114 - val_loss: 1.3151\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3113 - val_loss: 1.3152\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3112 - val_loss: 1.3150\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3113 - val_loss: 1.3146\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3112 - val_loss: 1.3148\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3112 - val_loss: 1.3145\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3111 - val_loss: 1.3143\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3111 - val_loss: 1.3145\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3110 - val_loss: 1.3143\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3110 - val_loss: 1.3142\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3111 - val_loss: 1.3146\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3109 - val_loss: 1.3144\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3110 - val_loss: 1.3140\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3109 - val_loss: 1.3146\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3108 - val_loss: 1.3142\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3109 - val_loss: 1.3146\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3107 - val_loss: 1.3140\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3107 - val_loss: 1.3136\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3107 - val_loss: 1.3143\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3108 - val_loss: 1.3136\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3107 - val_loss: 1.3137\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3106 - val_loss: 1.3135\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3107 - val_loss: 1.3144\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3106 - val_loss: 1.3137\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3105 - val_loss: 1.3133\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3106 - val_loss: 1.3137\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3106 - val_loss: 1.3136\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3104 - val_loss: 1.3142\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3106 - val_loss: 1.3132\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3105 - val_loss: 1.3136\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3103 - val_loss: 1.3133\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3104 - val_loss: 1.3133\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3104 - val_loss: 1.3131\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3104 - val_loss: 1.3135\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3104 - val_loss: 1.3137\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3104 - val_loss: 1.3132\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3103 - val_loss: 1.3134\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3103 - val_loss: 1.3138\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3104 - val_loss: 1.3132\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3102 - val_loss: 1.3129\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3102 - val_loss: 1.3132\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3102 - val_loss: 1.3131\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3103 - val_loss: 1.3131\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3102 - val_loss: 1.3128\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3101 - val_loss: 1.3130\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3101 - val_loss: 1.3128\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3101 - val_loss: 1.3133\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3101 - val_loss: 1.3132\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3101 - val_loss: 1.3129\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3101 - val_loss: 1.3127\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3100 - val_loss: 1.3129\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3100 - val_loss: 1.3129\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3099 - val_loss: 1.3127\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3100 - val_loss: 1.3129\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3100 - val_loss: 1.3129\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3099 - val_loss: 1.3127\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3100 - val_loss: 1.3128\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3100 - val_loss: 1.3128\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3099 - val_loss: 1.3125\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3098 - val_loss: 1.3125\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3099 - val_loss: 1.3125\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3099 - val_loss: 1.3126\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967997 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.89569867\n","The max value of N 0.84850615\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7230293333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.3584 - val_loss: 1.4214\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 437us/step - loss: 1.3321 - val_loss: 1.3716\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3284 - val_loss: 1.3450\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3264 - val_loss: 1.3383\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3252 - val_loss: 1.3334\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3243 - val_loss: 1.3338\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3236 - val_loss: 1.3336\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3228 - val_loss: 1.3315\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3223 - val_loss: 1.3307\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3220 - val_loss: 1.3304\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3216 - val_loss: 1.3304\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3215 - val_loss: 1.3297\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3209 - val_loss: 1.3300\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3206 - val_loss: 1.3273\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3203 - val_loss: 1.3264\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3201 - val_loss: 1.3261\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3200 - val_loss: 1.3271\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3197 - val_loss: 1.3256\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3196 - val_loss: 1.3258\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3193 - val_loss: 1.3250\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3192 - val_loss: 1.3250\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3190 - val_loss: 1.3249\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3188 - val_loss: 1.3246\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3186 - val_loss: 1.3253\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3185 - val_loss: 1.3240\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3185 - val_loss: 1.3256\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3184 - val_loss: 1.3234\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3182 - val_loss: 1.3246\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3181 - val_loss: 1.3230\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3181 - val_loss: 1.3234\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3181 - val_loss: 1.3240\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3176 - val_loss: 1.3238\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3175 - val_loss: 1.3248\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3174 - val_loss: 1.3232\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3174 - val_loss: 1.3226\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3175 - val_loss: 1.3258\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3174 - val_loss: 1.3246\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3172 - val_loss: 1.3217\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3170 - val_loss: 1.3218\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3169 - val_loss: 1.3232\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3169 - val_loss: 1.3214\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3168 - val_loss: 1.3211\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3167 - val_loss: 1.3231\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3166 - val_loss: 1.3212\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3167 - val_loss: 1.3208\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3164 - val_loss: 1.3212\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3164 - val_loss: 1.3209\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3162 - val_loss: 1.3204\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3164 - val_loss: 1.3202\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3162 - val_loss: 1.3202\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3162 - val_loss: 1.3214\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3160 - val_loss: 1.3207\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3161 - val_loss: 1.3212\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3160 - val_loss: 1.3203\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3159 - val_loss: 1.3209\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3158 - val_loss: 1.3205\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3158 - val_loss: 1.3204\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3156 - val_loss: 1.3209\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3155 - val_loss: 1.3197\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3155 - val_loss: 1.3199\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3155 - val_loss: 1.3204\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3154 - val_loss: 1.3201\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3153 - val_loss: 1.3196\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3152 - val_loss: 1.3202\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3151 - val_loss: 1.3196\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3150 - val_loss: 1.3188\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3152 - val_loss: 1.3199\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3150 - val_loss: 1.3194\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3149 - val_loss: 1.3203\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3149 - val_loss: 1.3191\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3148 - val_loss: 1.3189\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3148 - val_loss: 1.3187\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3147 - val_loss: 1.3180\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3147 - val_loss: 1.3182\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3147 - val_loss: 1.3192\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3145 - val_loss: 1.3190\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3145 - val_loss: 1.3191\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3181\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3144 - val_loss: 1.3195\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3144 - val_loss: 1.3176\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3145 - val_loss: 1.3178\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3143 - val_loss: 1.3188\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3143 - val_loss: 1.3188\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3142 - val_loss: 1.3180\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3142 - val_loss: 1.3171\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3142 - val_loss: 1.3184\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3141 - val_loss: 1.3181\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3141 - val_loss: 1.3186\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3142 - val_loss: 1.3173\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3140 - val_loss: 1.3177\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3140 - val_loss: 1.3171\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3139 - val_loss: 1.3175\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3139 - val_loss: 1.3167\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3139 - val_loss: 1.3169\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3139 - val_loss: 1.3169\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3139 - val_loss: 1.3175\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3136 - val_loss: 1.3171\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3137 - val_loss: 1.3165\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3136 - val_loss: 1.3169\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3137 - val_loss: 1.3164\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3135 - val_loss: 1.3172\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3137 - val_loss: 1.3163\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3137 - val_loss: 1.3166\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3135 - val_loss: 1.3172\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3135 - val_loss: 1.3175\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3135 - val_loss: 1.3164\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3134 - val_loss: 1.3166\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3134 - val_loss: 1.3163\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3133 - val_loss: 1.3160\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3133 - val_loss: 1.3161\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3133 - val_loss: 1.3162\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3133 - val_loss: 1.3157\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3132 - val_loss: 1.3161\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3133 - val_loss: 1.3165\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3132 - val_loss: 1.3159\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3132 - val_loss: 1.3160\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3132 - val_loss: 1.3162\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3157\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3131 - val_loss: 1.3166\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3156\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3130 - val_loss: 1.3156\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3158\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3132 - val_loss: 1.3157\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3130 - val_loss: 1.3157\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3129 - val_loss: 1.3153\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3129 - val_loss: 1.3155\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3130 - val_loss: 1.3155\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3129 - val_loss: 1.3157\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3128 - val_loss: 1.3153\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3129 - val_loss: 1.3159\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3128 - val_loss: 1.3154\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3128 - val_loss: 1.3153\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3151\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3128 - val_loss: 1.3153\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3127 - val_loss: 1.3156\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3127 - val_loss: 1.3150\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3127 - val_loss: 1.3150\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3126 - val_loss: 1.3154\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3128 - val_loss: 1.3155\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3127 - val_loss: 1.3150\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3126 - val_loss: 1.3151\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3126 - val_loss: 1.3150\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3125 - val_loss: 1.3149\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3125 - val_loss: 1.3151\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3125 - val_loss: 1.3150\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967996 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8184629\n","The max value of N 0.8783217\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.693906\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.3509 - val_loss: 1.4118\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 431us/step - loss: 1.3258 - val_loss: 1.3537\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3224 - val_loss: 1.3355\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3208 - val_loss: 1.3311\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3195 - val_loss: 1.3299\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3187 - val_loss: 1.3281\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3179 - val_loss: 1.3254\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3172 - val_loss: 1.3246\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3166 - val_loss: 1.3239\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3163 - val_loss: 1.3240\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3160 - val_loss: 1.3229\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3156 - val_loss: 1.3215\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3153 - val_loss: 1.3219\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3152 - val_loss: 1.3216\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3148 - val_loss: 1.3231\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3145 - val_loss: 1.3206\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3209\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3141 - val_loss: 1.3197\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3139 - val_loss: 1.3203\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3139 - val_loss: 1.3196\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3135 - val_loss: 1.3195\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3134 - val_loss: 1.3192\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3134 - val_loss: 1.3183\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3132 - val_loss: 1.3188\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3131 - val_loss: 1.3182\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3130 - val_loss: 1.3189\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3127 - val_loss: 1.3180\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3126 - val_loss: 1.3177\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3126 - val_loss: 1.3176\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3177\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3125 - val_loss: 1.3190\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3124 - val_loss: 1.3167\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3121 - val_loss: 1.3167\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3120 - val_loss: 1.3164\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3121 - val_loss: 1.3167\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3120 - val_loss: 1.3160\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3119 - val_loss: 1.3161\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3118 - val_loss: 1.3173\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3117 - val_loss: 1.3165\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3115 - val_loss: 1.3162\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3115 - val_loss: 1.3159\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3113 - val_loss: 1.3162\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3112 - val_loss: 1.3173\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3114 - val_loss: 1.3156\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3111 - val_loss: 1.3158\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3110 - val_loss: 1.3153\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3109 - val_loss: 1.3152\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3108 - val_loss: 1.3152\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3107 - val_loss: 1.3150\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3107 - val_loss: 1.3149\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3104 - val_loss: 1.3147\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3104 - val_loss: 1.3155\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3103 - val_loss: 1.3149\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3102 - val_loss: 1.3159\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3102 - val_loss: 1.3149\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3101 - val_loss: 1.3162\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3099 - val_loss: 1.3149\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3099 - val_loss: 1.3139\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3150\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3098 - val_loss: 1.3139\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3141\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3150\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3095 - val_loss: 1.3134\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3094 - val_loss: 1.3148\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3094 - val_loss: 1.3138\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3093 - val_loss: 1.3148\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3093 - val_loss: 1.3147\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3092 - val_loss: 1.3139\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3091 - val_loss: 1.3133\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3090 - val_loss: 1.3128\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3091 - val_loss: 1.3125\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3090 - val_loss: 1.3130\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3089 - val_loss: 1.3132\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3089 - val_loss: 1.3128\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3089 - val_loss: 1.3127\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3088 - val_loss: 1.3146\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3087 - val_loss: 1.3126\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3086 - val_loss: 1.3128\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3087 - val_loss: 1.3125\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3087 - val_loss: 1.3136\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3086 - val_loss: 1.3121\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3085 - val_loss: 1.3122\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3084 - val_loss: 1.3118\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3085 - val_loss: 1.3120\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3083 - val_loss: 1.3114\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3084 - val_loss: 1.3117\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3083 - val_loss: 1.3117\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3084 - val_loss: 1.3122\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3082 - val_loss: 1.3116\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3082 - val_loss: 1.3114\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3081 - val_loss: 1.3113\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3082 - val_loss: 1.3116\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3082 - val_loss: 1.3111\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3080 - val_loss: 1.3122\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3080 - val_loss: 1.3112\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3079 - val_loss: 1.3111\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3080 - val_loss: 1.3112\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3080 - val_loss: 1.3115\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3080 - val_loss: 1.3112\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3079 - val_loss: 1.3113\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3079 - val_loss: 1.3113\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3078 - val_loss: 1.3109\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3077 - val_loss: 1.3109\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3078 - val_loss: 1.3109\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3077 - val_loss: 1.3115\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3076 - val_loss: 1.3110\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3076 - val_loss: 1.3103\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3075 - val_loss: 1.3105\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3075 - val_loss: 1.3106\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3075 - val_loss: 1.3101\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3075 - val_loss: 1.3104\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3075 - val_loss: 1.3104\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3074 - val_loss: 1.3106\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3075 - val_loss: 1.3102\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3075 - val_loss: 1.3107\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3074 - val_loss: 1.3105\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3073 - val_loss: 1.3102\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3074 - val_loss: 1.3106\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3073 - val_loss: 1.3099\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3073 - val_loss: 1.3109\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3072 - val_loss: 1.3098\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3073 - val_loss: 1.3103\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3074 - val_loss: 1.3100\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3072 - val_loss: 1.3103\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3072 - val_loss: 1.3101\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3073 - val_loss: 1.3104\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3073 - val_loss: 1.3110\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3071 - val_loss: 1.3100\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3071 - val_loss: 1.3100\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3071 - val_loss: 1.3098\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3070 - val_loss: 1.3095\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3071 - val_loss: 1.3101\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3071 - val_loss: 1.3095\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3070 - val_loss: 1.3097\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3070 - val_loss: 1.3099\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3069 - val_loss: 1.3095\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3070 - val_loss: 1.3097\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3069 - val_loss: 1.3102\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3070 - val_loss: 1.3094\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3069 - val_loss: 1.3095\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3069 - val_loss: 1.3094\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3069 - val_loss: 1.3095\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3069 - val_loss: 1.3095\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3070 - val_loss: 1.3103\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3102\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3068 - val_loss: 1.3093\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3068 - val_loss: 1.3095\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3067 - val_loss: 1.3092\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3068 - val_loss: 1.3093\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3068 - val_loss: 1.3097\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.90855193\n","The max value of N 0.80974305\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6951360000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.3544 - val_loss: 1.4026\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3312 - val_loss: 1.3662\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3279 - val_loss: 1.3437\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3263 - val_loss: 1.3418\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3249 - val_loss: 1.3396\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3239 - val_loss: 1.3348\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3231 - val_loss: 1.3337\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3225 - val_loss: 1.3308\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3220 - val_loss: 1.3295\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3216 - val_loss: 1.3288\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3213 - val_loss: 1.3285\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3211 - val_loss: 1.3298\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3207 - val_loss: 1.3276\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3204 - val_loss: 1.3282\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3202 - val_loss: 1.3267\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3198 - val_loss: 1.3266\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3195 - val_loss: 1.3259\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3194 - val_loss: 1.3255\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3193 - val_loss: 1.3262\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3194 - val_loss: 1.3255\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3190 - val_loss: 1.3245\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3187 - val_loss: 1.3240\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3187 - val_loss: 1.3246\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3186 - val_loss: 1.3243\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3183 - val_loss: 1.3240\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3182 - val_loss: 1.3269\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3182 - val_loss: 1.3235\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3179 - val_loss: 1.3242\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3178 - val_loss: 1.3230\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3177 - val_loss: 1.3230\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3175 - val_loss: 1.3234\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3176 - val_loss: 1.3227\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3175 - val_loss: 1.3223\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3173 - val_loss: 1.3225\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3173 - val_loss: 1.3220\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3171 - val_loss: 1.3223\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3170 - val_loss: 1.3217\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3169 - val_loss: 1.3224\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3169 - val_loss: 1.3214\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3167 - val_loss: 1.3222\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3167 - val_loss: 1.3217\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3165 - val_loss: 1.3214\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3164 - val_loss: 1.3210\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3164 - val_loss: 1.3214\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3164 - val_loss: 1.3216\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3162 - val_loss: 1.3214\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3161 - val_loss: 1.3218\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3160 - val_loss: 1.3209\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3160 - val_loss: 1.3212\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3161 - val_loss: 1.3208\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3159 - val_loss: 1.3208\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3158 - val_loss: 1.3210\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3156 - val_loss: 1.3205\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3157 - val_loss: 1.3207\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3156 - val_loss: 1.3199\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3155 - val_loss: 1.3205\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3155 - val_loss: 1.3198\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3154 - val_loss: 1.3198\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3152 - val_loss: 1.3193\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3151 - val_loss: 1.3201\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3152 - val_loss: 1.3195\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3150 - val_loss: 1.3192\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3149 - val_loss: 1.3193\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3150 - val_loss: 1.3192\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3148 - val_loss: 1.3206\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3148 - val_loss: 1.3193\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3147 - val_loss: 1.3188\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3146 - val_loss: 1.3200\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3145 - val_loss: 1.3192\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3145 - val_loss: 1.3184\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3182\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3145 - val_loss: 1.3188\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3143 - val_loss: 1.3189\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3143 - val_loss: 1.3180\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3142 - val_loss: 1.3180\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3142 - val_loss: 1.3180\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3142 - val_loss: 1.3182\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3141 - val_loss: 1.3182\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3140 - val_loss: 1.3172\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3140 - val_loss: 1.3173\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3139 - val_loss: 1.3178\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3138 - val_loss: 1.3172\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3139 - val_loss: 1.3172\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3138 - val_loss: 1.3169\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3137 - val_loss: 1.3171\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3137 - val_loss: 1.3175\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3137 - val_loss: 1.3177\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3137 - val_loss: 1.3171\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3136 - val_loss: 1.3170\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3137 - val_loss: 1.3177\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3136 - val_loss: 1.3168\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3135 - val_loss: 1.3175\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3134 - val_loss: 1.3170\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3134 - val_loss: 1.3168\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3134 - val_loss: 1.3165\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3133 - val_loss: 1.3171\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3134 - val_loss: 1.3167\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3132 - val_loss: 1.3166\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3135 - val_loss: 1.3165\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3133 - val_loss: 1.3164\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3160\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3130 - val_loss: 1.3161\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3130 - val_loss: 1.3162\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3129 - val_loss: 1.3158\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3129 - val_loss: 1.3157\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3129 - val_loss: 1.3158\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3130 - val_loss: 1.3157\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3129 - val_loss: 1.3157\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3128 - val_loss: 1.3164\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3128 - val_loss: 1.3157\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3127 - val_loss: 1.3155\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3128 - val_loss: 1.3156\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3128 - val_loss: 1.3161\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3128 - val_loss: 1.3155\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3127 - val_loss: 1.3152\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3126 - val_loss: 1.3155\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3127 - val_loss: 1.3155\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3127 - val_loss: 1.3153\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3126 - val_loss: 1.3153\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3126 - val_loss: 1.3155\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3128 - val_loss: 1.3155\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3126 - val_loss: 1.3151\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3125 - val_loss: 1.3153\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3126 - val_loss: 1.3154\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3126 - val_loss: 1.3152\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3125 - val_loss: 1.3152\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3124 - val_loss: 1.3153\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3124 - val_loss: 1.3151\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3123 - val_loss: 1.3151\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3124 - val_loss: 1.3148\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3151\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3123 - val_loss: 1.3150\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3123 - val_loss: 1.3147\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3123 - val_loss: 1.3147\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3146\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3122 - val_loss: 1.3150\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3123 - val_loss: 1.3149\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3123 - val_loss: 1.3153\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3155\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3152\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3151\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3150\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967994 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.84756726\n","The max value of N 0.85836565\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.713806\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 1ms/step - loss: 1.3611 - val_loss: 1.4029\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 433us/step - loss: 1.3358 - val_loss: 1.3593\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3325 - val_loss: 1.3465\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3302 - val_loss: 1.3407\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3289 - val_loss: 1.3393\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3276 - val_loss: 1.3375\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3270 - val_loss: 1.3356\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3262 - val_loss: 1.3359\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3257 - val_loss: 1.3343\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3253 - val_loss: 1.3335\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3251 - val_loss: 1.3322\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3247 - val_loss: 1.3327\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3245 - val_loss: 1.3315\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3241 - val_loss: 1.3311\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3238 - val_loss: 1.3299\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3236 - val_loss: 1.3303\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3234 - val_loss: 1.3293\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3233 - val_loss: 1.3287\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3232 - val_loss: 1.3293\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3229 - val_loss: 1.3290\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3227 - val_loss: 1.3284\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3225 - val_loss: 1.3277\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3225 - val_loss: 1.3279\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3225 - val_loss: 1.3283\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3222 - val_loss: 1.3290\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3220 - val_loss: 1.3267\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3218 - val_loss: 1.3266\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3218 - val_loss: 1.3268\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3219 - val_loss: 1.3268\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3215 - val_loss: 1.3282\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3213 - val_loss: 1.3261\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3213 - val_loss: 1.3270\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3212 - val_loss: 1.3259\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3210 - val_loss: 1.3250\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3210 - val_loss: 1.3260\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3209 - val_loss: 1.3252\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3207 - val_loss: 1.3257\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3207 - val_loss: 1.3256\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3206 - val_loss: 1.3252\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3205 - val_loss: 1.3252\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3205 - val_loss: 1.3252\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3203 - val_loss: 1.3251\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3202 - val_loss: 1.3252\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3202 - val_loss: 1.3247\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3201 - val_loss: 1.3252\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3202 - val_loss: 1.3246\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3200 - val_loss: 1.3252\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3199 - val_loss: 1.3244\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3198 - val_loss: 1.3243\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3196 - val_loss: 1.3237\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3197 - val_loss: 1.3237\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3196 - val_loss: 1.3239\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3196 - val_loss: 1.3248\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3194 - val_loss: 1.3238\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3195 - val_loss: 1.3246\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3193 - val_loss: 1.3237\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3192 - val_loss: 1.3237\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3191 - val_loss: 1.3237\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3191 - val_loss: 1.3230\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3191 - val_loss: 1.3242\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3190 - val_loss: 1.3234\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3189 - val_loss: 1.3230\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3189 - val_loss: 1.3237\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3188 - val_loss: 1.3230\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3189 - val_loss: 1.3229\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3186 - val_loss: 1.3235\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3186 - val_loss: 1.3233\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3186 - val_loss: 1.3225\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3185 - val_loss: 1.3230\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3185 - val_loss: 1.3219\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3183 - val_loss: 1.3238\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3183 - val_loss: 1.3218\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3184 - val_loss: 1.3215\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3183 - val_loss: 1.3227\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3182 - val_loss: 1.3217\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3182 - val_loss: 1.3218\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3181 - val_loss: 1.3215\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3181 - val_loss: 1.3221\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3181 - val_loss: 1.3216\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3180 - val_loss: 1.3215\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3180 - val_loss: 1.3211\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3179 - val_loss: 1.3215\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3180 - val_loss: 1.3211\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3178 - val_loss: 1.3215\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3178 - val_loss: 1.3210\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3177 - val_loss: 1.3208\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3177 - val_loss: 1.3208\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3175 - val_loss: 1.3206\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3176 - val_loss: 1.3206\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3176 - val_loss: 1.3213\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3175 - val_loss: 1.3209\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3175 - val_loss: 1.3214\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3174 - val_loss: 1.3203\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3174 - val_loss: 1.3217\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3175 - val_loss: 1.3209\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3174 - val_loss: 1.3205\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3173 - val_loss: 1.3201\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3173 - val_loss: 1.3202\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3174 - val_loss: 1.3205\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3174 - val_loss: 1.3206\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3173 - val_loss: 1.3206\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3172 - val_loss: 1.3200\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3172 - val_loss: 1.3225\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3174 - val_loss: 1.3208\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3172 - val_loss: 1.3205\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3203\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3170 - val_loss: 1.3203\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3171 - val_loss: 1.3198\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3171 - val_loss: 1.3201\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3170 - val_loss: 1.3198\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3170 - val_loss: 1.3197\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3169 - val_loss: 1.3201\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3169 - val_loss: 1.3198\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3170 - val_loss: 1.3197\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3168 - val_loss: 1.3203\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3169 - val_loss: 1.3195\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3169 - val_loss: 1.3197\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3169 - val_loss: 1.3203\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3168 - val_loss: 1.3196\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3168 - val_loss: 1.3195\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3168 - val_loss: 1.3196\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3167 - val_loss: 1.3194\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3168 - val_loss: 1.3194\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3168 - val_loss: 1.3196\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3167 - val_loss: 1.3206\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3166 - val_loss: 1.3198\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3166 - val_loss: 1.3192\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3166 - val_loss: 1.3194\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3165 - val_loss: 1.3196\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3166 - val_loss: 1.3193\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3165 - val_loss: 1.3196\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3164 - val_loss: 1.3191\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3165 - val_loss: 1.3197\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3164 - val_loss: 1.3192\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3164 - val_loss: 1.3191\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3164 - val_loss: 1.3191\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3165 - val_loss: 1.3194\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3164 - val_loss: 1.3192\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3163 - val_loss: 1.3190\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3163 - val_loss: 1.3189\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3163 - val_loss: 1.3189\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3164 - val_loss: 1.3194\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3165 - val_loss: 1.3194\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3162 - val_loss: 1.3192\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3163 - val_loss: 1.3190\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3163 - val_loss: 1.3190\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3163 - val_loss: 1.3187\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3163 - val_loss: 1.3190\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3162 - val_loss: 1.3190\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3162 - val_loss: 1.3187\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967995 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8883542\n","The max value of N 0.8619082\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7209376666666668\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.3584 - val_loss: 1.4001\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3321 - val_loss: 1.3568\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3286 - val_loss: 1.3400\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3269 - val_loss: 1.3338\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3256 - val_loss: 1.3337\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3243 - val_loss: 1.3338\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3233 - val_loss: 1.3332\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3224 - val_loss: 1.3315\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3219 - val_loss: 1.3294\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3215 - val_loss: 1.3285\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3211 - val_loss: 1.3278\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3206 - val_loss: 1.3282\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3203 - val_loss: 1.3268\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3202 - val_loss: 1.3259\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3198 - val_loss: 1.3268\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3195 - val_loss: 1.3258\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3195 - val_loss: 1.3247\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3192 - val_loss: 1.3251\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3191 - val_loss: 1.3241\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3189 - val_loss: 1.3243\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3187 - val_loss: 1.3250\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3185 - val_loss: 1.3236\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3185 - val_loss: 1.3238\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3182 - val_loss: 1.3234\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3180 - val_loss: 1.3232\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3179 - val_loss: 1.3231\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3178 - val_loss: 1.3237\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3176 - val_loss: 1.3237\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3175 - val_loss: 1.3237\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3174 - val_loss: 1.3234\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3173 - val_loss: 1.3221\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3171 - val_loss: 1.3222\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3170 - val_loss: 1.3222\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3170 - val_loss: 1.3228\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3169 - val_loss: 1.3230\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3168 - val_loss: 1.3216\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3166 - val_loss: 1.3213\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3167 - val_loss: 1.3222\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3165 - val_loss: 1.3220\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3164 - val_loss: 1.3222\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3164 - val_loss: 1.3211\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3163 - val_loss: 1.3206\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3162 - val_loss: 1.3206\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3161 - val_loss: 1.3208\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3160 - val_loss: 1.3204\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3160 - val_loss: 1.3208\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3160 - val_loss: 1.3211\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3160 - val_loss: 1.3206\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3159 - val_loss: 1.3214\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3157 - val_loss: 1.3214\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3155 - val_loss: 1.3208\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3155 - val_loss: 1.3201\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3155 - val_loss: 1.3195\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3153 - val_loss: 1.3205\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3151 - val_loss: 1.3200\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3152 - val_loss: 1.3193\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3151 - val_loss: 1.3194\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3150 - val_loss: 1.3188\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3149 - val_loss: 1.3191\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3149 - val_loss: 1.3196\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3149 - val_loss: 1.3195\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3147 - val_loss: 1.3196\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3147 - val_loss: 1.3184\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3146 - val_loss: 1.3180\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3146 - val_loss: 1.3183\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3144 - val_loss: 1.3183\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3143 - val_loss: 1.3186\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3143 - val_loss: 1.3180\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3143 - val_loss: 1.3179\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3141 - val_loss: 1.3176\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3142 - val_loss: 1.3178\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3140 - val_loss: 1.3172\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3140 - val_loss: 1.3176\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3140 - val_loss: 1.3178\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3140 - val_loss: 1.3174\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3140 - val_loss: 1.3172\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3138 - val_loss: 1.3170\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3137 - val_loss: 1.3174\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3137 - val_loss: 1.3173\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3137 - val_loss: 1.3173\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3137 - val_loss: 1.3171\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3135 - val_loss: 1.3175\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3136 - val_loss: 1.3184\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3134 - val_loss: 1.3166\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3135 - val_loss: 1.3169\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3134 - val_loss: 1.3164\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3134 - val_loss: 1.3177\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3132 - val_loss: 1.3164\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3132 - val_loss: 1.3168\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3162\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3131 - val_loss: 1.3169\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3131 - val_loss: 1.3159\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3130 - val_loss: 1.3159\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3130 - val_loss: 1.3162\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3130 - val_loss: 1.3159\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3129 - val_loss: 1.3158\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3129 - val_loss: 1.3159\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3129 - val_loss: 1.3157\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3130 - val_loss: 1.3160\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3129 - val_loss: 1.3164\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3129 - val_loss: 1.3169\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3127 - val_loss: 1.3159\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3127 - val_loss: 1.3153\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3128 - val_loss: 1.3157\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3127 - val_loss: 1.3157\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3128 - val_loss: 1.3157\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3126 - val_loss: 1.3161\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3125 - val_loss: 1.3158\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3126 - val_loss: 1.3151\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3126 - val_loss: 1.3153\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3125 - val_loss: 1.3153\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3124 - val_loss: 1.3149\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3125 - val_loss: 1.3153\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3124 - val_loss: 1.3150\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3124 - val_loss: 1.3149\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3124 - val_loss: 1.3154\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3123 - val_loss: 1.3150\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3123 - val_loss: 1.3151\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3153\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3123 - val_loss: 1.3151\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3123 - val_loss: 1.3153\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3123 - val_loss: 1.3150\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3123 - val_loss: 1.3148\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3122 - val_loss: 1.3150\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3122 - val_loss: 1.3145\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3122 - val_loss: 1.3154\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3148\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3121 - val_loss: 1.3147\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3121 - val_loss: 1.3147\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3147\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3121 - val_loss: 1.3146\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3120 - val_loss: 1.3145\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3121 - val_loss: 1.3151\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3119 - val_loss: 1.3147\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3120 - val_loss: 1.3146\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3119 - val_loss: 1.3150\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3118 - val_loss: 1.3145\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3119 - val_loss: 1.3147\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3118 - val_loss: 1.3142\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3118 - val_loss: 1.3142\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967995 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8305616\n","The max value of N 0.8646936\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7121323333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.3552 - val_loss: 1.4043\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 439us/step - loss: 1.3313 - val_loss: 1.3607\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 404us/step - loss: 1.3279 - val_loss: 1.3454\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3261 - val_loss: 1.3385\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3246 - val_loss: 1.3388\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3236 - val_loss: 1.3370\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3228 - val_loss: 1.3347\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3222 - val_loss: 1.3316\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3220 - val_loss: 1.3306\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3211 - val_loss: 1.3301\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3207 - val_loss: 1.3306\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3204 - val_loss: 1.3301\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3201 - val_loss: 1.3289\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3198 - val_loss: 1.3281\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3196 - val_loss: 1.3267\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3193 - val_loss: 1.3270\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3191 - val_loss: 1.3259\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3191 - val_loss: 1.3254\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3188 - val_loss: 1.3251\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3186 - val_loss: 1.3265\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3185 - val_loss: 1.3247\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3183 - val_loss: 1.3249\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3182 - val_loss: 1.3246\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3181 - val_loss: 1.3250\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3179 - val_loss: 1.3240\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3177 - val_loss: 1.3245\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3176 - val_loss: 1.3232\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3176 - val_loss: 1.3241\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3175 - val_loss: 1.3237\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3174 - val_loss: 1.3230\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3172 - val_loss: 1.3229\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3242\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3226\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3169 - val_loss: 1.3220\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3168 - val_loss: 1.3218\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3168 - val_loss: 1.3233\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3167 - val_loss: 1.3211\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3166 - val_loss: 1.3220\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3165 - val_loss: 1.3246\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3164 - val_loss: 1.3220\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3163 - val_loss: 1.3214\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3162 - val_loss: 1.3216\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3161 - val_loss: 1.3208\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3161 - val_loss: 1.3222\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3160 - val_loss: 1.3220\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3160 - val_loss: 1.3210\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3158 - val_loss: 1.3217\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3158 - val_loss: 1.3216\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3157 - val_loss: 1.3203\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3155 - val_loss: 1.3205\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3157 - val_loss: 1.3202\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3154 - val_loss: 1.3209\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3153 - val_loss: 1.3203\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3154 - val_loss: 1.3209\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3152 - val_loss: 1.3195\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3151 - val_loss: 1.3198\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3151 - val_loss: 1.3194\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3152 - val_loss: 1.3192\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3153 - val_loss: 1.3194\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3149 - val_loss: 1.3195\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3149 - val_loss: 1.3193\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3149 - val_loss: 1.3200\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3146 - val_loss: 1.3200\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3145 - val_loss: 1.3190\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3198\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3144 - val_loss: 1.3184\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3143 - val_loss: 1.3186\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3142 - val_loss: 1.3183\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3142 - val_loss: 1.3189\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3141 - val_loss: 1.3183\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3142 - val_loss: 1.3189\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3139 - val_loss: 1.3182\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3139 - val_loss: 1.3177\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3139 - val_loss: 1.3177\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3140 - val_loss: 1.3183\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3140 - val_loss: 1.3186\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3138 - val_loss: 1.3185\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3137 - val_loss: 1.3180\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3138 - val_loss: 1.3180\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3137 - val_loss: 1.3180\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3135 - val_loss: 1.3186\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3135 - val_loss: 1.3168\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3135 - val_loss: 1.3176\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3134 - val_loss: 1.3172\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3133 - val_loss: 1.3173\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3169\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3171\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3163\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3131 - val_loss: 1.3165\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3131 - val_loss: 1.3167\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3172\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3130 - val_loss: 1.3166\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3131 - val_loss: 1.3168\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3130 - val_loss: 1.3171\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3130 - val_loss: 1.3161\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3129 - val_loss: 1.3161\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3129 - val_loss: 1.3162\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3130 - val_loss: 1.3159\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3129 - val_loss: 1.3158\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3127 - val_loss: 1.3156\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3127 - val_loss: 1.3158\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3128 - val_loss: 1.3158\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3127 - val_loss: 1.3156\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3128 - val_loss: 1.3161\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3127 - val_loss: 1.3161\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3126 - val_loss: 1.3157\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3125 - val_loss: 1.3156\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3126 - val_loss: 1.3156\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3164\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3125 - val_loss: 1.3153\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3124 - val_loss: 1.3156\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3157\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3125 - val_loss: 1.3157\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3158\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3125 - val_loss: 1.3153\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3124 - val_loss: 1.3156\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3123 - val_loss: 1.3150\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3124 - val_loss: 1.3152\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3122 - val_loss: 1.3157\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3123 - val_loss: 1.3152\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3153\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3123 - val_loss: 1.3153\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3122 - val_loss: 1.3156\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3121 - val_loss: 1.3150\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3121 - val_loss: 1.3151\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3120 - val_loss: 1.3150\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3121 - val_loss: 1.3148\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3120 - val_loss: 1.3150\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3121 - val_loss: 1.3150\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3121 - val_loss: 1.3148\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3120 - val_loss: 1.3148\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3121 - val_loss: 1.3149\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3121 - val_loss: 1.3149\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3120 - val_loss: 1.3146\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3119 - val_loss: 1.3149\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3121 - val_loss: 1.3149\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3120 - val_loss: 1.3147\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3119 - val_loss: 1.3147\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3120 - val_loss: 1.3145\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3120 - val_loss: 1.3181\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3128 - val_loss: 1.3308\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3120 - val_loss: 1.3212\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3175\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3120 - val_loss: 1.3157\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967995 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.85854524\n","The max value of N 0.8481664\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7174868333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/DCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 2\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.3579 - val_loss: 1.3895\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3316 - val_loss: 1.3495\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3276 - val_loss: 1.3410\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 398us/step - loss: 1.3257 - val_loss: 1.3379\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3245 - val_loss: 1.3345\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3234 - val_loss: 1.3335\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3226 - val_loss: 1.3324\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3220 - val_loss: 1.3312\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3216 - val_loss: 1.3290\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3211 - val_loss: 1.3312\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3206 - val_loss: 1.3292\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3203 - val_loss: 1.3289\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3200 - val_loss: 1.3292\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3197 - val_loss: 1.3273\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3195 - val_loss: 1.3271\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3192 - val_loss: 1.3273\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3190 - val_loss: 1.3260\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3187 - val_loss: 1.3249\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3186 - val_loss: 1.3245\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3185 - val_loss: 1.3255\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3183 - val_loss: 1.3242\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3182 - val_loss: 1.3247\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3179 - val_loss: 1.3235\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3179 - val_loss: 1.3251\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3179 - val_loss: 1.3227\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3176 - val_loss: 1.3236\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3176 - val_loss: 1.3240\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3174 - val_loss: 1.3235\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3174 - val_loss: 1.3234\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3172 - val_loss: 1.3225\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3171 - val_loss: 1.3229\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3171 - val_loss: 1.3221\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3170 - val_loss: 1.3217\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3169 - val_loss: 1.3227\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3168 - val_loss: 1.3221\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3168 - val_loss: 1.3237\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3167 - val_loss: 1.3227\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3165 - val_loss: 1.3229\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3165 - val_loss: 1.3211\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3163 - val_loss: 1.3213\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3162 - val_loss: 1.3211\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3163 - val_loss: 1.3211\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3163 - val_loss: 1.3200\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3160 - val_loss: 1.3232\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3159 - val_loss: 1.3212\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3160 - val_loss: 1.3204\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3158 - val_loss: 1.3204\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3158 - val_loss: 1.3198\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3156 - val_loss: 1.3200\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3155 - val_loss: 1.3198\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3156 - val_loss: 1.3199\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3155 - val_loss: 1.3192\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3154 - val_loss: 1.3202\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3154 - val_loss: 1.3205\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3153 - val_loss: 1.3201\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3152 - val_loss: 1.3197\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3151 - val_loss: 1.3191\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3150 - val_loss: 1.3197\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3150 - val_loss: 1.3194\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3149 - val_loss: 1.3192\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3148 - val_loss: 1.3198\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3147 - val_loss: 1.3205\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3146 - val_loss: 1.3200\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3146 - val_loss: 1.3188\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3145 - val_loss: 1.3194\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3144 - val_loss: 1.3183\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3144 - val_loss: 1.3182\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3142 - val_loss: 1.3192\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3142 - val_loss: 1.3192\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3141 - val_loss: 1.3199\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3141 - val_loss: 1.3183\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3140 - val_loss: 1.3188\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3140 - val_loss: 1.3174\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3140 - val_loss: 1.3177\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3139 - val_loss: 1.3185\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3138 - val_loss: 1.3177\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3136 - val_loss: 1.3171\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3138 - val_loss: 1.3170\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3137 - val_loss: 1.3174\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3136 - val_loss: 1.3183\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3135 - val_loss: 1.3175\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3135 - val_loss: 1.3170\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3135 - val_loss: 1.3166\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3135 - val_loss: 1.3169\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3135 - val_loss: 1.3167\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3133 - val_loss: 1.3168\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3133 - val_loss: 1.3170\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3132 - val_loss: 1.3165\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3131 - val_loss: 1.3170\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3133 - val_loss: 1.3162\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3131 - val_loss: 1.3164\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3132 - val_loss: 1.3166\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3130 - val_loss: 1.3169\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3130 - val_loss: 1.3167\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3130 - val_loss: 1.3174\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3129 - val_loss: 1.3156\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3128 - val_loss: 1.3159\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3128 - val_loss: 1.3157\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3128 - val_loss: 1.3163\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3128 - val_loss: 1.3153\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3127 - val_loss: 1.3153\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3127 - val_loss: 1.3155\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3126 - val_loss: 1.3158\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3128 - val_loss: 1.3154\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3127 - val_loss: 1.3157\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3126 - val_loss: 1.3155\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3126 - val_loss: 1.3166\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3125 - val_loss: 1.3154\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3124 - val_loss: 1.3154\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3124 - val_loss: 1.3153\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3125 - val_loss: 1.3154\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3124 - val_loss: 1.3152\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3124 - val_loss: 1.3153\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3124 - val_loss: 1.3150\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3123 - val_loss: 1.3153\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3123 - val_loss: 1.3153\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3123 - val_loss: 1.3154\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3123 - val_loss: 1.3153\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3124 - val_loss: 1.3158\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3122 - val_loss: 1.3150\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3122 - val_loss: 1.3149\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3122 - val_loss: 1.3155\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3123 - val_loss: 1.3159\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3122 - val_loss: 1.3151\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3122 - val_loss: 1.3151\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3121 - val_loss: 1.3146\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3121 - val_loss: 1.3147\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3120 - val_loss: 1.3149\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3121 - val_loss: 1.3146\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3120 - val_loss: 1.3149\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3120 - val_loss: 1.3147\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3120 - val_loss: 1.3146\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3120 - val_loss: 1.3146\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3119 - val_loss: 1.3147\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3120 - val_loss: 1.3146\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3120 - val_loss: 1.3150\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3119 - val_loss: 1.3154\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3118 - val_loss: 1.3151\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3119 - val_loss: 1.3146\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3119 - val_loss: 1.3145\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3119 - val_loss: 1.3144\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3118 - val_loss: 1.3147\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3118 - val_loss: 1.3144\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3117 - val_loss: 1.3147\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3117 - val_loss: 1.3145\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3117 - val_loss: 1.3143\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3117 - val_loss: 1.3142\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3117 - val_loss: 1.3140\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967992 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8672296\n","The max value of N 0.8464128\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7153136666666665\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.7385896666666667, 0.720618, 0.7230293333333334, 0.693906, 0.6951360000000001, 0.713806, 0.7209376666666668, 0.7121323333333333, 0.7174868333333333, 0.7153136666666665]\n","AUROC ===== 0.71509555 +/- 0.012422303673467064\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCjU1NTU1O/z/jfOuJXY3HkUAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f862833b3c8>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"h_g1Rm0rILfR","colab_type":"text"},"cell_type":"markdown","source":["##** Cat vs All **##"]},{"metadata":{"id":"AQ8UYlyWIJPf","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":24055},"outputId":"77cbeb9f-79e3-4200-9fa5-6315aab4af6d"},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 913us/step - loss: 1.4922 - val_loss: 1.6706\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3507 - val_loss: 1.4960\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3266 - val_loss: 1.3127\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3164 - val_loss: 1.3079\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3129 - val_loss: 1.3075\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3111 - val_loss: 1.3093\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3097 - val_loss: 1.3071\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3084 - val_loss: 1.3067\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3078 - val_loss: 1.3094\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3090 - val_loss: 1.3109\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3072 - val_loss: 1.3073\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3066 - val_loss: 1.3064\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3064 - val_loss: 1.3062\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3061\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3082\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3073 - val_loss: 1.3060\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3060\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3060\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0366188\n","The max value of N 0.118267804\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6293171666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 848us/step - loss: 1.4743 - val_loss: 1.4241\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3520 - val_loss: 1.4476\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3280 - val_loss: 1.3489\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3210 - val_loss: 1.3193\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3151 - val_loss: 1.3100\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3122 - val_loss: 1.3064\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3103 - val_loss: 1.3066\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3088 - val_loss: 1.3061\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3078 - val_loss: 1.3061\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3074 - val_loss: 1.3058\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3068 - val_loss: 1.3058\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3065 - val_loss: 1.3061\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3062 - val_loss: 1.3060\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3061 - val_loss: 1.3059\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3059 - val_loss: 1.3058\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3076 - val_loss: 1.3065\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3120 - val_loss: 1.3096\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3077 - val_loss: 1.3060\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3068 - val_loss: 1.3064\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3070 - val_loss: 1.3067\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3064 - val_loss: 1.3058\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3062 - val_loss: 1.3057\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3065 - val_loss: 1.3058\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 360us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 360us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 360us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 360us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3055 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0382539\n","The max value of N 0.119337715\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5960178333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 947us/step - loss: 1.5168 - val_loss: 1.7010\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3637 - val_loss: 1.5774\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3291 - val_loss: 1.5311\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3188 - val_loss: 1.3059\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3123 - val_loss: 1.3040\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3092 - val_loss: 1.3045\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3089 - val_loss: 1.3057\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3068 - val_loss: 1.3053\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3063 - val_loss: 1.3047\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3043\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3057 - val_loss: 1.3077\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3051 - val_loss: 1.3053\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3044 - val_loss: 1.3045\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3042 - val_loss: 1.3045\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3042 - val_loss: 1.3041\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3041 - val_loss: 1.3043\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3041 - val_loss: 1.3042\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3038 - val_loss: 1.3037\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3039 - val_loss: 1.3038\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3038 - val_loss: 1.3037\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3038 - val_loss: 1.3037\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3037 - val_loss: 1.3039\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3037 - val_loss: 1.3037\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3038\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3042 - val_loss: 1.3295\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3040 - val_loss: 1.3074\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3038 - val_loss: 1.3042\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3036 - val_loss: 1.3039\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3037 - val_loss: 1.3057\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0148488\n","The max value of N 0.12515369\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.630421\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.5474 - val_loss: 1.9446\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.4044 - val_loss: 1.3268\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3192 - val_loss: 1.3105\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3092 - val_loss: 1.3085\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3077 - val_loss: 1.3076\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3071 - val_loss: 1.3071\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3068 - val_loss: 1.3069\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3066 - val_loss: 1.3067\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3065 - val_loss: 1.3065\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 360us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967998 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.12264055\n","The max value of N 0.11948891\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6022566666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4945 - val_loss: 1.4346\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3528 - val_loss: 1.4183\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3231 - val_loss: 1.3254\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3117 - val_loss: 1.3043\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3107 - val_loss: 1.3023\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3014\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3036 - val_loss: 1.3010\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3029 - val_loss: 1.3009\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3036 - val_loss: 1.3014\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3019 - val_loss: 1.3008\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3013 - val_loss: 1.3007\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3011 - val_loss: 1.3006\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3009 - val_loss: 1.3006\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3008 - val_loss: 1.3006\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3007 - val_loss: 1.3006\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3007 - val_loss: 1.3006\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3007 - val_loss: 1.3005\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3018 - val_loss: 1.3057\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3005 - val_loss: 1.3024\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.12264813\n","The max value of N 0.12142479\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5779743333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4867 - val_loss: 1.4163\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3545 - val_loss: 1.3787\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3297 - val_loss: 1.4084\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3193 - val_loss: 1.3082\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3148 - val_loss: 1.3081\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3131 - val_loss: 1.3102\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3100 - val_loss: 1.3057\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3084 - val_loss: 1.3058\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3081 - val_loss: 1.3058\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3071 - val_loss: 1.3057\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3065 - val_loss: 1.3057\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3063 - val_loss: 1.3059\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3057 - val_loss: 1.3072\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.1226784\n","The max value of N 0.1380989\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6105278333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.5185 - val_loss: 1.7650\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3635 - val_loss: 1.7763\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3313 - val_loss: 1.4568\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3248 - val_loss: 1.3509\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3179 - val_loss: 1.3111\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3151 - val_loss: 1.3114\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3138 - val_loss: 1.3117\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3150 - val_loss: 1.3184\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3122 - val_loss: 1.3125\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3109 - val_loss: 1.3108\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3105 - val_loss: 1.3105\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3103 - val_loss: 1.3104\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3101 - val_loss: 1.3098\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3099 - val_loss: 1.3097\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3098 - val_loss: 1.3097\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3098 - val_loss: 1.3097\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3098 - val_loss: 1.3097\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 421us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 130/150\n","5850/5850 [==============================] - 4s 619us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 131/150\n","5850/5850 [==============================] - 3s 584us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 132/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.99009746\n","The max value of N 0.119620085\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6277563333333332\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.4961 - val_loss: 2.0932\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3592 - val_loss: 1.8596\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3343 - val_loss: 1.5578\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3210 - val_loss: 1.3305\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3145 - val_loss: 1.3063\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3127 - val_loss: 1.3071\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3099 - val_loss: 1.3059\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3081 - val_loss: 1.3056\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3071 - val_loss: 1.3056\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3065 - val_loss: 1.3056\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3062 - val_loss: 1.3056\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3062 - val_loss: 1.3056\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3056\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3055\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3059\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.99304223\n","The max value of N 0.11037758\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6057925\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.4814 - val_loss: 1.4548\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 462us/step - loss: 1.3530 - val_loss: 1.3395\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3267 - val_loss: 1.3206\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 436us/step - loss: 1.3174 - val_loss: 1.3063\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 422us/step - loss: 1.3126 - val_loss: 1.3078\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3102 - val_loss: 1.3064\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3104 - val_loss: 1.3089\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3084 - val_loss: 1.3061\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3070 - val_loss: 1.3059\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3071 - val_loss: 1.3077\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3069 - val_loss: 1.3060\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3057\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3058\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3057\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3056\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3056\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.016154\n","The max value of N 0.12348438\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5888448333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 3\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 12s 2ms/step - loss: 1.5289 - val_loss: 1.7399\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 399us/step - loss: 1.3608 - val_loss: 1.4589\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3339 - val_loss: 1.3707\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3224 - val_loss: 1.3491\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3172 - val_loss: 1.3193\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3164 - val_loss: 1.3351\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3146 - val_loss: 1.3277\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3119 - val_loss: 1.3125\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3092 - val_loss: 1.3092\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3097 - val_loss: 1.3082\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3083 - val_loss: 1.3068\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3073 - val_loss: 1.3062\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3074 - val_loss: 1.3066\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3068 - val_loss: 1.3065\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3066\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3062 - val_loss: 1.3072\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3062 - val_loss: 1.3094\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3073 - val_loss: 1.3916\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3064 - val_loss: 1.3295\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3134\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3091\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3070\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3064\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3063\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3063\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3066\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3065\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3064\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 5s 775us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 4s 729us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 3s 568us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 65/150\n","5850/5850 [==============================] - 3s 521us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3127\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3091\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 115/150\n","5800/5850 [============================>.] - ETA: 0s - loss: 1.3053"],"name":"stdout"}]},{"metadata":{"id":"SX3jnpYleJJs","colab_type":"text"},"cell_type":"markdown","source":["##** Deer vs All **##"]},{"metadata":{"id":"ScxsTvvieItV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":61922},"outputId":"b124646c-aa42-4b43-c13c-cd18fbc28a9f","executionInfo":{"status":"ok","timestamp":1541724439673,"user_tz":-660,"elapsed":3758787,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":10,"outputs":[{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4910 - val_loss: 1.5525\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 433us/step - loss: 1.3485 - val_loss: 1.4589\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3281 - val_loss: 1.3395\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3168 - val_loss: 1.3069\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3150 - val_loss: 1.3092\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3114 - val_loss: 1.3150\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3102 - val_loss: 1.3075\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3086 - val_loss: 1.3085\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3083 - val_loss: 1.3093\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3095 - val_loss: 1.3123\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3073 - val_loss: 1.3069\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3068 - val_loss: 1.3064\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3065 - val_loss: 1.3063\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3061\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3070\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3072\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3072\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3073\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3075\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3075\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3077\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3080\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3083\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3080\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3082\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3080\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3075\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3075\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3076\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3076\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3077\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3078\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3078\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3078\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3078\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3078\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3081\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3079\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3079\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3080\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3066\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3066\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3066\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3068\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3068\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3069\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3069\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3068\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3068\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3069\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3069\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3069\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3069\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3073\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3103\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3059\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.87900114\n","The max value of N 0.120881364\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6307263333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4667 - val_loss: 1.4252\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 450us/step - loss: 1.3505 - val_loss: 1.6586\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3300 - val_loss: 1.3547\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3211 - val_loss: 1.3205\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3175 - val_loss: 1.3229\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3159 - val_loss: 1.5704\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3140 - val_loss: 1.3494\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3125 - val_loss: 1.3137\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3121 - val_loss: 1.3570\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3103 - val_loss: 1.3075\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3089 - val_loss: 1.3088\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3089 - val_loss: 1.3081\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3088 - val_loss: 1.3141\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3079 - val_loss: 1.3088\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3073 - val_loss: 1.3079\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3070 - val_loss: 1.3075\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3072 - val_loss: 1.3069\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3070 - val_loss: 1.3064\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3069\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3066 - val_loss: 1.3092\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3066 - val_loss: 1.3081\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3075\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3058\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3081 - val_loss: 1.3065\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3069 - val_loss: 1.3057\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3066 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3065 - val_loss: 1.3056\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3064 - val_loss: 1.3057\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3058\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3057\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3062 - val_loss: 1.3057\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3069 - val_loss: 1.3092\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3074\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3062\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3058\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3058\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3079\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0402508\n","The max value of N 0.12270776\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5972321666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.5092 - val_loss: 1.5234\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 430us/step - loss: 1.3539 - val_loss: 1.5244\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3246 - val_loss: 1.3333\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3224 - val_loss: 1.9257\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3129 - val_loss: 1.6599\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3107 - val_loss: 1.3601\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3075 - val_loss: 1.3096\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3062 - val_loss: 1.3054\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3050\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3062 - val_loss: 1.3052\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3052 - val_loss: 1.3048\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3045 - val_loss: 1.3043\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3042 - val_loss: 1.3041\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3042 - val_loss: 1.3041\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3040 - val_loss: 1.3040\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3039 - val_loss: 1.3038\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3038 - val_loss: 1.3039\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3037 - val_loss: 1.3037\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3043 - val_loss: 1.3037\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3050 - val_loss: 1.4809\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3041 - val_loss: 1.3455\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3039 - val_loss: 1.3085\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3044 - val_loss: 1.3086\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3040 - val_loss: 1.3037\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3038 - val_loss: 1.3035\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3037 - val_loss: 1.3208\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3037 - val_loss: 1.3097\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3061\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3036 - val_loss: 1.3041\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3036 - val_loss: 1.3039\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3035 - val_loss: 1.3038\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3034\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3041\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.99396783\n","The max value of N 0.12385205\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.656923\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.5473 - val_loss: 1.6198\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 461us/step - loss: 1.4022 - val_loss: 1.3265\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3199 - val_loss: 1.3113\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 437us/step - loss: 1.3094 - val_loss: 1.3087\n","Epoch 5/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3078 - val_loss: 1.3078\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3072 - val_loss: 1.3073\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 407us/step - loss: 1.3069 - val_loss: 1.3070\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3067 - val_loss: 1.3068\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3060\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.19781104\n","The max value of N 0.11466182\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5712213333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.4948 - val_loss: 1.5897\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3506 - val_loss: 1.8089\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3211 - val_loss: 1.3075\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3114 - val_loss: 1.3176\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3074 - val_loss: 1.3021\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3051 - val_loss: 1.3057\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3037 - val_loss: 1.3024\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3029 - val_loss: 1.3015\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3029 - val_loss: 1.3022\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3022 - val_loss: 1.3017\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3015 - val_loss: 1.3011\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3012 - val_loss: 1.3008\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3013 - val_loss: 1.3008\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3026 - val_loss: 1.3010\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3022 - val_loss: 1.3026\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3014 - val_loss: 1.3014\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3013 - val_loss: 1.3016\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3011 - val_loss: 1.3012\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3009 - val_loss: 1.3009\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3008 - val_loss: 1.3008\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3008 - val_loss: 1.3007\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3008 - val_loss: 1.3007\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3007 - val_loss: 1.3006\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3007 - val_loss: 1.3006\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3007 - val_loss: 1.3008\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3007 - val_loss: 1.3007\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3007 - val_loss: 1.3009\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3010 - val_loss: 1.3045\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3009 - val_loss: 1.3017\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3007 - val_loss: 1.3012\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3007 - val_loss: 1.3009\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3006 - val_loss: 1.3008\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9823219\n","The max value of N 0.121800326\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5723113333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 12s 2ms/step - loss: 1.4784 - val_loss: 1.4743\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 469us/step - loss: 1.3489 - val_loss: 1.3736\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 436us/step - loss: 1.3297 - val_loss: 1.3469\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 412us/step - loss: 1.3234 - val_loss: 1.4015\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3175 - val_loss: 1.5598\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3129 - val_loss: 1.3156\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3121 - val_loss: 1.3060\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3093 - val_loss: 1.3058\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3088 - val_loss: 1.3059\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3074 - val_loss: 1.3057\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3066 - val_loss: 1.3057\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3070 - val_loss: 1.3057\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3113 - val_loss: 1.3204\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3084 - val_loss: 1.3064\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3072 - val_loss: 1.3059\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3068 - val_loss: 1.3058\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3065 - val_loss: 1.3057\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3057\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3057\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3072\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3057\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3072\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3066\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3123\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3084 - val_loss: 1.4365\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3183\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3066\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.12155749\n","The max value of N 0.12721875\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6836391666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 13s 2ms/step - loss: 1.5197 - val_loss: 1.7296\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 447us/step - loss: 1.3606 - val_loss: 1.6700\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 403us/step - loss: 1.3317 - val_loss: 1.4153\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3228 - val_loss: 1.3646\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3179 - val_loss: 1.3132\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3171 - val_loss: 1.3197\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3139 - val_loss: 1.3118\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3126 - val_loss: 1.3101\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3117 - val_loss: 1.3102\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3112 - val_loss: 1.3100\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3107 - val_loss: 1.3106\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3122 - val_loss: 1.3200\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3113 - val_loss: 1.3118\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3115 - val_loss: 1.3135\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3105 - val_loss: 1.3109\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3102 - val_loss: 1.3104\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3101 - val_loss: 1.3101\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3100 - val_loss: 1.3099\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3099 - val_loss: 1.3099\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3098 - val_loss: 1.3100\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3098 - val_loss: 1.3102\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3098 - val_loss: 1.3103\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3098 - val_loss: 1.3102\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3101\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3097 - val_loss: 1.3103\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.2519845\n","The max value of N 0.12472203\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5831231666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 14s 2ms/step - loss: 1.5131 - val_loss: 1.4474\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 444us/step - loss: 1.3705 - val_loss: 1.8272\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 425us/step - loss: 1.3318 - val_loss: 1.3110\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3196 - val_loss: 1.3121\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3142 - val_loss: 1.3078\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3104 - val_loss: 1.3059\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3101 - val_loss: 1.3066\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3081 - val_loss: 1.3059\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3075 - val_loss: 1.3061\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3065 - val_loss: 1.3057\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3067\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3099 - val_loss: 1.5408\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.4343\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3232\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3075\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.121555164\n","The max value of N 0.121329114\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6158778333333332\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 15s 3ms/step - loss: 1.4879 - val_loss: 1.4859\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 451us/step - loss: 1.3501 - val_loss: 1.3956\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3254 - val_loss: 1.3358\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3158 - val_loss: 1.3064\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 403us/step - loss: 1.3126 - val_loss: 1.3101\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3105 - val_loss: 1.3061\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3085 - val_loss: 1.3057\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3077 - val_loss: 1.3057\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3093 - val_loss: 1.3071\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3069 - val_loss: 1.3057\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3056\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3056\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.90250224\n","The max value of N 0.120567106\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5731246666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 4\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 16s 3ms/step - loss: 1.5150 - val_loss: 1.5394\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 434us/step - loss: 1.3572 - val_loss: 1.5105\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 423us/step - loss: 1.3317 - val_loss: 1.9234\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3305 - val_loss: 1.3117\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3160 - val_loss: 1.3064\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3119 - val_loss: 1.3060\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3105 - val_loss: 1.3063\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3086 - val_loss: 1.3057\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3074 - val_loss: 1.3056\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3071 - val_loss: 1.3056\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3076 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3099 - val_loss: 1.3174\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3077 - val_loss: 1.3083\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3068 - val_loss: 1.3076\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3066 - val_loss: 1.3063\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3067 - val_loss: 1.3072\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3064 - val_loss: 1.3069\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3101\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3064 - val_loss: 1.3078\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3071\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3058 - val_loss: 1.3066\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3054\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3053\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0282183\n","The max value of N 0.12136191\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6009183333333333\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6307263333333334, 0.5972321666666667, 0.656923, 0.5712213333333334, 0.5723113333333333, 0.6836391666666667, 0.5831231666666666, 0.6158778333333332, 0.5731246666666666, 0.6009183333333333]\n","AUROC ===== 0.6085097333333334 +/- 0.03649422538789149\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQkERARAEQSgo/x/TTZO8JMXX5gAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f9b5c0dba20>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Wpw3lThHnSlL","colab_type":"text"},"cell_type":"markdown","source":["##** Dog vs All **##*"]},{"metadata":{"id":"7FJG3okgnROr","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":64319},"outputId":"1eaf68bf-b079-40e4-8927-8357f5e97f09","executionInfo":{"status":"ok","timestamp":1541731053858,"user_tz":-660,"elapsed":3588938,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 883us/step - loss: 1.4953 - val_loss: 1.7956\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3526 - val_loss: 1.4100\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3241 - val_loss: 1.3206\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3160 - val_loss: 1.3100\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3119 - val_loss: 1.3088\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3132 - val_loss: 1.3158\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3099 - val_loss: 1.3069\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3084 - val_loss: 1.3083\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3083 - val_loss: 1.3069\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3072 - val_loss: 1.3064\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3067 - val_loss: 1.3063\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3067 - val_loss: 1.3064\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3070 - val_loss: 1.3067\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3075\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3059\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.081084\n","The max value of N 0.106741354\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6312441666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 837us/step - loss: 1.4793 - val_loss: 1.4752\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3476 - val_loss: 1.3523\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3268 - val_loss: 1.3525\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3213 - val_loss: 1.3237\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3198 - val_loss: 1.3072\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3150 - val_loss: 1.3072\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3115 - val_loss: 1.3059\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3102 - val_loss: 1.3060\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3160 - val_loss: 1.3071\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3083 - val_loss: 1.3061\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3070 - val_loss: 1.3058\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3070 - val_loss: 1.3058\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3063 - val_loss: 1.3057\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3057\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 359us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3098\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3080 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 360us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.12763515\n","The max value of N 0.10707929\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6191481666666667\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 952us/step - loss: 1.5083 - val_loss: 1.8045\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3576 - val_loss: 1.7796\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3247 - val_loss: 1.3389\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3173 - val_loss: 1.3060\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3118 - val_loss: 1.3055\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3093 - val_loss: 1.3043\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3074 - val_loss: 1.3228\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3073 - val_loss: 1.3088\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3057 - val_loss: 1.3050\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3083 - val_loss: 1.3095\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3079 - val_loss: 1.3364\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3051 - val_loss: 1.3053\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 360us/step - loss: 1.3046 - val_loss: 1.3043\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3043 - val_loss: 1.3042\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3043 - val_loss: 1.3039\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3041 - val_loss: 1.3038\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3041 - val_loss: 1.3039\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3040 - val_loss: 1.3039\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3047 - val_loss: 1.3086\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3045 - val_loss: 1.3050\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3041 - val_loss: 1.3040\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3040 - val_loss: 1.3038\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3039 - val_loss: 1.3037\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3038 - val_loss: 1.3035\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3042 - val_loss: 1.3038\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3039 - val_loss: 1.3036\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3038 - val_loss: 1.3036\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3037 - val_loss: 1.3035\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3035 - val_loss: 1.3034\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3034\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3034\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3034\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 358us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0290817\n","The max value of N 0.105721466\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5964308333333332\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.5480 - val_loss: 1.8329\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.4041 - val_loss: 1.3220\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3190 - val_loss: 1.3121\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3091 - val_loss: 1.3096\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3076 - val_loss: 1.3081\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3071 - val_loss: 1.3073\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3068 - val_loss: 1.3069\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3066 - val_loss: 1.3067\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.17121755\n","The max value of N 0.13892761\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.596682\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.5094 - val_loss: 1.6293\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3619 - val_loss: 1.4931\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3242 - val_loss: 1.4447\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3113 - val_loss: 1.3321\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3070 - val_loss: 1.3125\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3047\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3036 - val_loss: 1.3024\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3026 - val_loss: 1.3017\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3020 - val_loss: 1.3010\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3016 - val_loss: 1.3016\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3011 - val_loss: 1.3011\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3009 - val_loss: 1.3011\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3008 - val_loss: 1.3009\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3007 - val_loss: 1.3008\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3008 - val_loss: 1.3006\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3009 - val_loss: 1.3005\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3006 - val_loss: 1.3015\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3038\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3050\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3052\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3049\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3042\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3034\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3029\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3022\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3005 - val_loss: 1.3019\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3011\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3010\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3010\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3011\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.13432267\n","The max value of N 0.10731788\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6385888333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4812 - val_loss: 1.5261\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3524 - val_loss: 1.3347\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 412us/step - loss: 1.3258 - val_loss: 1.3128\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3194 - val_loss: 1.3177\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3186 - val_loss: 1.3090\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3148 - val_loss: 1.3063\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3130 - val_loss: 1.3110\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3094 - val_loss: 1.3065\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3130 - val_loss: 1.3123\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3093 - val_loss: 1.3074\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3078 - val_loss: 1.3059\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3072 - val_loss: 1.3059\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3071 - val_loss: 1.3133\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3065 - val_loss: 1.3083\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3090\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3111\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3131\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3057 - val_loss: 1.3121\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3160\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3165\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3057 - val_loss: 1.3161\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3120\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3158\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3078\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3095\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3120\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3104\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3103\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3106\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3092\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3090\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3084\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3085\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3080\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3070\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3075\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3068\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3058\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9896679\n","The max value of N 0.10576646\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6247698333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.5121 - val_loss: 1.5181\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3625 - val_loss: 1.5807\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3333 - val_loss: 1.3312\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3241 - val_loss: 1.3180\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3187 - val_loss: 1.3117\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3152 - val_loss: 1.3102\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3133 - val_loss: 1.3102\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3126 - val_loss: 1.3115\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3121 - val_loss: 1.3107\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3111 - val_loss: 1.3101\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3106 - val_loss: 1.3101\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3104 - val_loss: 1.3098\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3106 - val_loss: 1.3100\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3103 - val_loss: 1.3099\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3101 - val_loss: 1.3099\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3100 - val_loss: 1.3098\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3099 - val_loss: 1.3098\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3099 - val_loss: 1.3098\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3099 - val_loss: 1.3098\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0110849\n","The max value of N 0.105613105\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.631493\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.5070 - val_loss: 1.4177\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3597 - val_loss: 1.4020\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3330 - val_loss: 1.3754\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3232 - val_loss: 1.3262\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 420us/step - loss: 1.3193 - val_loss: 1.3089\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3157 - val_loss: 1.3111\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3124 - val_loss: 1.3087\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3094 - val_loss: 1.3068\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3077 - val_loss: 1.3065\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3082 - val_loss: 1.3088\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3073 - val_loss: 1.3082\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3066 - val_loss: 1.3063\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3064\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3074 - val_loss: 1.3106\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3057\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3125\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3090\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3137\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3185\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3215\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3207\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3183\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3171\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3158\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3131\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3119\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3117\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3111\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3095\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3102\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3088\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3076\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3069\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3072\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3066\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3069\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3089\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3099\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3057\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0295953\n","The max value of N 0.10724164\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6933173333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.4893 - val_loss: 1.5513\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3532 - val_loss: 1.3300\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3282 - val_loss: 1.3169\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3165 - val_loss: 1.3068\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3151 - val_loss: 1.3501\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3120 - val_loss: 1.3087\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3083 - val_loss: 1.3063\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3071 - val_loss: 1.3060\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3065 - val_loss: 1.3058\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3062 - val_loss: 1.3057\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3055\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3055\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3055\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3065 - val_loss: 1.3075\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3069\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.1337088\n","The max value of N 0.10704818\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.614544\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 5\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 12s 2ms/step - loss: 1.5320 - val_loss: 1.6131\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 445us/step - loss: 1.3629 - val_loss: 1.5296\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3276 - val_loss: 1.4431\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3215 - val_loss: 1.3424\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3176 - val_loss: 1.3134\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3205 - val_loss: 1.3371\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3137 - val_loss: 1.3122\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3108 - val_loss: 1.3087\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3093 - val_loss: 1.3079\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3085 - val_loss: 1.3083\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3078 - val_loss: 1.3077\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3073 - val_loss: 1.3082\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3073 - val_loss: 1.3103\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3067 - val_loss: 1.3087\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3074\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3073\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3073\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3075\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3067\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3065\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3087\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3099\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3067\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3053\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.32973713\n","The max value of N 0.103027515\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6279393333333333\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6312441666666666, 0.6191481666666667, 0.5964308333333332, 0.596682, 0.6385888333333334, 0.6247698333333334, 0.631493, 0.6933173333333333, 0.614544, 0.6279393333333333]\n","AUROC ===== 0.6274157499999998 +/- 0.02577804407902737\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niGSUEoCISEYpAYiIZJQSgIhIRv1/y4+p2cPfvAAAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f2e029d1a20>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"FyKheRPFnrQE","colab_type":"text"},"cell_type":"markdown","source":["##** Frog vs All **##"]},{"metadata":{"id":"2PjYDKFgnp-U","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":64319},"outputId":"0c86a6d8-9feb-429b-80ac-c1f7d26dd783","executionInfo":{"status":"ok","timestamp":1541735369073,"user_tz":-660,"elapsed":3595225,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 902us/step - loss: 1.4830 - val_loss: 1.4964\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3480 - val_loss: 1.6404\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3263 - val_loss: 1.4447\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3186 - val_loss: 1.3113\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3144 - val_loss: 1.3069\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3126 - val_loss: 1.3065\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3105 - val_loss: 1.3064\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3093 - val_loss: 1.3063\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3082 - val_loss: 1.3062\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3089 - val_loss: 1.3087\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3080 - val_loss: 1.3068\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3073 - val_loss: 1.3069\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3069 - val_loss: 1.3066\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3079 - val_loss: 1.3072\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3083 - val_loss: 1.3179\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3080 - val_loss: 1.3121\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3074 - val_loss: 1.3105\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3070 - val_loss: 1.3099\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3069 - val_loss: 1.3092\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3067 - val_loss: 1.3077\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3067 - val_loss: 1.3095\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3068 - val_loss: 1.3117\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3070 - val_loss: 1.3190\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3068 - val_loss: 1.3161\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3067 - val_loss: 1.3172\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3066 - val_loss: 1.3095\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3089\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3063 - val_loss: 1.3074\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3075\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3062 - val_loss: 1.3068\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3067\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3062 - val_loss: 1.3069\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3061 - val_loss: 1.3067\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3069\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3084\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3084\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3086\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3087\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3107\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3103\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3105\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3095\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3094\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3097\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3173\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3183\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3161\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3156\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3145\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3166\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3133\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3151\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3203\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3120\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3109\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3132\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3107\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3085\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3115\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3063 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3059\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3080\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3076\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3085\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3083\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3169\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3160\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3147\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3132\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3120\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3124\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3138\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3142\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3123\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3118\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3119\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3101\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3105\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3108\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3128\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3076\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3068\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3077\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3072\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3072\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3078\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3080\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3082\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3082\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3059 - val_loss: 1.3087\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3085\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3087\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3087\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3083\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3087\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3089\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3092\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3141\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3108\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3093\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3095\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3090\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3087\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3085\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3087\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3089\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3089\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3088\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3087\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3085\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3085\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3083\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3083\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3083\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3086\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3085\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3083\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3084\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3086\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3090\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3058 - val_loss: 1.3087\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3087\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3088\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3087\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3058 - val_loss: 1.3088\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3087\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3058 - val_loss: 1.3086\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3085\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3087\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3086\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3086\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3083\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3085\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0522414\n","The max value of N 0.14449283\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6324671666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 852us/step - loss: 1.4795 - val_loss: 1.8346\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 433us/step - loss: 1.3509 - val_loss: 1.4034\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3251 - val_loss: 1.3258\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3204 - val_loss: 1.3151\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3215 - val_loss: 1.3602\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3144 - val_loss: 1.3480\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3156 - val_loss: 1.3451\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3111 - val_loss: 1.3067\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3093 - val_loss: 1.3059\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3090 - val_loss: 1.3058\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3081 - val_loss: 1.3058\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3082 - val_loss: 1.3078\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3090 - val_loss: 1.3067\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3085 - val_loss: 1.3065\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3068 - val_loss: 1.3058\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3063 - val_loss: 1.3057\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3064 - val_loss: 1.3060\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3058\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3059 - val_loss: 1.3127\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3057 - val_loss: 1.3103\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3057 - val_loss: 1.3078\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3057 - val_loss: 1.3066\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.39817002\n","The max value of N 0.14622267\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6230218333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 969us/step - loss: 1.5083 - val_loss: 1.9004\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 406us/step - loss: 1.3569 - val_loss: 2.0805\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3259 - val_loss: 1.5797\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3148 - val_loss: 1.3226\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3129 - val_loss: 1.3100\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3105 - val_loss: 1.3044\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3083 - val_loss: 1.3047\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3081 - val_loss: 1.3047\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3081 - val_loss: 1.3189\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3067 - val_loss: 1.3099\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3090\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3070 - val_loss: 1.3163\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3049 - val_loss: 1.3061\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3048 - val_loss: 1.3061\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3044 - val_loss: 1.3059\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3044 - val_loss: 1.3059\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 361us/step - loss: 1.3043 - val_loss: 1.3060\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3041 - val_loss: 1.3061\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3040 - val_loss: 1.3059\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3041 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3040 - val_loss: 1.3054\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3040 - val_loss: 1.3054\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3039 - val_loss: 1.3054\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3039 - val_loss: 1.3054\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3039 - val_loss: 1.3053\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3039 - val_loss: 1.3053\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3038 - val_loss: 1.3052\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3038 - val_loss: 1.3051\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3038 - val_loss: 1.3050\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3038 - val_loss: 1.3049\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3038 - val_loss: 1.3074\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3038 - val_loss: 1.3066\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3038 - val_loss: 1.3064\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3038 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3037 - val_loss: 1.3053\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3038 - val_loss: 1.3056\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3037 - val_loss: 1.3094\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3038 - val_loss: 1.3116\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3038 - val_loss: 1.3079\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3037 - val_loss: 1.3065\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3037 - val_loss: 1.3063\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3037 - val_loss: 1.3051\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3037 - val_loss: 1.3040\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3037 - val_loss: 1.3039\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3036 - val_loss: 1.3039\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3036 - val_loss: 1.3035\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3036 - val_loss: 1.3042\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3036 - val_loss: 1.3048\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3036 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3035 - val_loss: 1.3070\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3079\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3036 - val_loss: 1.3066\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3037 - val_loss: 1.3043\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3037 - val_loss: 1.3053\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3036 - val_loss: 1.3039\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3036 - val_loss: 1.3037\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3038\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3035 - val_loss: 1.3039\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3035 - val_loss: 1.3039\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3042\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3046\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3041\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3043\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3044\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3049\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 362us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3039\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3040\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3041\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3042\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 359us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0291011\n","The max value of N 0.1402323\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.670235\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.5466 - val_loss: 1.5909\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.4055 - val_loss: 1.3332\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3220 - val_loss: 1.3115\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3100 - val_loss: 1.3090\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3081 - val_loss: 1.3082\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3073 - val_loss: 1.3076\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3070 - val_loss: 1.3071\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3067 - val_loss: 1.3069\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3066 - val_loss: 1.3067\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3065 - val_loss: 1.3066\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3064 - val_loss: 1.3065\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3063 - val_loss: 1.3064\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3063\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 363us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 364us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.11725248\n","The max value of N 0.13765089\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5750518333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4917 - val_loss: 1.6177\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 408us/step - loss: 1.3471 - val_loss: 1.5264\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3211 - val_loss: 1.3911\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3139 - val_loss: 1.3249\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3100 - val_loss: 1.3269\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3091 - val_loss: 1.3165\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3073 - val_loss: 1.3090\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3049 - val_loss: 1.3044\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3037 - val_loss: 1.3023\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3032 - val_loss: 1.3175\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3030 - val_loss: 1.3044\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3018 - val_loss: 1.3096\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3013 - val_loss: 1.3137\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3016 - val_loss: 1.3034\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3012 - val_loss: 1.3083\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3015 - val_loss: 1.3047\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3012 - val_loss: 1.3075\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3010 - val_loss: 1.3085\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3009 - val_loss: 1.3071\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3008 - val_loss: 1.3029\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3008 - val_loss: 1.3021\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3007 - val_loss: 1.3010\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3007 - val_loss: 1.3014\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3008 - val_loss: 1.3035\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3007 - val_loss: 1.3064\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3007 - val_loss: 1.3031\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3006 - val_loss: 1.3058\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3006 - val_loss: 1.3065\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3006 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3006 - val_loss: 1.3077\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3073\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3073\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3012\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3014\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3015\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3019\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3020\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3019\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3018\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3015\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3016\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3017\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3017\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3017\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3018\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3016\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3014\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3013\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3013\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3011\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3011\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3010\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3010\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3011\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3010\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3011\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3011\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3016\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3015\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3014\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3014\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3013\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3013\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3013\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3010\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3010\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3004\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3010\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3009\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.992911\n","The max value of N 0.13284613\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6772669999999998\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4745 - val_loss: 1.7345\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3484 - val_loss: 1.4413\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3254 - val_loss: 1.3262\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3246 - val_loss: 1.3244\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3178 - val_loss: 1.3065\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3130 - val_loss: 1.3061\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3122 - val_loss: 1.3059\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3125 - val_loss: 1.3076\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3096 - val_loss: 1.3060\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3100 - val_loss: 1.3059\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3084 - val_loss: 1.3058\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3077 - val_loss: 1.3057\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3071 - val_loss: 1.3057\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3068 - val_loss: 1.3057\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3065 - val_loss: 1.3057\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3062 - val_loss: 1.3057\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3057\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3058 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0243508\n","The max value of N 0.13801172\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6087589999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.5241 - val_loss: 1.8416\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3627 - val_loss: 1.6042\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3320 - val_loss: 1.6185\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3220 - val_loss: 1.3472\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3172 - val_loss: 1.3115\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3143 - val_loss: 1.3102\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3130 - val_loss: 1.3102\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3126 - val_loss: 1.3105\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3116 - val_loss: 1.3101\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3114 - val_loss: 1.3107\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3109 - val_loss: 1.3102\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3107 - val_loss: 1.3102\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3105 - val_loss: 1.3100\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3103 - val_loss: 1.3103\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3102 - val_loss: 1.3101\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3101 - val_loss: 1.3104\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3102 - val_loss: 1.3112\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3104 - val_loss: 1.3101\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3111 - val_loss: 1.3106\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3104 - val_loss: 1.3101\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3101 - val_loss: 1.3099\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3101 - val_loss: 1.3100\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3100 - val_loss: 1.3101\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3100 - val_loss: 1.3101\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3099 - val_loss: 1.3100\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3098 - val_loss: 1.3099\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3098 - val_loss: 1.3098\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3111 - val_loss: 1.3106\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3106 - val_loss: 1.3226\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3103 - val_loss: 1.3114\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3100 - val_loss: 1.3106\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3100 - val_loss: 1.3104\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3099 - val_loss: 1.3102\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3099 - val_loss: 1.3100\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3098 - val_loss: 1.3100\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3098 - val_loss: 1.3101\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3098 - val_loss: 1.3102\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3098 - val_loss: 1.3101\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3098 - val_loss: 1.3102\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3098 - val_loss: 1.3102\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3098 - val_loss: 1.3106\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3098 - val_loss: 1.3105\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3098 - val_loss: 1.3103\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3097 - val_loss: 1.3104\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3116\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3097 - val_loss: 1.3110\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3097 - val_loss: 1.3104\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3103\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3106\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3097 - val_loss: 1.3104\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3103\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3101\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3126\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3110\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3106\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3101\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3097 - val_loss: 1.3097\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3101\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3108\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3097 - val_loss: 1.3102\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3101\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3099 - val_loss: 1.3116\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3097 - val_loss: 1.3103\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3100\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3097 - val_loss: 1.3099\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0288414\n","The max value of N 0.14510891\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6933643333333332\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.5025 - val_loss: 1.8236\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 435us/step - loss: 1.3605 - val_loss: 1.6359\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3311 - val_loss: 1.7741\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3226 - val_loss: 1.3385\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3135 - val_loss: 1.3264\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3110 - val_loss: 1.3069\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3128 - val_loss: 1.3206\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3094 - val_loss: 1.3088\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3080 - val_loss: 1.3069\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3071 - val_loss: 1.3061\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3066 - val_loss: 1.3061\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3063 - val_loss: 1.3061\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3065 - val_loss: 1.3101\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3075\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3068\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3059 - val_loss: 1.3067\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3068 - val_loss: 1.3102\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3062 - val_loss: 1.3074\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3057 - val_loss: 1.3069\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3063\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3062\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3069\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3065\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3061\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3097\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3092\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0283988\n","The max value of N 0.13512637\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.721529\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.4887 - val_loss: 1.9646\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 442us/step - loss: 1.3511 - val_loss: 1.9650\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3229 - val_loss: 1.5153\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3185 - val_loss: 1.3198\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3131 - val_loss: 1.3139\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3107 - val_loss: 1.3114\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3087 - val_loss: 1.3123\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3077 - val_loss: 1.3076\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3068 - val_loss: 1.3069\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3066\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3062 - val_loss: 1.3065\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3073 - val_loss: 1.3071\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3059\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3062 - val_loss: 1.3065\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3079 - val_loss: 1.3193\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3081\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3077\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3059 - val_loss: 1.3089\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3104\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3057 - val_loss: 1.3158\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3057 - val_loss: 1.3175\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3137\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3132\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3117\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3118\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3110\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3089\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3057 - val_loss: 1.3083\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3139\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3115\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3113\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3096\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3103\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3097\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3094\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3107\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3086\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3093\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3074\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3074\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3074\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3068\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3071\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3072\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3073\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3075\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3080\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3082\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3078\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3076\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3070\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3074\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3078\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3080\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3085\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3085\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3085\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3082\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3081\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3082\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3081\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3078\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3080\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3082\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3081\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0393968\n","The max value of N 0.13410436\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6390349999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 6\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 12s 2ms/step - loss: 1.5337 - val_loss: 1.8133\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3714 - val_loss: 1.6157\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3271 - val_loss: 1.4954\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3167 - val_loss: 1.3267\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3132 - val_loss: 1.3102\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3111 - val_loss: 1.3087\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3112 - val_loss: 1.3472\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3089 - val_loss: 1.3113\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3078 - val_loss: 1.3062\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3068 - val_loss: 1.3058\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3066 - val_loss: 1.3125\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3065 - val_loss: 1.3072\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3057 - val_loss: 1.3074\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3074\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3088\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3090\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3094\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3143\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3129\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3097\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3567\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3082 - val_loss: 1.3237\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3545\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3605\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3897\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3517\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3307\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3239\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3133\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3110\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3112\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3113\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3117\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3107\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3103\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3099\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3109\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3081\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3086\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3077\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3080\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3080\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3080\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3081\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3076\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3068\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3066\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3070\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3076\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3062\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3061\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3066\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3053 - val_loss: 1.3063\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0237901\n","The max value of N 0.14563873\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6764083333333333\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.6324671666666666, 0.6230218333333333, 0.670235, 0.5750518333333334, 0.6772669999999998, 0.6087589999999999, 0.6933643333333332, 0.721529, 0.6390349999999999, 0.6764083333333333]\n","AUROC ===== 0.6517138499999999 +/- 0.04158285190525921\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAKWUyilNAEoplVP/Hy0dt6zho55TAAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7f589bdf5e10>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"ESHv4ac4nzhU","colab_type":"text"},"cell_type":"markdown","source":["##**Horse vs All **##*"]},{"metadata":{"id":"ALoMitYDny7b","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":64319},"outputId":"581d988d-9eda-4c8a-a25e-759fc09e2b39","executionInfo":{"status":"ok","timestamp":1541740251564,"user_tz":-660,"elapsed":3615002,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 894us/step - loss: 1.4971 - val_loss: 1.7702\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3554 - val_loss: 1.3922\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3258 - val_loss: 1.3314\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3163 - val_loss: 1.3072\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3125 - val_loss: 1.3073\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3106 - val_loss: 1.3066\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3088 - val_loss: 1.3062\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3080 - val_loss: 1.3064\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3079 - val_loss: 1.3066\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3090 - val_loss: 1.3249\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3074 - val_loss: 1.3077\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3068 - val_loss: 1.3065\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3063\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3062\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3066 - val_loss: 1.3061\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3074 - val_loss: 1.3063\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3069 - val_loss: 1.3061\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3067 - val_loss: 1.3061\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3060\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3063 - val_loss: 1.3060\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3060\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3062 - val_loss: 1.3060\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3062 - val_loss: 1.3063\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3062\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3060\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3069\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3064 - val_loss: 1.3068\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3062 - val_loss: 1.3068\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3062 - val_loss: 1.3068\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3061 - val_loss: 1.3067\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3061 - val_loss: 1.3073\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3066\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3070\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3066\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3068\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3066\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3060\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.03878\n","The max value of N 0.13683245\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5836933333333334\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 838us/step - loss: 1.4817 - val_loss: 1.8651\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3511 - val_loss: 1.4919\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3262 - val_loss: 1.3193\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3178 - val_loss: 1.3658\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3139 - val_loss: 1.3100\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3128 - val_loss: 1.3065\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3099 - val_loss: 1.3061\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3091 - val_loss: 1.3061\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3080 - val_loss: 1.3062\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3079 - val_loss: 1.3059\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3075 - val_loss: 1.3062\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3098 - val_loss: 1.3186\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3074 - val_loss: 1.3070\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3067 - val_loss: 1.3066\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3063 - val_loss: 1.3094\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3070\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3057 - val_loss: 1.3078\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3071\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3057 - val_loss: 1.3080\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3057 - val_loss: 1.3068\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3071\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 365us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3062\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0235043\n","The max value of N 0.13425556\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6583198333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 925us/step - loss: 1.5056 - val_loss: 1.6349\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3572 - val_loss: 1.6264\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3269 - val_loss: 1.3361\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3187 - val_loss: 1.3123\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3124 - val_loss: 1.3060\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3103 - val_loss: 1.3139\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3092 - val_loss: 1.3064\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3075 - val_loss: 1.3052\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3073 - val_loss: 1.3085\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3068 - val_loss: 1.3050\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3038\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3068 - val_loss: 1.3072\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 366us/step - loss: 1.3056 - val_loss: 1.3048\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3050 - val_loss: 1.3043\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3048 - val_loss: 1.3042\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3047 - val_loss: 1.3039\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3043 - val_loss: 1.3039\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3041 - val_loss: 1.3039\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3042 - val_loss: 1.3040\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3041 - val_loss: 1.3039\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3040 - val_loss: 1.3039\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3038 - val_loss: 1.3039\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3039 - val_loss: 1.3041\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3038 - val_loss: 1.3039\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3037 - val_loss: 1.3039\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3037 - val_loss: 1.3036\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3036 - val_loss: 1.3036\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3036 - val_loss: 1.3039\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3036 - val_loss: 1.3043\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3037\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3037 - val_loss: 1.3748\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3352\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3053\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0354602\n","The max value of N 0.1623226\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5930111666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.5286 - val_loss: 1.7960\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 411us/step - loss: 1.3707 - val_loss: 2.1534\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3307 - val_loss: 1.8192\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3188 - val_loss: 1.4858\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3157 - val_loss: 1.3636\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3130 - val_loss: 1.3142\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3122 - val_loss: 1.3187\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3103 - val_loss: 1.3143\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3086 - val_loss: 1.3094\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3079 - val_loss: 1.3090\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3074 - val_loss: 1.3083\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3071 - val_loss: 1.3084\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3069 - val_loss: 1.3081\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3070 - val_loss: 1.3080\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3068 - val_loss: 1.3079\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3068 - val_loss: 1.3087\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3069 - val_loss: 1.3087\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3068 - val_loss: 1.3079\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3067 - val_loss: 1.3076\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3066 - val_loss: 1.3076\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3073\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3064 - val_loss: 1.3071\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3077\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3078 - val_loss: 1.3209\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3068 - val_loss: 1.3091\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3066 - val_loss: 1.3084\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3065 - val_loss: 1.3074\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3065 - val_loss: 1.3074\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3066 - val_loss: 1.3119\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3065 - val_loss: 1.3086\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3064 - val_loss: 1.3075\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3071\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3062 - val_loss: 1.3071\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3062 - val_loss: 1.3072\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3069\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3067\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3067\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3067\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3066\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3066\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3069\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.121027365\n","The max value of N 0.13157222\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6409253333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4987 - val_loss: 1.6787\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 419us/step - loss: 1.3519 - val_loss: 1.6865\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3231 - val_loss: 1.4064\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3138 - val_loss: 1.3085\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3098 - val_loss: 1.3048\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3057 - val_loss: 1.3022\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3032 - val_loss: 1.3015\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3022 - val_loss: 1.3011\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3015 - val_loss: 1.3010\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3012 - val_loss: 1.3007\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3009 - val_loss: 1.3006\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3008 - val_loss: 1.3006\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3007 - val_loss: 1.3005\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3006 - val_loss: 1.3005\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3006 - val_loss: 1.3013\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3049\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3073\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3069\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3005 - val_loss: 1.3032\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3015\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3011\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3008\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3007\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3005 - val_loss: 1.9028\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3390\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3005 - val_loss: 1.3023\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3006\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3006 - val_loss: 2.1809\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3005 - val_loss: 1.9547\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3005 - val_loss: 1.4253\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3005 - val_loss: 1.3869\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3005 - val_loss: 1.3587\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3005 - val_loss: 1.3030\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3005 - val_loss: 1.3010\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3004 - val_loss: 1.3009\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3020\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3010\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3012\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3008\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3004 - val_loss: 1.3007\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3004 - val_loss: 1.3006\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3004 - val_loss: 1.3006\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0226731\n","The max value of N 0.1196418\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5918755\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4738 - val_loss: 1.5711\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 408us/step - loss: 1.3524 - val_loss: 1.5086\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3306 - val_loss: 1.5084\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3204 - val_loss: 1.3123\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3160 - val_loss: 1.3071\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3124 - val_loss: 1.3061\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3118 - val_loss: 1.3076\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3089 - val_loss: 1.3058\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3076 - val_loss: 1.3057\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3071 - val_loss: 1.3058\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3067 - val_loss: 1.3057\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3063 - val_loss: 1.3058\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3060 - val_loss: 1.3057\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3058 - val_loss: 1.3056\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3058 - val_loss: 1.3056\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3071\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 367us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.6161896\n","The max value of N 0.14140597\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.605393\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 1ms/step - loss: 1.5229 - val_loss: 1.6208\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 436us/step - loss: 1.3641 - val_loss: 1.3528\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3330 - val_loss: 1.3636\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3219 - val_loss: 1.3351\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3181 - val_loss: 1.3154\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3166 - val_loss: 1.3533\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3147 - val_loss: 1.3155\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3133 - val_loss: 1.3124\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3123 - val_loss: 1.3131\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3113 - val_loss: 1.3117\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3109 - val_loss: 1.3109\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3106 - val_loss: 1.3107\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3107 - val_loss: 1.3119\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3103 - val_loss: 1.3121\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3102 - val_loss: 1.3140\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3101 - val_loss: 1.3130\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3100 - val_loss: 1.3127\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3100 - val_loss: 1.3119\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3099 - val_loss: 1.3111\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3099 - val_loss: 1.3107\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3098 - val_loss: 1.3113\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3098 - val_loss: 1.3114\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3098 - val_loss: 1.3116\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3098 - val_loss: 1.3103\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3098 - val_loss: 1.3100\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3097 - val_loss: 1.3101\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3098 - val_loss: 1.3102\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3100 - val_loss: 1.3194\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3098 - val_loss: 1.3142\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3209\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3219\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3097 - val_loss: 1.3211\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3097 - val_loss: 1.3148\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3136\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3121\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3118\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3112\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3111\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3111\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3106\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3111\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3099 - val_loss: 1.3233\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3102\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3111\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3106\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3103\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3101\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3101\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3101\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3096\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.035415\n","The max value of N 0.12439851\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6266728333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.5003 - val_loss: 1.7463\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3605 - val_loss: 1.5151\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3338 - val_loss: 1.4184\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3237 - val_loss: 1.3084\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3148 - val_loss: 1.3066\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3118 - val_loss: 1.3080\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3092 - val_loss: 1.3060\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3079 - val_loss: 1.3059\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3070 - val_loss: 1.3058\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3067 - val_loss: 1.3058\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3090 - val_loss: 1.3133\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3114 - val_loss: 1.3106\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3081 - val_loss: 1.3085\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3072 - val_loss: 1.3074\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3067 - val_loss: 1.3066\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3065 - val_loss: 1.3062\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3067 - val_loss: 1.3062\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3074 - val_loss: 1.3076\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3073 - val_loss: 1.3067\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3066 - val_loss: 1.3063\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3070\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3064 - val_loss: 1.3078\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3062 - val_loss: 1.3069\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3061\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3058 - val_loss: 1.3066\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3058 - val_loss: 1.3059\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 369us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 368us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0086508\n","The max value of N 0.13640486\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6192833333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.4878 - val_loss: 1.6548\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 460us/step - loss: 1.3541 - val_loss: 1.3516\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 436us/step - loss: 1.3271 - val_loss: 1.3201\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 418us/step - loss: 1.3173 - val_loss: 1.3062\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3135 - val_loss: 1.3062\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3110 - val_loss: 1.3061\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3104 - val_loss: 1.3183\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3103 - val_loss: 1.3096\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3075 - val_loss: 1.3066\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3067 - val_loss: 1.3058\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3061\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3057\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3056\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3073\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0091014\n","The max value of N 0.14400281\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5847603333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 7\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 11s 2ms/step - loss: 1.5213 - val_loss: 1.9752\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3575 - val_loss: 1.8186\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 407us/step - loss: 1.3271 - val_loss: 1.4975\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3208 - val_loss: 1.5047\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3156 - val_loss: 1.3542\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3131 - val_loss: 1.3208\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3114 - val_loss: 1.3081\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3101 - val_loss: 1.3076\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3108 - val_loss: 1.3442\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3236\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3083 - val_loss: 1.3159\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3074 - val_loss: 1.3083\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3069 - val_loss: 1.3083\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3067 - val_loss: 1.3085\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3066\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3065\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3058 - val_loss: 1.3061\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3069 - val_loss: 1.3139\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3080 - val_loss: 1.3552\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3070 - val_loss: 1.3136\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3087\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3074\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3078\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3069\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3058 - val_loss: 1.3067\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3057 - val_loss: 1.3066\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3057 - val_loss: 1.3063\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3070\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3056 - val_loss: 1.3071\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3067\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3056 - val_loss: 1.3066\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3056 - val_loss: 1.3076\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3069\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3069\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3065\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3063\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3074\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3073\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3053 - val_loss: 1.3058\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3059\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3057\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3053\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3053 - val_loss: 1.3055\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3053 - val_loss: 1.3055\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0310677\n","The max value of N 0.13567841\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6078753333333333\n","=======================\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.5836933333333334, 0.6583198333333333, 0.5930111666666666, 0.6409253333333333, 0.5918755, 0.605393, 0.6266728333333333, 0.6192833333333333, 0.5847603333333333, 0.6078753333333333]\n","AUROC ===== 0.611181 +/- 0.023692481243119207\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAiCIKSU/w/K5+J94CzIqgAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f96c4c53550>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"MBX8bnXdYH9U","colab_type":"text"},"cell_type":"markdown","source":["##** Truck vs All **##"]},{"metadata":{"id":"IcLzry0woBrp","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":58395},"outputId":"38cdbe4a-282e-4dc0-d2de-ab787a111618"},"cell_type":"code","source":["\n","## Obtaining the training and testing data\n","%reload_ext autoreload\n","%autoreload 2\n","from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"cifar10\"\n","IMG_DIM= 3072\n","IMG_HGT =32\n","IMG_WDT=32\n","IMG_CHANNEL=3\n","HIDDEN_LAYER_SIZE= 128\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/cifar10/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/cifar10/RCAE/\"\n","\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","# RANDOM_SEED = [42]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 450, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 480, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 432, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 68s 0us/step\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  42\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4821 - val_loss: 2.0242\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3478 - val_loss: 1.8464\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3252 - val_loss: 1.5024\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3168 - val_loss: 1.3176\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3145 - val_loss: 1.3084\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3113 - val_loss: 1.3080\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3093 - val_loss: 1.3074\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3089 - val_loss: 1.3085\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3079 - val_loss: 1.3080\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3083 - val_loss: 1.3132\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3073 - val_loss: 1.3082\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3067 - val_loss: 1.3069\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3064 - val_loss: 1.3064\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3062 - val_loss: 1.3086\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3065 - val_loss: 1.3168\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3073 - val_loss: 1.3164\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3068 - val_loss: 1.3103\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3065 - val_loss: 1.3086\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3063 - val_loss: 1.3079\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3063 - val_loss: 1.3078\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3062 - val_loss: 1.3075\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3062 - val_loss: 1.3076\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3061 - val_loss: 1.3077\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3061 - val_loss: 1.3083\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3061 - val_loss: 1.3077\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3065 - val_loss: 1.3116\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3084\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3062 - val_loss: 1.3083\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3062 - val_loss: 1.3079\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3061 - val_loss: 1.3078\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3061 - val_loss: 1.3079\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3061 - val_loss: 1.3078\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3072\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3061 - val_loss: 1.3070\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3063\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3061 - val_loss: 1.3067\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3060 - val_loss: 1.3072\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3077\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3069\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3067 - val_loss: 1.3065\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3073 - val_loss: 1.3214\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3063 - val_loss: 1.3067\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3061 - val_loss: 1.3062\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3061 - val_loss: 1.3061\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3060 - val_loss: 1.3059\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3059\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3060\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3061\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3059 - val_loss: 1.3062\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3059 - val_loss: 1.3063\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3059 - val_loss: 1.3082\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0540311\n","The max value of N 0.13879146\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.5353988333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  56\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 5s 852us/step - loss: 1.4747 - val_loss: 1.8337\n","Epoch 2/150\n","5850/5850 [==============================] - 2s 425us/step - loss: 1.3481 - val_loss: 1.5165\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3258 - val_loss: 1.3687\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3172 - val_loss: 1.3103\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3134 - val_loss: 1.3081\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3114 - val_loss: 1.3071\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3096 - val_loss: 1.3067\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3117 - val_loss: 1.3405\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3092 - val_loss: 1.3105\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3080 - val_loss: 1.3090\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3072 - val_loss: 1.3080\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3067 - val_loss: 1.3067\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3066 - val_loss: 1.3072\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3063 - val_loss: 1.3083\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3074\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3058\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3063 - val_loss: 1.3059\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3059 - val_loss: 1.3057\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3057 - val_loss: 1.3057\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3057\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.1626621\n","The max value of N 0.11473181\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.6689210000000001\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  81\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 981us/step - loss: 1.4900 - val_loss: 1.8961\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 437us/step - loss: 1.3508 - val_loss: 1.6772\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3265 - val_loss: 1.3317\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3177 - val_loss: 1.3050\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3132 - val_loss: 1.3058\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3113 - val_loss: 1.3078\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3087 - val_loss: 1.3057\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3093 - val_loss: 1.3130\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3070 - val_loss: 1.3074\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3058 - val_loss: 1.3053\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3053 - val_loss: 1.3046\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3049 - val_loss: 1.3044\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3048 - val_loss: 1.3067\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3044 - val_loss: 1.3050\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3045 - val_loss: 1.3072\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3042 - val_loss: 1.3042\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3041 - val_loss: 1.3041\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3053 - val_loss: 1.3236\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3046 - val_loss: 1.3044\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3041 - val_loss: 1.3037\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3039 - val_loss: 1.3037\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3038 - val_loss: 1.3039\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3039 - val_loss: 1.3039\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3038 - val_loss: 1.3038\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3037 - val_loss: 1.3037\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3037\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3036 - val_loss: 1.3037\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3036 - val_loss: 1.3037\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3036 - val_loss: 1.3037\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3039\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3036 - val_loss: 1.3038\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3038\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3035 - val_loss: 1.3039\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 372us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 371us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 370us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3036 - val_loss: 1.3046\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3035 - val_loss: 1.3043\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3039\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3035 - val_loss: 1.3037\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3035 - val_loss: 1.3036\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3035 - val_loss: 1.3035\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3036\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3061\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3038\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3034 - val_loss: 1.3035\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3034 - val_loss: 1.3034\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3034 - val_loss: 1.3034\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0435259\n","The max value of N 0.13664329\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7495216666666666\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  67\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 6s 1ms/step - loss: 1.5328 - val_loss: 2.1671\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 437us/step - loss: 1.3680 - val_loss: 1.8051\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 416us/step - loss: 1.3338 - val_loss: 1.3370\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3248 - val_loss: 1.3129\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3201 - val_loss: 1.3077\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3164 - val_loss: 1.3080\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3135 - val_loss: 1.3160\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3110 - val_loss: 1.3094\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3093 - val_loss: 1.3087\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3084 - val_loss: 1.3080\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3086 - val_loss: 1.3107\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3087 - val_loss: 1.3091\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3076 - val_loss: 1.3084\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3071 - val_loss: 1.3082\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3068 - val_loss: 1.3079\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3067 - val_loss: 1.3088\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3074 - val_loss: 1.3089\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3090 - val_loss: 1.3168\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3073 - val_loss: 1.3072\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3070 - val_loss: 1.3068\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3070 - val_loss: 1.3094\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3069 - val_loss: 1.3082\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3067 - val_loss: 1.3077\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3066 - val_loss: 1.3073\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3065 - val_loss: 1.3070\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3064 - val_loss: 1.3068\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3064 - val_loss: 1.3066\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3064 - val_loss: 1.3068\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3063 - val_loss: 1.3067\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3063 - val_loss: 1.3068\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3063 - val_loss: 1.3067\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3063 - val_loss: 1.3067\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3062 - val_loss: 1.3065\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3061 - val_loss: 1.3066\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3061 - val_loss: 1.3064\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3064\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3063\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3062\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 374us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 375us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3060\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 373us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3061\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3060 - val_loss: 1.3061\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0330048\n","The max value of N 0.12284504\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.722971\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  33\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 7s 1ms/step - loss: 1.4911 - val_loss: 2.0505\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 453us/step - loss: 1.3491 - val_loss: 1.6971\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 432us/step - loss: 1.3214 - val_loss: 1.4447\n","Epoch 4/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3139 - val_loss: 1.3194\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3083 - val_loss: 1.3021\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 400us/step - loss: 1.3057 - val_loss: 1.3040\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3035 - val_loss: 1.3032\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3028 - val_loss: 1.3026\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3018 - val_loss: 1.3014\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3013 - val_loss: 1.3014\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3011 - val_loss: 1.3009\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3009 - val_loss: 1.3007\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3008 - val_loss: 1.3009\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3008 - val_loss: 1.3010\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3007 - val_loss: 1.3007\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3006 - val_loss: 1.3007\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3006 - val_loss: 1.3006\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3005 - val_loss: 1.3005\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3005\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 397us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3004 - val_loss: 1.3004\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3004 - val_loss: 1.3004\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.739944\n","The max value of N 0.15317324\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7548076666666665\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  25\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.4707 - val_loss: 1.9411\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 438us/step - loss: 1.3477 - val_loss: 1.5550\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 413us/step - loss: 1.3263 - val_loss: 1.5273\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3200 - val_loss: 1.5235\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3146 - val_loss: 1.4232\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3124 - val_loss: 1.3141\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3111 - val_loss: 1.3077\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3100 - val_loss: 1.3069\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3100 - val_loss: 1.3233\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3085 - val_loss: 1.3108\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3075 - val_loss: 1.3064\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3068 - val_loss: 1.3057\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3067 - val_loss: 1.3057\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3059 - val_loss: 1.3058\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3058 - val_loss: 1.3060\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3070\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3089\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3082\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3057 - val_loss: 1.3099\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3099\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3101\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3103\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3102\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3109\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3097\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3093\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3096\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3092\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3095\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3056 - val_loss: 1.3098\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3097\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3098\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3101\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3094\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3061\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3064\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3065\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3079\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3084\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3125\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3057 - val_loss: 1.3097\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3056\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.9725337\n","The max value of N 0.13870305\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7725023333333333\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  90\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 8s 1ms/step - loss: 1.5236 - val_loss: 2.1304\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 449us/step - loss: 1.3649 - val_loss: 1.7303\n","Epoch 3/150\n","5850/5850 [==============================] - 2s 398us/step - loss: 1.3318 - val_loss: 1.3949\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3216 - val_loss: 1.3363\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3171 - val_loss: 1.3173\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3151 - val_loss: 1.3179\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3133 - val_loss: 1.3116\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3138 - val_loss: 1.3121\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3114 - val_loss: 1.3113\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3108 - val_loss: 1.3107\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3105 - val_loss: 1.3104\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3106 - val_loss: 1.3133\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3103 - val_loss: 1.3107\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3101 - val_loss: 1.3104\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3099 - val_loss: 1.3103\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3099 - val_loss: 1.3105\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3098 - val_loss: 1.3101\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3099 - val_loss: 1.3102\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3101 - val_loss: 1.3109\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3099 - val_loss: 1.3105\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3098 - val_loss: 1.3103\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3098 - val_loss: 1.3103\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3098 - val_loss: 1.3105\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3098 - val_loss: 1.3104\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3098 - val_loss: 1.3104\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3098 - val_loss: 1.3105\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3098 - val_loss: 1.3107\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3097 - val_loss: 1.3104\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3097 - val_loss: 1.3104\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3097 - val_loss: 1.3106\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3097 - val_loss: 1.3098\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3099\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3100\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3096 - val_loss: 1.3098\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3096 - val_loss: 1.3097\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3096 - val_loss: 1.3096\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3096 - val_loss: 1.3096\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.8751044\n","The max value of N 0.13504343\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7724869999999999\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  77\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 9s 2ms/step - loss: 1.4921 - val_loss: 1.8588\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 448us/step - loss: 1.3546 - val_loss: 1.7255\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3307 - val_loss: 1.3203\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 410us/step - loss: 1.3191 - val_loss: 1.3113\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3144 - val_loss: 1.3180\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3116 - val_loss: 1.3067\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3088 - val_loss: 1.3065\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3080 - val_loss: 1.3103\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3072 - val_loss: 1.3062\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3066 - val_loss: 1.3059\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3063 - val_loss: 1.3058\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3084 - val_loss: 1.3072\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3115 - val_loss: 1.3085\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3079 - val_loss: 1.3075\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3070 - val_loss: 1.3067\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3068 - val_loss: 1.3075\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3065 - val_loss: 1.3073\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3069\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3062 - val_loss: 1.3070\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3061 - val_loss: 1.3071\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3061 - val_loss: 1.3075\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3069 - val_loss: 1.3140\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3065 - val_loss: 1.3091\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3064 - val_loss: 1.3073\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3062 - val_loss: 1.3066\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3061 - val_loss: 1.3065\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3059 - val_loss: 1.3064\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3060 - val_loss: 1.3065\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3065\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3058 - val_loss: 1.3062\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3063\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3058 - val_loss: 1.3064\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3057 - val_loss: 1.3062\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3057 - val_loss: 1.3061\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3062\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3057 - val_loss: 1.3060\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3057 - val_loss: 1.3059\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3057 - val_loss: 1.3058\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3058\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3056 - val_loss: 1.3057\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 61/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 62/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3056 - val_loss: 1.3063\n","Epoch 63/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3056 - val_loss: 1.3059\n","Epoch 64/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 65/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 376us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 379us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 378us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 2s 377us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19967999 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -0.99054104\n","The max value of N 0.12223443\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.7383099999999998\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  15\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n","[INFO:] Shape of U, V (256, 128) (256, 128)\n","Train on 5850 samples, validate on 650 samples\n","Epoch 1/150\n","5850/5850 [==============================] - 10s 2ms/step - loss: 1.4740 - val_loss: 2.0109\n","Epoch 2/150\n","5850/5850 [==============================] - 3s 457us/step - loss: 1.3469 - val_loss: 1.6595\n","Epoch 3/150\n","5850/5850 [==============================] - 3s 452us/step - loss: 1.3242 - val_loss: 1.3146\n","Epoch 4/150\n","5850/5850 [==============================] - 2s 417us/step - loss: 1.3193 - val_loss: 1.3179\n","Epoch 5/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3167 - val_loss: 1.3098\n","Epoch 6/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3127 - val_loss: 1.3066\n","Epoch 7/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3100 - val_loss: 1.3101\n","Epoch 8/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3088 - val_loss: 1.3143\n","Epoch 9/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3113 - val_loss: 1.3077\n","Epoch 10/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3081 - val_loss: 1.3057\n","Epoch 11/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3069 - val_loss: 1.3056\n","Epoch 12/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3064 - val_loss: 1.3056\n","Epoch 13/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3060 - val_loss: 1.3056\n","Epoch 14/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 15/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3057 - val_loss: 1.3056\n","Epoch 16/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 17/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 18/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3056 - val_loss: 1.3055\n","Epoch 19/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 20/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 21/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 22/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3055 - val_loss: 1.3069\n","Epoch 23/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 24/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3056\n","Epoch 25/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 26/150\n","5850/5850 [==============================] - 2s 381us/step - loss: 1.3055 - val_loss: 1.3064\n","Epoch 27/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3067\n","Epoch 28/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3065\n","Epoch 29/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3063\n","Epoch 30/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 31/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 32/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 33/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3062\n","Epoch 34/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 35/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 36/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 37/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3060\n","Epoch 38/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 39/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 40/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 41/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 42/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3055 - val_loss: 1.3059\n","Epoch 43/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3055 - val_loss: 1.3058\n","Epoch 44/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 45/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 46/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 47/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 48/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 49/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3056\n","Epoch 50/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 51/150\n","5850/5850 [==============================] - 2s 382us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 52/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 53/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3075\n","Epoch 54/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 55/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 56/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3067\n","Epoch 57/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3072\n","Epoch 58/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 59/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3064\n","Epoch 60/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 61/150\n","5850/5850 [==============================] - 4s 615us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 62/150\n","5850/5850 [==============================] - 4s 740us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 63/150\n","5850/5850 [==============================] - 3s 532us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 64/150\n","5850/5850 [==============================] - 3s 559us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 65/150\n","5850/5850 [==============================] - 3s 428us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 66/150\n","5850/5850 [==============================] - 2s 414us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 67/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 68/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3058\n","Epoch 69/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 70/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 71/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3060\n","Epoch 72/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3061\n","Epoch 73/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 74/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3062\n","Epoch 75/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3059\n","Epoch 76/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3057\n","Epoch 77/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3133\n","Epoch 78/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3056 - val_loss: 1.3084\n","Epoch 79/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3063 - val_loss: 1.9265\n","Epoch 80/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3055 - val_loss: 1.4348\n","Epoch 81/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3168\n","Epoch 82/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3055 - val_loss: 1.3068\n","Epoch 83/150\n","5850/5850 [==============================] - 2s 393us/step - loss: 1.3055 - val_loss: 1.3057\n","Epoch 84/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3055 - val_loss: 1.3055\n","Epoch 85/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 86/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 87/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 88/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 89/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 90/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 91/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 92/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 93/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 94/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 95/150\n","5850/5850 [==============================] - 2s 396us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 96/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 97/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 98/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 99/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 100/150\n","5850/5850 [==============================] - 2s 395us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 101/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 102/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 103/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 104/150\n","5850/5850 [==============================] - 2s 392us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 105/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3055\n","Epoch 106/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 107/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 108/150\n","5850/5850 [==============================] - 2s 394us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 109/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 110/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 111/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 112/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 113/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 114/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 115/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 116/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 117/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 118/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 119/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 120/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 121/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 122/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 123/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 124/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 125/150\n","5850/5850 [==============================] - 2s 383us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 126/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 127/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 128/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 129/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 130/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 131/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 132/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 133/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 134/150\n","5850/5850 [==============================] - 2s 384us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 135/150\n","5850/5850 [==============================] - 2s 387us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 136/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 137/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 138/150\n","5850/5850 [==============================] - 2s 385us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 139/150\n","5850/5850 [==============================] - 2s 386us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 140/150\n","5850/5850 [==============================] - 2s 390us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 141/150\n","5850/5850 [==============================] - 2s 391us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 142/150\n","5850/5850 [==============================] - 3s 483us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 143/150\n","5850/5850 [==============================] - 3s 541us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 144/150\n","5850/5850 [==============================] - 3s 557us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 145/150\n","5850/5850 [==============================] - 3s 563us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 146/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 147/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 148/150\n","5850/5850 [==============================] - 2s 389us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 149/150\n","5850/5850 [==============================] - 2s 380us/step - loss: 1.3054 - val_loss: 1.3054\n","Epoch 150/150\n","5850/5850 [==============================] - 2s 388us/step - loss: 1.3054 - val_loss: 1.3054\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6500, 3072) 3072\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 19968000 0.0\n","The shape of N (6500, 3072)\n","The minimum value of N  -1.0168964\n","The max value of N 0.12641063\n","[INFO:] The shape of input data   (6500, 32, 32, 3)\n","[INFO:] The shape of decoded  data   (6500, 32, 32, 3)\n","[INFO:] The shape of N  data   (6500, 32, 32, 3)\n","img shape: (128, 320, 3)\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE//worst/\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_anomalousmost anomalous digit: @\n","[INFO:] The  top_100_anomalies (100, 32, 32, 3)\n","img shape: (320, 320, 3)\n","Saving Top-200most_normalmost anomalous digit: @\n","=====================\n","AUROC 0.0 0.708472\n","=======================\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/cifar10/RCAE/\n","INFO: The dataset is  cifar10\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","[INFO:] Assertions of memory muted\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Inside the CIFAR10_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/reports/figures/cifar10/RCAE/\n","[INFO:] Loading data...\n","The normal label used in experiment, 9\n","INFO: Random Seed set is  11\n","x_outlier shape: (500, 32, 32, 3)\n","Data loaded.\n","Train Data Shape:  (6500, 32, 32, 3)\n","Train Label Shape:  (6500,)\n","Validation Data Shape:  (6500, 32, 32, 3)\n","Validation Label Shape:  (6500,)\n","Test Data Shape:  (6500, 32, 32, 3)\n","Test Label Shape:  (6500,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO] compiling model...\n"],"name":"stdout"}]},{"metadata":{"id":"2dPb3wdU-lHb","colab_type":"text"},"cell_type":"markdown","source":["##** END OF EXPERIMENTS  **##"]},{"metadata":{"id":"EviuCbjUI9cW","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}