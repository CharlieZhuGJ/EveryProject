{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DCAE_MNIST_Experiments--Success.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"izSliiGYb5NE","colab_type":"code","outputId":"44dfe185-9541-4b01-f535-e9ed95b825d4","executionInfo":{"status":"ok","timestamp":1541369252648,"user_tz":-660,"elapsed":7196,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":391}},"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","!pip install fuel\n","!pip install picklable_itertools"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Requirement already satisfied: fuel in /usr/local/lib/python3.6/dist-packages (0.2.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from fuel) (1.11.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from fuel) (0.19.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from fuel) (2.18.4)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from fuel) (2.8.0)\n","Requirement already satisfied: progressbar2 in /usr/local/lib/python3.6/dist-packages (from fuel) (3.38.0)\n","Requirement already satisfied: tables in /usr/local/lib/python3.6/dist-packages (from fuel) (3.4.4)\n","Requirement already satisfied: pyzmq in /usr/local/lib/python3.6/dist-packages (from fuel) (16.0.4)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.6/dist-packages (from fuel) (4.0.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from fuel) (1.14.6)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from fuel) (3.13)\n","Requirement already satisfied: picklable-itertools in /usr/local/lib/python3.6/dist-packages (from fuel) (0.1.1)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2.6)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (2018.10.15)\n","Requirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (1.22)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->fuel) (3.0.4)\n","Requirement already satisfied: python-utils>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from progressbar2->fuel) (2.3.0)\n","Requirement already satisfied: numexpr>=2.5.2 in /usr/local/lib/python3.6/dist-packages (from tables->fuel) (2.6.8)\n","Requirement already satisfied: olefile in /usr/local/lib/python3.6/dist-packages (from pillow->fuel) (0.46)\n","Requirement already satisfied: picklable_itertools in /usr/local/lib/python3.6/dist-packages (0.1.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from picklable_itertools) (1.11.0)\n"],"name":"stdout"}]},{"metadata":{"id":"dzqnWOhKbdZt","colab_type":"code","colab":{}},"cell_type":"code","source":["%matplotlib inline\n","import numpy as np\n","# np.random.seed(42)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5k3HKHTabdZw","colab_type":"code","colab":{}},"cell_type":"code","source":["%reload_ext autoreload\n","%autoreload 2\n","\n","PROJECT_DIR = \"/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/\"\n","import sys,os\n","import numpy as np\n","sys.path.append(PROJECT_DIR)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"lvRl3Ryn67u3","colab_type":"text"},"cell_type":"markdown","source":["# **DCAE-MNIST 9_Vs_all** "]},{"metadata":{"id":"SgVS5SOM63Ae","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":99917},"outputId":"a29cb7d1-ed80-49bd-b29e-c9407a7733f0","executionInfo":{"status":"ok","timestamp":1541325979925,"user_tz":-660,"elapsed":4364776,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 5s 860us/step - loss: 5.0727 - val_loss: 5.1976\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9959 - val_loss: 5.0086\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9820 - val_loss: 4.9833\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9754 - val_loss: 4.9786\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9709 - val_loss: 4.9737\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9676 - val_loss: 4.9730\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9652 - val_loss: 4.9753\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9652 - val_loss: 4.9745\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9635 - val_loss: 4.9730\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9617 - val_loss: 4.9738\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9617 - val_loss: 4.9745\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9600 - val_loss: 4.9716\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9593 - val_loss: 4.9660\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9592 - val_loss: 4.9686\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9583 - val_loss: 4.9658\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9584 - val_loss: 4.9720\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9589 - val_loss: 4.9706\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9584 - val_loss: 4.9657\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9587 - val_loss: 4.9689\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9580 - val_loss: 4.9652\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9574 - val_loss: 4.9656\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9572 - val_loss: 4.9642\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9570 - val_loss: 4.9613\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9571 - val_loss: 4.9651\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9642\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9688\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9570 - val_loss: 4.9674\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9611 - val_loss: 4.9769\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9576 - val_loss: 4.9663\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9568 - val_loss: 4.9628\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9565 - val_loss: 4.9610\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9614\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9603\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9588\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9595\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9563 - val_loss: 4.9610\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9562 - val_loss: 4.9628\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9597\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9592\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9705\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9607\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9785\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9669\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9635\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9593\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9563\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724314 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99998444\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9963187360993941\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 4s 793us/step - loss: 5.0627 - val_loss: 5.1586\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9927 - val_loss: 5.0135\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9803 - val_loss: 4.9938\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9737 - val_loss: 4.9767\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9693 - val_loss: 4.9738\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9662 - val_loss: 4.9743\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9641 - val_loss: 4.9695\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9632 - val_loss: 4.9677\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9609 - val_loss: 4.9682\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9612 - val_loss: 4.9686\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9596 - val_loss: 4.9675\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9594 - val_loss: 4.9717\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9619 - val_loss: 4.9752\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9591 - val_loss: 4.9690\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9592 - val_loss: 4.9718\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9580 - val_loss: 4.9665\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9572 - val_loss: 4.9647\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9570 - val_loss: 4.9636\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9623\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9566 - val_loss: 4.9617\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9611\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9557 - val_loss: 4.9605\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9564 - val_loss: 4.9630\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9577 - val_loss: 4.9718\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9563 - val_loss: 4.9645\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9578 - val_loss: 4.9644\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9563 - val_loss: 4.9611\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9556 - val_loss: 4.9618\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9562 - val_loss: 4.9649\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9592\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9601\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9644\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9602\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9574 - val_loss: 4.9729\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9610\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9716\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9611\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9636\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9551\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9552\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9551\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9550\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9552\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9550\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9549\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9551\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9548\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9550\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9550\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9550\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9550\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9548\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9550\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9768\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9586\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9549\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9550\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9549\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9548\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9548\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9548\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9548\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9548\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9548\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9547\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9547\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9546\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9546\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9546\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9548\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9546\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9547\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9547\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9549\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9547\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9546\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9544\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9545\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9546\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9548\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9517 - val_loss: 4.9548\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9517 - val_loss: 4.9548\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9517 - val_loss: 4.9543\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9657\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9552\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9576\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9518 - val_loss: 4.9579\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9742\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9548\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9515 - val_loss: 4.9546\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9515 - val_loss: 4.9546\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9552\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9688\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9635\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9585\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9565\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9517 - val_loss: 4.9551\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9517 - val_loss: 4.9547\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9516 - val_loss: 4.9544\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724383 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -0.99666095\n","The max value of N 0.9999893\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9878171752548622\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 5s 1ms/step - loss: 5.0599 - val_loss: 5.1272\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9934 - val_loss: 5.0145\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9819 - val_loss: 4.9910\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9756 - val_loss: 4.9795\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9707 - val_loss: 4.9803\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9671 - val_loss: 4.9748\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9649 - val_loss: 4.9705\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9632 - val_loss: 4.9705\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9623 - val_loss: 4.9697\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9608 - val_loss: 4.9664\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9601 - val_loss: 4.9645\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9623 - val_loss: 4.9887\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9617 - val_loss: 4.9937\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9603 - val_loss: 4.9931\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9596 - val_loss: 4.9721\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9590 - val_loss: 4.9695\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9585 - val_loss: 4.9693\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9616 - val_loss: 4.9873\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9587 - val_loss: 4.9670\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9578 - val_loss: 4.9628\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9577 - val_loss: 4.9610\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9573 - val_loss: 4.9619\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9573 - val_loss: 4.9618\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9570 - val_loss: 4.9604\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9595\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9569 - val_loss: 4.9611\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9567 - val_loss: 4.9609\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9564 - val_loss: 4.9595\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9589\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9563 - val_loss: 4.9591\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9580 - val_loss: 4.9654\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9567 - val_loss: 4.9605\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9561 - val_loss: 4.9586\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9583\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9558 - val_loss: 4.9587\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9578\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9577\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9575\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9566 - val_loss: 4.9601\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9589\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9598\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9595\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9601\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9560 - val_loss: 4.9594\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9568\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9564\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9568\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9824\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9653\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9597\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9611\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9565\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9675\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9597\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9562\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9579 - val_loss: 4.9789\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9563 - val_loss: 4.9669\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9644\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9616\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9597\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9562\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9553\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724379 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -0.99586606\n","The max value of N 0.99867576\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9804290831210073\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 6s 1ms/step - loss: 5.0712 - val_loss: 5.1596\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9941 - val_loss: 5.0063\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9801 - val_loss: 4.9889\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9735 - val_loss: 4.9768\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9699 - val_loss: 4.9747\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9666 - val_loss: 4.9735\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9651 - val_loss: 4.9703\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9628 - val_loss: 4.9662\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9618 - val_loss: 4.9658\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9609 - val_loss: 4.9662\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9609 - val_loss: 4.9728\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9605 - val_loss: 4.9750\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9595 - val_loss: 4.9678\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9588 - val_loss: 4.9725\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9625 - val_loss: 5.0059\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9604 - val_loss: 4.9850\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9589 - val_loss: 4.9747\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9582 - val_loss: 4.9679\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9579 - val_loss: 4.9644\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9587 - val_loss: 4.9664\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9576 - val_loss: 4.9626\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9573 - val_loss: 4.9643\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9573 - val_loss: 4.9641\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9626\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9564 - val_loss: 4.9602\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9612\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9567 - val_loss: 4.9607\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9596\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9568 - val_loss: 4.9682\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9567 - val_loss: 4.9645\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9621\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9564 - val_loss: 4.9620\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9592\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9584\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9558\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9559\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9521 - val_loss: 4.9558\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9560\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9555\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9520 - val_loss: 4.9557\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9554\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9555\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9555\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9519 - val_loss: 4.9554\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9518 - val_loss: 4.9553\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9553\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9554\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724383 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.996548815093182\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 6s 1ms/step - loss: 5.0735 - val_loss: 5.1915\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9987 - val_loss: 5.0273\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9838 - val_loss: 4.9921\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9762 - val_loss: 4.9824\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9721 - val_loss: 4.9812\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9688 - val_loss: 4.9737\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9666 - val_loss: 4.9716\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9645 - val_loss: 4.9692\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9629 - val_loss: 4.9739\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9650 - val_loss: 4.9796\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9619 - val_loss: 4.9742\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9618 - val_loss: 4.9764\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9610 - val_loss: 4.9745\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9603 - val_loss: 4.9681\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9599 - val_loss: 4.9672\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9595 - val_loss: 4.9672\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9588 - val_loss: 4.9635\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9585 - val_loss: 4.9647\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9582 - val_loss: 4.9632\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9577 - val_loss: 4.9621\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9576 - val_loss: 4.9626\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9595 - val_loss: 4.9718\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9582 - val_loss: 4.9662\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9574 - val_loss: 4.9694\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9571 - val_loss: 4.9642\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9568 - val_loss: 4.9615\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9567 - val_loss: 4.9613\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9584\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9562 - val_loss: 4.9684\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9567 - val_loss: 4.9633\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9562 - val_loss: 4.9596\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9559 - val_loss: 4.9593\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9587\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9558 - val_loss: 4.9579\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9553 - val_loss: 4.9571\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9566\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9566\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9566\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9565\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9562\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9619\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9616\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9559\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9555\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724167 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9977361363203836\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 7s 1ms/step - loss: 5.0642 - val_loss: 5.1489\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9948 - val_loss: 5.0126\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9824 - val_loss: 4.9958\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9761 - val_loss: 4.9847\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9714 - val_loss: 4.9786\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9676 - val_loss: 4.9773\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9656 - val_loss: 4.9729\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9636 - val_loss: 4.9718\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9633 - val_loss: 4.9722\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9631 - val_loss: 4.9703\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9616 - val_loss: 4.9781\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9624 - val_loss: 4.9886\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9637 - val_loss: 4.9749\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9617 - val_loss: 4.9754\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9606 - val_loss: 4.9671\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9604 - val_loss: 4.9669\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9602 - val_loss: 4.9669\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9598 - val_loss: 4.9691\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9588 - val_loss: 4.9639\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9592 - val_loss: 4.9691\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9589 - val_loss: 4.9643\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9583 - val_loss: 4.9625\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9590 - val_loss: 4.9633\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9585 - val_loss: 4.9625\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9584 - val_loss: 4.9625\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9579 - val_loss: 4.9614\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9577 - val_loss: 4.9660\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9589 - val_loss: 4.9870\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9585 - val_loss: 4.9703\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9577 - val_loss: 4.9652\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9575 - val_loss: 4.9613\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9570 - val_loss: 4.9607\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9569 - val_loss: 4.9605\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9569 - val_loss: 4.9667\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9594 - val_loss: 5.0161\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9577 - val_loss: 4.9741\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9573 - val_loss: 4.9666\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9570 - val_loss: 4.9634\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9568 - val_loss: 4.9616\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9568 - val_loss: 4.9604\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9564 - val_loss: 4.9600\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9564 - val_loss: 4.9595\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9569 - val_loss: 4.9603\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9565 - val_loss: 4.9593\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9563 - val_loss: 4.9587\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9586\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9592\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9573 - val_loss: 4.9716\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9567 - val_loss: 4.9644\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9564 - val_loss: 4.9604\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9563 - val_loss: 4.9602\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9560 - val_loss: 4.9587\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9617\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9558 - val_loss: 4.9598\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9558 - val_loss: 4.9620\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9595\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9662\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9566\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724348 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -0.99642986\n","The max value of N 0.9999048\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9967874155311842\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 8s 1ms/step - loss: 5.0756 - val_loss: 5.4365\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9965 - val_loss: 5.0245\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9819 - val_loss: 4.9905\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9744 - val_loss: 4.9758\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9691 - val_loss: 4.9800\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9695 - val_loss: 5.0006\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9657 - val_loss: 4.9807\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9685 - val_loss: 4.9900\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9654 - val_loss: 4.9743\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9632 - val_loss: 4.9721\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9630 - val_loss: 4.9738\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9645 - val_loss: 4.9776\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9619 - val_loss: 4.9675\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9607 - val_loss: 4.9647\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9609 - val_loss: 4.9660\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9613 - val_loss: 4.9657\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9602 - val_loss: 4.9633\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9599 - val_loss: 4.9622\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9593 - val_loss: 4.9636\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9591 - val_loss: 4.9620\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9588 - val_loss: 4.9615\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9583 - val_loss: 4.9612\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9583 - val_loss: 4.9612\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9596 - val_loss: 4.9656\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9586 - val_loss: 4.9619\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9579 - val_loss: 4.9607\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9576 - val_loss: 4.9603\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9572 - val_loss: 4.9601\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9573 - val_loss: 4.9630\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9627 - val_loss: 4.9727\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9593 - val_loss: 4.9667\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9583 - val_loss: 4.9651\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9577 - val_loss: 4.9636\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9572 - val_loss: 4.9617\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9571 - val_loss: 4.9603\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9569 - val_loss: 4.9598\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9567 - val_loss: 4.9598\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9567 - val_loss: 4.9592\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9563 - val_loss: 4.9590\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9566 - val_loss: 4.9599\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9572 - val_loss: 4.9731\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9565 - val_loss: 4.9616\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9563 - val_loss: 4.9590\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9591\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9562 - val_loss: 4.9762\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9586 - val_loss: 4.9814\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9568 - val_loss: 4.9645\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9563 - val_loss: 4.9599\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9561 - val_loss: 4.9594\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9552 - val_loss: 4.9572\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9572\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9575 - val_loss: 4.9629\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9607\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9587\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9556 - val_loss: 4.9621\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9548 - val_loss: 4.9570\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9568\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9566\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9547 - val_loss: 4.9569\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9568\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9565\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9544 - val_loss: 4.9565\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9544 - val_loss: 4.9564\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9633\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9621\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9583\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9644\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9599\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9557\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724382 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -0.99373347\n","The max value of N 0.99942255\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.99241307416781\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 8s 2ms/step - loss: 5.0554 - val_loss: 5.1294\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9946 - val_loss: 5.0174\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9821 - val_loss: 4.9991\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9755 - val_loss: 4.9837\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9721 - val_loss: 4.9777\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9688 - val_loss: 4.9774\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9671 - val_loss: 4.9782\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9656 - val_loss: 4.9792\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9634 - val_loss: 4.9720\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9620 - val_loss: 4.9702\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9632 - val_loss: 4.9713\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9612 - val_loss: 4.9703\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9606 - val_loss: 4.9698\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9599 - val_loss: 4.9679\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9593 - val_loss: 4.9659\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9585 - val_loss: 4.9644\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9590 - val_loss: 4.9647\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9596 - val_loss: 4.9650\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9586 - val_loss: 4.9637\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9600 - val_loss: 4.9759\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9582 - val_loss: 4.9658\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9576 - val_loss: 4.9673\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9614 - val_loss: 5.0081\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9610 - val_loss: 4.9830\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9593 - val_loss: 4.9763\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9581 - val_loss: 4.9672\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9587 - val_loss: 4.9673\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9581 - val_loss: 4.9616\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9576 - val_loss: 4.9627\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9570 - val_loss: 4.9609\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9570 - val_loss: 4.9611\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9566 - val_loss: 4.9601\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9566 - val_loss: 4.9598\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9564 - val_loss: 4.9595\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9563 - val_loss: 4.9637\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9647 - val_loss: 4.9812\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9587 - val_loss: 4.9648\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9578 - val_loss: 4.9618\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9570 - val_loss: 4.9609\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9570 - val_loss: 4.9597\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9565 - val_loss: 4.9605\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9561 - val_loss: 4.9597\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9560 - val_loss: 4.9594\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9559 - val_loss: 4.9597\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9557 - val_loss: 4.9602\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9564 - val_loss: 4.9711\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9557 - val_loss: 4.9621\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9550 - val_loss: 4.9618\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9542 - val_loss: 4.9589\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9664\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9534 - val_loss: 4.9609\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9586\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9524 - val_loss: 4.9557\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9522 - val_loss: 4.9555\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724364 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -0.99551183\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.993151599333055\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 9s 2ms/step - loss: 5.0606 - val_loss: 5.1139\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9935 - val_loss: 5.0011\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9803 - val_loss: 4.9833\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9751 - val_loss: 4.9763\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9706 - val_loss: 4.9733\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9671 - val_loss: 4.9722\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9645 - val_loss: 4.9717\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9631 - val_loss: 4.9693\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9614 - val_loss: 4.9657\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9602 - val_loss: 4.9665\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9590 - val_loss: 4.9653\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9583 - val_loss: 4.9670\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9604 - val_loss: 5.0093\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9632 - val_loss: 4.9881\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9583 - val_loss: 4.9689\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9575 - val_loss: 4.9627\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9575 - val_loss: 4.9608\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9565 - val_loss: 4.9604\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9565 - val_loss: 4.9599\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9560 - val_loss: 4.9593\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9556 - val_loss: 4.9578\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9578\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9555 - val_loss: 4.9590\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9582 - val_loss: 4.9668\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9561 - val_loss: 4.9627\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9573 - val_loss: 4.9960\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9563 - val_loss: 4.9714\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9566 - val_loss: 4.9619\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9550 - val_loss: 4.9572\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9572\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9550 - val_loss: 4.9596\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9547 - val_loss: 4.9566\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9545 - val_loss: 4.9560\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9560\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 316us/step - loss: 4.9542 - val_loss: 4.9561\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9572 - val_loss: 4.9671\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9596\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9540 - val_loss: 4.9558\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9560\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9538 - val_loss: 4.9557\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9538 - val_loss: 4.9556\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9537 - val_loss: 4.9555\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9552\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9550\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9550\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9550\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9534 - val_loss: 4.9552\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9551\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9650\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9552\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9553\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9551\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9551\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9550\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9548\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9550\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9549\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9548\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9548\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9528 - val_loss: 4.9547\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9548\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9550\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9547\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9546\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9546\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9549\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9547\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9526 - val_loss: 4.9548\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9547\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9552\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9548\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9551\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9548\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9547\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9547\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9548\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9548\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9526 - val_loss: 4.9548\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9547\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9525 - val_loss: 4.9547\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9547\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9546\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9546\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9524 - val_loss: 4.9547\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9548\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9547\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9547\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9524 - val_loss: 4.9545\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9522 - val_loss: 4.9545\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9546\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9545\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9524 - val_loss: 4.9546\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9523 - val_loss: 4.9545\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9546\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9522 - val_loss: 4.9545\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9524 - val_loss: 4.9546\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9545\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9523 - val_loss: 4.9545\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9523 - val_loss: 4.9547\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9545\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9545\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9522 - val_loss: 4.9544\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9544\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9546\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9522 - val_loss: 4.9545\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9521 - val_loss: 4.9545\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9640\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9549\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9547\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9523 - val_loss: 4.9549\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9548\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9523 - val_loss: 4.9546\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9545\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9521 - val_loss: 4.9548\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9520 - val_loss: 4.9545\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9543\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9520 - val_loss: 4.9544\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9521 - val_loss: 4.9547\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9543\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9519 - val_loss: 4.9544\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9519 - val_loss: 4.9543\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9520 - val_loss: 4.9548\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9546\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9547\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9523 - val_loss: 4.9548\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9520 - val_loss: 4.9545\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9577\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9552\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9544\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9518 - val_loss: 4.9548\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9520 - val_loss: 4.9551\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9549\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9543\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9557\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9519 - val_loss: 4.9545\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9542\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9543\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9518 - val_loss: 4.9543\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9543\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9517 - val_loss: 4.9546\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9544\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9543\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9520 - val_loss: 4.9546\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9549\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9546\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9550\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9545\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9549\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9547\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9547\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9545\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9547\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9543\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9518 - val_loss: 4.9544\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9516 - val_loss: 4.9544\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9544\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9515 - val_loss: 4.9542\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9583\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9516 - val_loss: 4.9546\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9545\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9515 - val_loss: 4.9544\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9515 - val_loss: 4.9545\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724312 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -0.9977356\n","The max value of N 0.9974698\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9872689623437381\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  9\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5009, 28, 28, 1)\n","Train Label Shape:  (5009,)\n","Validation Data Shape:  (1001, 28, 28, 1)\n","Validation Label Shape:  (1001,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6026, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5967, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5423 samples, validate on 603 samples\n","Epoch 1/250\n","5423/5423 [==============================] - 10s 2ms/step - loss: 5.0630 - val_loss: 5.1787\n","Epoch 2/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9932 - val_loss: 5.0215\n","Epoch 3/250\n","5423/5423 [==============================] - 2s 314us/step - loss: 4.9806 - val_loss: 4.9976\n","Epoch 4/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9747 - val_loss: 4.9786\n","Epoch 5/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9701 - val_loss: 4.9746\n","Epoch 6/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9665 - val_loss: 4.9694\n","Epoch 7/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9644 - val_loss: 4.9659\n","Epoch 8/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9622 - val_loss: 4.9659\n","Epoch 9/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9608 - val_loss: 4.9676\n","Epoch 10/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9599 - val_loss: 4.9642\n","Epoch 11/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9591 - val_loss: 4.9613\n","Epoch 12/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9582 - val_loss: 4.9612\n","Epoch 13/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9580 - val_loss: 4.9618\n","Epoch 14/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9580 - val_loss: 4.9635\n","Epoch 15/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9570 - val_loss: 4.9610\n","Epoch 16/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9567 - val_loss: 4.9602\n","Epoch 17/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9563 - val_loss: 4.9622\n","Epoch 18/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9571 - val_loss: 4.9838\n","Epoch 19/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9604 - val_loss: 5.0100\n","Epoch 20/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9576 - val_loss: 4.9788\n","Epoch 21/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9616 - val_loss: 5.0157\n","Epoch 22/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9582 - val_loss: 4.9742\n","Epoch 23/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9568 - val_loss: 4.9670\n","Epoch 24/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9563 - val_loss: 4.9607\n","Epoch 25/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9561 - val_loss: 4.9593\n","Epoch 26/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 27/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9553 - val_loss: 4.9574\n","Epoch 28/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9554 - val_loss: 4.9599\n","Epoch 29/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9554 - val_loss: 4.9610\n","Epoch 30/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 31/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 32/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 33/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 34/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9544 - val_loss: 4.9565\n","Epoch 35/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 36/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 37/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 38/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 39/250\n","5423/5423 [==============================] - 2s 315us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 40/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 41/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 42/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 43/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 44/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 45/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9570 - val_loss: 4.9602\n","Epoch 46/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 47/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 48/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 49/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9555\n","Epoch 50/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 51/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 52/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9553\n","Epoch 53/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9552\n","Epoch 54/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 55/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 56/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 57/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 58/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 59/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9552\n","Epoch 60/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 61/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9550\n","Epoch 62/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 63/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9552\n","Epoch 64/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 65/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 66/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 67/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 68/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 69/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 70/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9549\n","Epoch 71/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9526 - val_loss: 4.9548\n","Epoch 72/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9525 - val_loss: 4.9548\n","Epoch 73/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9544\n","Epoch 74/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 75/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9553\n","Epoch 76/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9550\n","Epoch 77/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9527 - val_loss: 4.9546\n","Epoch 78/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9544\n","Epoch 79/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9548\n","Epoch 80/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9523 - val_loss: 4.9544\n","Epoch 81/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 82/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9532 - val_loss: 4.9552\n","Epoch 83/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9525 - val_loss: 4.9546\n","Epoch 84/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9524 - val_loss: 4.9544\n","Epoch 85/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9522 - val_loss: 4.9545\n","Epoch 86/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 87/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 88/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9545\n","Epoch 89/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9521 - val_loss: 4.9547\n","Epoch 90/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9521 - val_loss: 4.9543\n","Epoch 91/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 92/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 93/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9520 - val_loss: 4.9543\n","Epoch 94/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9541\n","Epoch 95/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9542\n","Epoch 96/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9541\n","Epoch 97/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9547\n","Epoch 98/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9543\n","Epoch 99/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9543\n","Epoch 100/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9542\n","Epoch 101/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9543\n","Epoch 102/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9544\n","Epoch 103/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9542\n","Epoch 104/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9520 - val_loss: 4.9540\n","Epoch 105/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9542\n","Epoch 106/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9552\n","Epoch 107/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9523 - val_loss: 4.9625\n","Epoch 108/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9653\n","Epoch 109/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 110/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 111/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9525 - val_loss: 4.9548\n","Epoch 112/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9523 - val_loss: 4.9549\n","Epoch 113/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9523 - val_loss: 4.9543\n","Epoch 114/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9522 - val_loss: 4.9545\n","Epoch 115/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9522 - val_loss: 4.9547\n","Epoch 116/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 117/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9520 - val_loss: 4.9541\n","Epoch 118/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 119/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9521 - val_loss: 4.9544\n","Epoch 120/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9519 - val_loss: 4.9541\n","Epoch 121/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9519 - val_loss: 4.9543\n","Epoch 122/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9518 - val_loss: 4.9542\n","Epoch 123/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9519 - val_loss: 4.9546\n","Epoch 124/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9524 - val_loss: 4.9549\n","Epoch 125/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9521 - val_loss: 4.9545\n","Epoch 126/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9518 - val_loss: 4.9543\n","Epoch 127/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9518 - val_loss: 4.9542\n","Epoch 128/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9540\n","Epoch 129/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9517 - val_loss: 4.9542\n","Epoch 130/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9518 - val_loss: 4.9542\n","Epoch 131/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9518 - val_loss: 4.9542\n","Epoch 132/250\n","5423/5423 [==============================] - 2s 314us/step - loss: 4.9516 - val_loss: 4.9540\n","Epoch 133/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9520 - val_loss: 4.9542\n","Epoch 134/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9519 - val_loss: 4.9541\n","Epoch 135/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9542\n","Epoch 136/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9517 - val_loss: 4.9541\n","Epoch 137/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9541\n","Epoch 138/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9517 - val_loss: 4.9542\n","Epoch 139/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9516 - val_loss: 4.9542\n","Epoch 140/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9515 - val_loss: 4.9543\n","Epoch 141/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9519 - val_loss: 4.9559\n","Epoch 142/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9541\n","Epoch 143/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9541\n","Epoch 144/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9515 - val_loss: 4.9537\n","Epoch 145/250\n","5423/5423 [==============================] - 2s 313us/step - loss: 4.9516 - val_loss: 4.9538\n","Epoch 146/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9518 - val_loss: 4.9539\n","Epoch 147/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9516 - val_loss: 4.9536\n","Epoch 148/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 149/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9514 - val_loss: 4.9538\n","Epoch 150/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9513 - val_loss: 4.9539\n","Epoch 151/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9514 - val_loss: 4.9540\n","Epoch 152/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9514 - val_loss: 4.9539\n","Epoch 153/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9514 - val_loss: 4.9540\n","Epoch 154/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9514 - val_loss: 4.9541\n","Epoch 155/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9514 - val_loss: 4.9538\n","Epoch 156/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9538\n","Epoch 157/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9516 - val_loss: 4.9543\n","Epoch 158/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9514 - val_loss: 4.9538\n","Epoch 159/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9516 - val_loss: 4.9545\n","Epoch 160/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9514 - val_loss: 4.9542\n","Epoch 161/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9514 - val_loss: 4.9537\n","Epoch 162/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9514 - val_loss: 4.9546\n","Epoch 163/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9514 - val_loss: 4.9537\n","Epoch 164/250\n","5423/5423 [==============================] - 2s 303us/step - loss: 4.9513 - val_loss: 4.9540\n","Epoch 165/250\n","5423/5423 [==============================] - 2s 312us/step - loss: 4.9513 - val_loss: 4.9535\n","Epoch 166/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9514 - val_loss: 4.9538\n","Epoch 167/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9513 - val_loss: 4.9536\n","Epoch 168/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9513 - val_loss: 4.9540\n","Epoch 169/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9538\n","Epoch 170/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9513 - val_loss: 4.9536\n","Epoch 171/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9512 - val_loss: 4.9542\n","Epoch 172/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9515 - val_loss: 4.9572\n","Epoch 173/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9554\n","Epoch 174/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9514 - val_loss: 4.9544\n","Epoch 175/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9540\n","Epoch 176/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9541\n","Epoch 177/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 178/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9513 - val_loss: 4.9536\n","Epoch 179/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9513 - val_loss: 4.9536\n","Epoch 180/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 181/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9511 - val_loss: 4.9535\n","Epoch 182/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 183/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 184/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 185/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 186/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9511 - val_loss: 4.9537\n","Epoch 187/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9511 - val_loss: 4.9537\n","Epoch 188/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9541\n","Epoch 189/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9513 - val_loss: 4.9539\n","Epoch 190/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 191/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9512 - val_loss: 4.9536\n","Epoch 192/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9513 - val_loss: 4.9574\n","Epoch 193/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9537\n","Epoch 194/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9537\n","Epoch 195/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9513 - val_loss: 4.9540\n","Epoch 196/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9537\n","Epoch 197/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9537\n","Epoch 198/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9511 - val_loss: 4.9540\n","Epoch 199/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9510 - val_loss: 4.9537\n","Epoch 200/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9511 - val_loss: 4.9536\n","Epoch 201/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9537\n","Epoch 202/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9511 - val_loss: 4.9536\n","Epoch 203/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9536\n","Epoch 204/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9510 - val_loss: 4.9536\n","Epoch 205/250\n","5423/5423 [==============================] - 2s 304us/step - loss: 4.9511 - val_loss: 4.9536\n","Epoch 206/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9537\n","Epoch 207/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9613\n","Epoch 208/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9514 - val_loss: 4.9546\n","Epoch 209/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9540\n","Epoch 210/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9538\n","Epoch 211/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9511 - val_loss: 4.9538\n","Epoch 212/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9511 - val_loss: 4.9540\n","Epoch 213/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9512 - val_loss: 4.9537\n","Epoch 214/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9512 - val_loss: 4.9539\n","Epoch 215/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9511 - val_loss: 4.9536\n","Epoch 216/250\n","5423/5423 [==============================] - 2s 310us/step - loss: 4.9510 - val_loss: 4.9536\n","Epoch 217/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9509 - val_loss: 4.9535\n","Epoch 218/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9510 - val_loss: 4.9535\n","Epoch 219/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9510 - val_loss: 4.9536\n","Epoch 220/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9510 - val_loss: 4.9535\n","Epoch 221/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9509 - val_loss: 4.9535\n","Epoch 222/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9510 - val_loss: 4.9537\n","Epoch 223/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9536\n","Epoch 224/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9510 - val_loss: 4.9539\n","Epoch 225/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9512 - val_loss: 4.9540\n","Epoch 226/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9510 - val_loss: 4.9537\n","Epoch 227/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9510 - val_loss: 4.9537\n","Epoch 228/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9510 - val_loss: 4.9537\n","Epoch 229/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9510 - val_loss: 4.9536\n","Epoch 230/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9509 - val_loss: 4.9536\n","Epoch 231/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9509 - val_loss: 4.9537\n","Epoch 232/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9509 - val_loss: 4.9536\n","Epoch 233/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9509 - val_loss: 4.9539\n","Epoch 234/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9510 - val_loss: 4.9536\n","Epoch 235/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9510 - val_loss: 4.9537\n","Epoch 236/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9508 - val_loss: 4.9535\n","Epoch 237/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9509 - val_loss: 4.9536\n","Epoch 238/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9509 - val_loss: 4.9536\n","Epoch 239/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9536\n","Epoch 240/250\n","5423/5423 [==============================] - 2s 305us/step - loss: 4.9508 - val_loss: 4.9536\n","Epoch 241/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9509 - val_loss: 4.9536\n","Epoch 242/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9509 - val_loss: 4.9538\n","Epoch 243/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9509 - val_loss: 4.9537\n","Epoch 244/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9508 - val_loss: 4.9538\n","Epoch 245/250\n","5423/5423 [==============================] - 2s 308us/step - loss: 4.9508 - val_loss: 4.9537\n","Epoch 246/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9508 - val_loss: 4.9537\n","Epoch 247/250\n","5423/5423 [==============================] - 2s 309us/step - loss: 4.9509 - val_loss: 4.9539\n","Epoch 248/250\n","5423/5423 [==============================] - 2s 306us/step - loss: 4.9508 - val_loss: 4.9536\n","Epoch 249/250\n","5423/5423 [==============================] - 2s 311us/step - loss: 4.9509 - val_loss: 4.9537\n","Epoch 250/250\n","5423/5423 [==============================] - 2s 307us/step - loss: 4.9508 - val_loss: 4.9539\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6026, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4724384 0.0\n","The shape of N (6026, 784)\n","The minimum value of N  -0.99913824\n","The max value of N 0.9982701\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9963073741737749\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9963187360993941, 0.9878171752548622, 0.9804290831210073, 0.996548815093182, 0.9977361363203836, 0.9967874155311842, 0.99241307416781, 0.993151599333055, 0.9872689623437381, 0.9963073741737749]\n","AUROC ===== 0.9924778371438391 +/- 0.005354415611260171\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7f3818cc3e10>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"eC1Lp2myJ_O7","colab_type":"text"},"cell_type":"markdown","source":["# **DCAE-MNIST 0_Vs_all** "]},{"metadata":{"id":"n7OiP7u8JuSL","colab_type":"code","outputId":"e49f7804-0c91-4e8b-a20e-9d2754e007c2","executionInfo":{"status":"ok","timestamp":1541310890501,"user_tz":-660,"elapsed":4253691,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":99917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 6s 1ms/step - loss: 5.0645 - val_loss: 5.0565\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9925 - val_loss: 4.9925\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9762 - val_loss: 4.9837\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9695 - val_loss: 4.9771\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9663 - val_loss: 4.9749\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9646 - val_loss: 4.9706\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9633 - val_loss: 4.9686\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9624 - val_loss: 4.9690\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9615 - val_loss: 4.9669\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9608 - val_loss: 4.9672\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9602 - val_loss: 4.9676\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9598 - val_loss: 4.9665\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9595 - val_loss: 4.9656\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9589 - val_loss: 4.9642\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9584 - val_loss: 4.9647\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9585 - val_loss: 4.9644\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9582 - val_loss: 4.9643\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9581 - val_loss: 4.9648\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9577 - val_loss: 4.9647\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9574 - val_loss: 4.9648\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9573 - val_loss: 4.9640\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9630\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9623\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9566 - val_loss: 4.9624\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9564 - val_loss: 4.9640\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9623\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9615\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9617\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9560 - val_loss: 4.9614\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9560 - val_loss: 4.9609\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9613\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9611\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9555 - val_loss: 4.9614\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9603\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9604\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9600\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9596\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9592\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9600\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9598\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9599\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9595\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9587\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9587\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9664\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9578\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9578\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9573\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9578\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9576\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9577\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9612\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9574\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9574\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9576\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9575\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9777\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9573\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9574\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684398 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.999816\n","The max value of N 0.999747\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9992637031434433\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 4s 787us/step - loss: 5.0735 - val_loss: 5.1148\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9969 - val_loss: 5.0000\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9799 - val_loss: 4.9858\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9721 - val_loss: 4.9781\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9685 - val_loss: 4.9738\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9664 - val_loss: 4.9726\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9646 - val_loss: 4.9702\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9639 - val_loss: 4.9897\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9629 - val_loss: 4.9708\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9623 - val_loss: 4.9688\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9614 - val_loss: 4.9677\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9611 - val_loss: 4.9685\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9607 - val_loss: 4.9680\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9606 - val_loss: 4.9693\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9599 - val_loss: 4.9684\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9599 - val_loss: 4.9659\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9591 - val_loss: 4.9658\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9592 - val_loss: 4.9701\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9588 - val_loss: 4.9664\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9585 - val_loss: 4.9684\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9581 - val_loss: 4.9651\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9580 - val_loss: 4.9659\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9577 - val_loss: 4.9657\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9578 - val_loss: 4.9633\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9575 - val_loss: 4.9626\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9575 - val_loss: 4.9632\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9574 - val_loss: 4.9626\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9572 - val_loss: 4.9614\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 288us/step - loss: 4.9569 - val_loss: 4.9620\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9622\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9566 - val_loss: 4.9609\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9567 - val_loss: 4.9613\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9609\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9612\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9604\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9564 - val_loss: 4.9606\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9606\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9596\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9604\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9593\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9604\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9594\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9596\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9594\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9591\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9554 - val_loss: 4.9591\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9588\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9592\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9597\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9595\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9580\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684398 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.9999374\n","The max value of N 0.9997255\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9996304190875649\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 5s 896us/step - loss: 5.0619 - val_loss: 5.0762\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9923 - val_loss: 4.9911\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9774 - val_loss: 4.9825\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9709 - val_loss: 4.9745\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9680 - val_loss: 4.9981\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9660 - val_loss: 4.9756\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9641 - val_loss: 4.9716\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9627 - val_loss: 4.9681\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9623 - val_loss: 4.9677\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9615 - val_loss: 4.9685\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9610 - val_loss: 4.9666\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9605 - val_loss: 4.9671\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9601 - val_loss: 4.9657\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9597 - val_loss: 4.9649\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9592 - val_loss: 4.9653\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9591 - val_loss: 4.9675\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9588 - val_loss: 4.9649\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9584 - val_loss: 4.9639\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9582 - val_loss: 4.9667\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9581 - val_loss: 4.9648\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9582 - val_loss: 4.9648\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9576 - val_loss: 4.9628\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9574 - val_loss: 4.9629\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9573 - val_loss: 4.9618\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9570 - val_loss: 4.9630\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9637\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9566 - val_loss: 4.9623\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9642\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9613\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9561 - val_loss: 4.9615\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9615\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9617\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9611\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9619\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9615\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9616\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9614\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9600\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9598\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9597\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9592\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9599\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9589\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9599\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9592\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9589\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9590\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9586\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9587\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9591\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9587\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9592\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9586\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9587\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9586\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9604\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9587\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9593\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9586\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9586\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9586\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9588\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9586\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9586\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9589\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9586\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9588\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9589\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9588\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9590\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9589\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9588\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9589\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9588\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9588\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9590\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9591\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9606\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9590\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9587\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9590\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9588\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9608\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9593\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9594\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9591\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9591\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9603\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9591\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9591\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9608\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9594\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9589\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9589\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9593\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9595\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9591\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9592\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9588\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9587\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9586\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9585\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9584\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9585\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9587\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9586\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9584\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9586\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684390 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.99999225\n","The max value of N 0.9996303\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9992121337138011\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 6s 1ms/step - loss: 5.0522 - val_loss: 5.1009\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9890 - val_loss: 4.9951\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9772 - val_loss: 4.9814\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9716 - val_loss: 4.9779\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9673 - val_loss: 4.9750\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9675 - val_loss: 4.9906\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9677 - val_loss: 4.9763\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9631 - val_loss: 4.9714\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9618 - val_loss: 4.9698\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9618 - val_loss: 4.9665\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9625 - val_loss: 4.9741\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9609 - val_loss: 4.9669\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9608 - val_loss: 4.9681\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9599 - val_loss: 4.9668\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9601 - val_loss: 4.9707\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9592 - val_loss: 4.9654\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9590 - val_loss: 4.9641\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9585 - val_loss: 4.9643\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9580 - val_loss: 4.9632\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9579 - val_loss: 4.9643\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9585 - val_loss: 4.9682\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9590 - val_loss: 4.9928\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9582 - val_loss: 4.9663\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9578 - val_loss: 4.9641\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9643\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9569 - val_loss: 4.9630\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9571 - val_loss: 4.9618\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9613\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9568 - val_loss: 4.9622\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9631\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9618\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9563 - val_loss: 4.9602\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9599\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9591\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9597\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9597\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9557 - val_loss: 4.9595\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 290us/step - loss: 4.9555 - val_loss: 4.9596\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9591\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9615\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9626\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9728\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9554 - val_loss: 4.9662\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9612\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9612\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9593\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9626 - val_loss: 5.0351\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9598 - val_loss: 5.0009\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9779\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9566 - val_loss: 4.9687\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9564 - val_loss: 4.9664\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9613\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9604\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9571 - val_loss: 4.9628\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9591\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9621\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9593\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9588\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9592\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9602\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9638\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9618\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9597\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9629\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9593\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9587\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9576\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9575\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9574\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9572\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684398 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.99842936\n","The max value of N 0.9993243\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9988826623577542\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 6s 1ms/step - loss: 5.0608 - val_loss: 5.1276\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9927 - val_loss: 5.0018\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9777 - val_loss: 4.9842\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9709 - val_loss: 4.9790\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9673 - val_loss: 4.9783\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9653 - val_loss: 4.9743\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9636 - val_loss: 4.9772\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9631 - val_loss: 4.9711\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9624 - val_loss: 4.9685\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9612 - val_loss: 4.9703\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9609 - val_loss: 4.9671\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9606 - val_loss: 4.9685\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9599 - val_loss: 4.9687\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9596 - val_loss: 4.9676\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9593 - val_loss: 4.9671\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9592 - val_loss: 4.9676\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9588 - val_loss: 4.9677\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9585 - val_loss: 4.9653\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9582 - val_loss: 4.9641\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9578 - val_loss: 4.9644\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9656\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9576 - val_loss: 4.9635\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9575 - val_loss: 4.9643\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9572 - val_loss: 4.9645\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9572 - val_loss: 4.9636\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9570 - val_loss: 4.9626\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9627\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9570 - val_loss: 4.9637\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9567 - val_loss: 4.9621\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9565 - val_loss: 4.9624\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9563 - val_loss: 4.9616\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9564 - val_loss: 4.9613\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9622\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9563 - val_loss: 4.9623\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9610\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9612\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9606\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9605\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9606\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9629\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9598\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9604\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9600\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9595\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9592\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9599\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9592\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9631\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9589\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9592\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9587\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9589\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9589\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9582\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9584\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9587\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9585\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9651\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9598\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9596\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9582\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9579\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684399 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.9999168\n","The max value of N 0.9998592\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9994040865908025\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 7s 1ms/step - loss: 5.0678 - val_loss: 5.1910\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9936 - val_loss: 5.0019\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9784 - val_loss: 4.9809\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9707 - val_loss: 4.9768\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9671 - val_loss: 4.9718\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9648 - val_loss: 4.9779\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9636 - val_loss: 4.9719\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9621 - val_loss: 4.9736\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9613 - val_loss: 4.9693\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9606 - val_loss: 4.9664\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9607 - val_loss: 4.9692\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9604 - val_loss: 4.9681\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9595 - val_loss: 4.9676\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9592 - val_loss: 4.9657\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9587 - val_loss: 4.9641\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9583 - val_loss: 4.9640\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9578 - val_loss: 4.9621\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9579 - val_loss: 4.9631\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9576 - val_loss: 4.9638\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9577 - val_loss: 4.9643\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9634\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9630\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9567 - val_loss: 4.9626\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9664\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9564 - val_loss: 4.9631\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9627\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9611\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9563 - val_loss: 4.9606\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9613\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9601\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9568 - val_loss: 4.9789\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9722\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9575 - val_loss: 4.9854\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9567 - val_loss: 4.9727\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9656\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9620\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9556 - val_loss: 4.9602\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9604\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9595\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9606\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9561\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9565\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9560\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9562\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9561\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9561\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9562\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9564\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9563\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9569\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9563\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9521 - val_loss: 4.9567\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9562\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9519 - val_loss: 4.9567\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9565\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9566\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9520 - val_loss: 4.9567\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9577\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9566\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9520 - val_loss: 4.9563\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9520 - val_loss: 4.9564\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9520 - val_loss: 4.9572\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9566\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9519 - val_loss: 4.9566\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9565\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9519 - val_loss: 4.9568\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9519 - val_loss: 4.9578\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9563\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9518 - val_loss: 4.9563\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9518 - val_loss: 4.9565\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9520 - val_loss: 4.9570\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9565\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9519 - val_loss: 4.9564\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9518 - val_loss: 4.9564\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9566\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9517 - val_loss: 4.9567\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9519 - val_loss: 4.9568\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9572\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9518 - val_loss: 4.9570\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9518 - val_loss: 4.9568\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9518 - val_loss: 4.9569\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9519 - val_loss: 4.9572\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9518 - val_loss: 4.9567\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684391 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9996497\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9994929006085193\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 7s 1ms/step - loss: 5.0568 - val_loss: 5.1051\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9908 - val_loss: 4.9996\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9784 - val_loss: 4.9830\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9727 - val_loss: 4.9839\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9683 - val_loss: 4.9768\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9682 - val_loss: 4.9905\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9665 - val_loss: 4.9745\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9644 - val_loss: 4.9721\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9635 - val_loss: 4.9701\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9631 - val_loss: 4.9705\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9623 - val_loss: 4.9673\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9626 - val_loss: 4.9693\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9617 - val_loss: 4.9712\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9612 - val_loss: 4.9694\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9602 - val_loss: 4.9686\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9611 - val_loss: 4.9693\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9605 - val_loss: 4.9686\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9604 - val_loss: 4.9814\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9601 - val_loss: 4.9697\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9622 - val_loss: 4.9864\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9617 - val_loss: 4.9770\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9597 - val_loss: 4.9673\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9592 - val_loss: 4.9657\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9587 - val_loss: 4.9657\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9587 - val_loss: 4.9678\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9584 - val_loss: 4.9686\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9590 - val_loss: 4.9751\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9582 - val_loss: 4.9664\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9580 - val_loss: 4.9650\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9581 - val_loss: 4.9701\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9576 - val_loss: 4.9625\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9579 - val_loss: 4.9654\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9578 - val_loss: 4.9612\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9576 - val_loss: 4.9615\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9576 - val_loss: 4.9609\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9612\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9576 - val_loss: 4.9607\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9611\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9569 - val_loss: 4.9602\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9569 - val_loss: 4.9597\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9590\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9596\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9566 - val_loss: 4.9593\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9590\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9567 - val_loss: 4.9596\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9568 - val_loss: 4.9600\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9592\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9563 - val_loss: 4.9589\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9587\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9562 - val_loss: 4.9587\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9588\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9589\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9566 - val_loss: 4.9652\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9573 - val_loss: 4.9612\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9594\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9562 - val_loss: 4.9588\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9563 - val_loss: 4.9668\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9575 - val_loss: 4.9721\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9584 - val_loss: 4.9637\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9568 - val_loss: 4.9608\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9564 - val_loss: 4.9593\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9584\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9581\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9581\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9557 - val_loss: 4.9583\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9563 - val_loss: 4.9599\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9586\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9582\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9577\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9620\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9586\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9577\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9577\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9597\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9600\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9575\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4683079 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9983125336633777\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 8s 2ms/step - loss: 5.0551 - val_loss: 5.1007\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9901 - val_loss: 5.0041\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9769 - val_loss: 4.9869\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9711 - val_loss: 4.9766\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9681 - val_loss: 4.9756\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9651 - val_loss: 4.9736\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9659 - val_loss: 4.9750\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9639 - val_loss: 4.9703\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9641 - val_loss: 4.9719\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9629 - val_loss: 4.9772\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9627 - val_loss: 4.9730\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9620 - val_loss: 4.9712\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9612 - val_loss: 4.9716\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9602 - val_loss: 4.9699\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9593 - val_loss: 4.9678\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9594 - val_loss: 4.9706\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9589 - val_loss: 4.9699\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9585 - val_loss: 4.9678\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9586 - val_loss: 4.9649\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9596 - val_loss: 4.9669\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9585 - val_loss: 4.9653\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9580 - val_loss: 4.9638\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9575 - val_loss: 4.9628\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9620\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9576 - val_loss: 4.9648\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9629\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9570 - val_loss: 4.9634\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9570 - val_loss: 4.9644\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9637\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9565 - val_loss: 4.9611\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9569 - val_loss: 4.9612\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9569 - val_loss: 4.9676\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9568 - val_loss: 4.9655\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9603 - val_loss: 4.9745\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9600 - val_loss: 4.9677\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9576 - val_loss: 4.9633\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9572 - val_loss: 4.9638\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9619\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9566 - val_loss: 4.9603\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9597\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9593\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9603\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9610\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9559 - val_loss: 4.9586\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9630\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9599\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9593\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9557 - val_loss: 4.9591\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9586\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9575\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9599\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9607\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9590\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9803\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9600\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9592\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9591\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9761\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9697\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9612\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9597\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9591\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9590\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9589\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9654\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9574\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4682956 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9979486826875694\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 9s 2ms/step - loss: 5.0570 - val_loss: 5.1014\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9907 - val_loss: 5.0048\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9782 - val_loss: 4.9883\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9723 - val_loss: 4.9844\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9689 - val_loss: 4.9831\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9681 - val_loss: 5.0139\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9679 - val_loss: 4.9786\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9650 - val_loss: 4.9725\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9637 - val_loss: 4.9729\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9631 - val_loss: 4.9680\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9620 - val_loss: 4.9673\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9642 - val_loss: 4.9740\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9619 - val_loss: 4.9756\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9619 - val_loss: 4.9711\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 308us/step - loss: 4.9610 - val_loss: 4.9689\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9603 - val_loss: 4.9664\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9596 - val_loss: 4.9663\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9594 - val_loss: 4.9652\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9594 - val_loss: 4.9639\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9590 - val_loss: 4.9647\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9585 - val_loss: 4.9668\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9582 - val_loss: 4.9647\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9590 - val_loss: 4.9713\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9595 - val_loss: 4.9660\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9587 - val_loss: 4.9633\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9581 - val_loss: 4.9628\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9657\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9576 - val_loss: 4.9626\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9579 - val_loss: 4.9674\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9577 - val_loss: 4.9629\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9617\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9573 - val_loss: 4.9635\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9569 - val_loss: 4.9603\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9572 - val_loss: 4.9604\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9568 - val_loss: 4.9602\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9567 - val_loss: 4.9601\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9565 - val_loss: 4.9622\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9565 - val_loss: 4.9592\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9577 - val_loss: 4.9640\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9572 - val_loss: 4.9607\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9571 - val_loss: 4.9609\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9566 - val_loss: 4.9603\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9566 - val_loss: 4.9612\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9606\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9561 - val_loss: 4.9598\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9575 - val_loss: 4.9695\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9658\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9564 - val_loss: 4.9634\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9564 - val_loss: 4.9701\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9571 - val_loss: 4.9647\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9564 - val_loss: 4.9608\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9563 - val_loss: 4.9598\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9596\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9561 - val_loss: 4.9591\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9589\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9595\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9557 - val_loss: 4.9587\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9554 - val_loss: 4.9586\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9593\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9587\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9600\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9600\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9646\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9606\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9665\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9598\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9592\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9606\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9614\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9597\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9577\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684383 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.9993882\n","The max value of N 0.99999905\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9987737935618431\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  0\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4986, 28, 28, 1)\n","Train Label Shape:  (4986,)\n","Validation Data Shape:  (997, 28, 28, 1)\n","Validation Label Shape:  (997,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5975, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5916, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5377 samples, validate on 598 samples\n","Epoch 1/250\n","5377/5377 [==============================] - 10s 2ms/step - loss: 5.0653 - val_loss: 5.1158\n","Epoch 2/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9926 - val_loss: 4.9984\n","Epoch 3/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9777 - val_loss: 4.9851\n","Epoch 4/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9710 - val_loss: 4.9777\n","Epoch 5/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9675 - val_loss: 4.9739\n","Epoch 6/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9649 - val_loss: 4.9726\n","Epoch 7/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9638 - val_loss: 4.9732\n","Epoch 8/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9631 - val_loss: 4.9705\n","Epoch 9/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9619 - val_loss: 4.9743\n","Epoch 10/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9622 - val_loss: 4.9684\n","Epoch 11/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9616 - val_loss: 4.9720\n","Epoch 12/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9609 - val_loss: 4.9727\n","Epoch 13/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9602 - val_loss: 4.9669\n","Epoch 14/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9603 - val_loss: 4.9720\n","Epoch 15/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9599 - val_loss: 4.9689\n","Epoch 16/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9593 - val_loss: 4.9665\n","Epoch 17/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9592 - val_loss: 4.9684\n","Epoch 18/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9585 - val_loss: 4.9650\n","Epoch 19/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9582 - val_loss: 4.9650\n","Epoch 20/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9577 - val_loss: 4.9637\n","Epoch 21/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9637\n","Epoch 22/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9576 - val_loss: 4.9623\n","Epoch 23/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9586 - val_loss: 5.0650\n","Epoch 24/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9595 - val_loss: 5.0058\n","Epoch 25/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9583 - val_loss: 4.9793\n","Epoch 26/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9591 - val_loss: 4.9682\n","Epoch 27/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9643\n","Epoch 28/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9589 - val_loss: 4.9759\n","Epoch 29/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9590 - val_loss: 4.9757\n","Epoch 30/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9579 - val_loss: 4.9673\n","Epoch 31/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9573 - val_loss: 4.9662\n","Epoch 32/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9572 - val_loss: 4.9637\n","Epoch 33/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9569 - val_loss: 4.9615\n","Epoch 34/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9567 - val_loss: 4.9616\n","Epoch 35/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9571 - val_loss: 4.9627\n","Epoch 36/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9570 - val_loss: 4.9616\n","Epoch 37/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9567 - val_loss: 4.9641\n","Epoch 38/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9563 - val_loss: 4.9600\n","Epoch 39/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 40/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9562 - val_loss: 4.9594\n","Epoch 41/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9560 - val_loss: 4.9593\n","Epoch 42/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9561 - val_loss: 4.9599\n","Epoch 43/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9594\n","Epoch 44/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 45/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9595\n","Epoch 46/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9594\n","Epoch 47/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 48/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9614\n","Epoch 49/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9555 - val_loss: 4.9596\n","Epoch 50/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 51/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 52/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 53/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9588\n","Epoch 54/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9586\n","Epoch 55/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 56/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 57/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 58/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 59/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9562 - val_loss: 4.9792\n","Epoch 60/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9564 - val_loss: 4.9670\n","Epoch 61/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9558 - val_loss: 4.9618\n","Epoch 62/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9600\n","Epoch 63/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9614\n","Epoch 64/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9600\n","Epoch 65/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9595\n","Epoch 66/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 67/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 68/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 69/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 70/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 71/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9584\n","Epoch 72/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 73/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 74/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 75/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 76/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 77/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 78/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 79/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 80/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 81/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 82/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 83/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 84/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9627\n","Epoch 85/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9594\n","Epoch 86/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 87/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 88/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 89/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 90/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 91/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 92/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 93/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 94/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 95/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 96/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 97/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 98/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 99/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 100/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 101/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 102/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 103/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 104/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 105/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 106/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 107/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 108/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 109/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 110/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 111/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 112/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 113/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 114/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 115/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 116/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 117/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 118/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 119/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 120/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 121/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 122/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 123/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 124/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 125/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 126/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 127/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 128/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 129/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 130/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 131/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 132/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 133/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 134/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 135/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 136/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 137/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 138/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 139/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 140/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 141/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 142/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 143/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 144/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 145/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 146/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 147/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 148/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 149/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 150/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 151/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 152/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9588\n","Epoch 153/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 154/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 155/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 156/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 157/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 158/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 159/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 160/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 161/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 162/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 163/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 164/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 165/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 166/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 167/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 168/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 169/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 170/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 171/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 172/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 173/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 174/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9600\n","Epoch 175/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 176/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 177/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 178/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 179/250\n","5377/5377 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 180/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9685\n","Epoch 181/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9701\n","Epoch 182/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9602\n","Epoch 183/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9589\n","Epoch 184/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 185/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 186/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 187/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 188/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 189/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 190/250\n","5377/5377 [==============================] - 2s 312us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 191/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 192/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 193/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 194/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 195/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 196/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 197/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 198/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 199/250\n","5377/5377 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 200/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 201/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 202/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 203/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 204/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 205/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 206/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 207/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 208/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 209/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 210/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 211/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 212/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 213/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 214/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 215/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 216/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 217/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 218/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 219/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 220/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 221/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 222/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 223/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 224/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 225/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 226/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 227/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 228/250\n","5377/5377 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 229/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 230/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 231/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 232/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 233/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 234/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 235/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 236/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9581\n","Epoch 237/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9676\n","Epoch 238/250\n","5377/5377 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9582\n","Epoch 239/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 240/250\n","5377/5377 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 241/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 242/250\n","5377/5377 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 243/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 244/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 245/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9584\n","Epoch 246/250\n","5377/5377 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 247/250\n","5377/5377 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 248/250\n","5377/5377 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 249/250\n","5377/5377 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 250/250\n","5377/5377 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9579\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5975, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4684362 0.0\n","The shape of N (5975, 784)\n","The minimum value of N  -0.9999931\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9995387401015345\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9992637031434433, 0.9996304190875649, 0.9992121337138011, 0.9988826623577542, 0.9994040865908025, 0.9994929006085193, 0.9983125336633777, 0.9979486826875694, 0.9987737935618431, 0.9995387401015345]\n","AUROC ===== 0.9990459655516212 +/- 0.0005319899789567341\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n4KRgZmaFvwH03gOn9cFRAAAAAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7f89770fcdd8>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"ce4jNl9iXmhX","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 7_Vs_all** "]},{"metadata":{"id":"d7MS-COEWoIq","colab_type":"code","colab":{}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[]},{"metadata":{"id":"muKsh23999sd","colab_type":"text"},"cell_type":"markdown","source":["# **DCAE-MNIST 8_Vs_all**"]},{"metadata":{"id":"5TM8nMK6-MXU","colab_type":"code","outputId":"dadf79f2-24b3-4c4a-be58-3691843341b1","executionInfo":{"status":"ok","timestamp":1541316765268,"user_tz":-660,"elapsed":4212597,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":99917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 5s 880us/step - loss: 5.0639 - val_loss: 5.1596\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9996 - val_loss: 5.0098\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9880 - val_loss: 4.9925\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9807 - val_loss: 4.9841\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9753 - val_loss: 4.9809\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9723 - val_loss: 4.9793\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9695 - val_loss: 4.9767\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9697 - val_loss: 5.0175\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9681 - val_loss: 4.9775\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9655 - val_loss: 4.9735\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9641 - val_loss: 4.9724\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9638 - val_loss: 4.9742\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9626 - val_loss: 4.9729\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9614 - val_loss: 4.9700\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9613 - val_loss: 4.9701\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9604 - val_loss: 4.9656\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9601 - val_loss: 4.9653\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9599 - val_loss: 4.9661\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9593 - val_loss: 4.9653\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9592 - val_loss: 4.9654\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9589 - val_loss: 4.9646\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9594 - val_loss: 4.9652\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9588 - val_loss: 4.9650\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9583 - val_loss: 4.9639\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9579 - val_loss: 4.9643\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9575 - val_loss: 4.9630\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9576 - val_loss: 4.9634\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9638\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9572 - val_loss: 4.9624\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9586 - val_loss: 5.0331\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9587 - val_loss: 4.9785\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9578 - val_loss: 4.9650\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9575 - val_loss: 4.9632\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9571 - val_loss: 4.9628\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9574 - val_loss: 4.9641\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9629 - val_loss: 4.9864\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9592 - val_loss: 4.9654\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9580 - val_loss: 4.9631\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9573 - val_loss: 4.9612\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9573 - val_loss: 4.9599\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9925\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9738\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9654\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9564 - val_loss: 4.9627\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9634\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9607\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9572 - val_loss: 4.9833\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9630 - val_loss: 5.0028\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9583 - val_loss: 4.9742\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9668\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9571 - val_loss: 4.9633\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9567 - val_loss: 4.9604\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9568 - val_loss: 4.9645\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9566 - val_loss: 4.9607\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9592\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9562 - val_loss: 4.9586\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9585\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9663\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9602\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9555 - val_loss: 4.9584\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9594\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9595\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9630\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9565 - val_loss: 4.9726\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9625\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9594\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9580\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9578\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631851 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -0.9996061\n","The max value of N 0.9995527\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9567698202180961\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 4s 783us/step - loss: 5.0642 - val_loss: 5.2132\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 5.0014 - val_loss: 5.0218\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9893 - val_loss: 5.0009\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9819 - val_loss: 4.9896\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9762 - val_loss: 4.9806\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9729 - val_loss: 4.9779\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9705 - val_loss: 4.9746\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9685 - val_loss: 4.9763\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9672 - val_loss: 4.9794\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9655 - val_loss: 4.9733\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9644 - val_loss: 4.9709\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9644 - val_loss: 4.9798\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9628 - val_loss: 4.9757\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9622 - val_loss: 4.9764\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9618 - val_loss: 4.9717\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9613 - val_loss: 4.9722\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9607 - val_loss: 4.9681\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9601 - val_loss: 4.9692\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9599 - val_loss: 4.9688\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9617 - val_loss: 5.0357\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9632 - val_loss: 5.0416\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9603 - val_loss: 5.0045\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9602 - val_loss: 4.9862\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9597 - val_loss: 4.9689\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9589 - val_loss: 4.9666\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9585 - val_loss: 4.9710\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9589 - val_loss: 5.0047\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9593 - val_loss: 4.9694\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9586 - val_loss: 4.9663\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9582 - val_loss: 4.9683\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9578 - val_loss: 4.9669\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9576 - val_loss: 4.9649\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9627\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9579 - val_loss: 4.9835\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9580 - val_loss: 4.9684\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9575 - val_loss: 4.9610\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9609\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9606\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9570 - val_loss: 4.9604\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9567 - val_loss: 4.9601\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9566 - val_loss: 4.9598\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9566 - val_loss: 4.9607\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9567 - val_loss: 4.9619\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9589 - val_loss: 4.9630\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9572 - val_loss: 4.9612\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9565 - val_loss: 4.9602\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9562 - val_loss: 4.9594\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9562 - val_loss: 4.9604\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9563 - val_loss: 4.9600\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9569 - val_loss: 4.9715\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9599 - val_loss: 5.0013\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9571 - val_loss: 4.9631\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9610\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9601\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9562 - val_loss: 4.9589\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9594\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9563 - val_loss: 4.9678\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9610\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9601\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9596\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9569 - val_loss: 4.9719\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9680\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9612\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9601\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 4.9588\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9664\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9623\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9633\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9592\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9666\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9601\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9593\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9626\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9619\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9595\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9591\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9587\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9592\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9633\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9592\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9591\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9589\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9586\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9586\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9594\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9589\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9586\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9587\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9592\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9587\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9591\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9596\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9594\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9587\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9588\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9595\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9589\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9587\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9876\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9683\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9619\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9599\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9608\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9595\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9590\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9583\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9619\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9604\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9599\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9592\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9594\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9588\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9588\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9593\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9593\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9590\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9585\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9590\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9592\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9633\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9609\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9593\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9584\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9586\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4630858 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9758149130562924\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 5s 905us/step - loss: 5.0604 - val_loss: 5.2484\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 5.0009 - val_loss: 5.0408\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9887 - val_loss: 4.9970\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9813 - val_loss: 4.9854\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9753 - val_loss: 4.9786\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9714 - val_loss: 4.9769\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9691 - val_loss: 4.9757\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9676 - val_loss: 4.9790\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9659 - val_loss: 4.9714\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9652 - val_loss: 4.9712\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9643 - val_loss: 4.9769\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9644 - val_loss: 4.9715\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9634 - val_loss: 4.9681\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9646 - val_loss: 4.9747\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9632 - val_loss: 4.9707\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9619 - val_loss: 4.9685\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9609 - val_loss: 4.9679\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9608 - val_loss: 4.9663\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9604 - val_loss: 4.9669\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9600 - val_loss: 4.9646\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9601 - val_loss: 4.9643\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9595 - val_loss: 4.9634\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9590 - val_loss: 4.9635\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9601 - val_loss: 4.9653\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9592 - val_loss: 4.9640\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9586 - val_loss: 4.9638\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9584 - val_loss: 4.9640\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9582 - val_loss: 4.9628\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9579 - val_loss: 4.9623\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9576 - val_loss: 4.9617\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9576 - val_loss: 4.9617\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9575 - val_loss: 4.9606\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9581 - val_loss: 4.9632\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9577 - val_loss: 4.9618\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9577 - val_loss: 4.9611\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9573 - val_loss: 4.9605\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9572 - val_loss: 4.9606\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9760\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9582 - val_loss: 4.9858\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9575 - val_loss: 4.9680\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9571 - val_loss: 4.9615\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9618\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9577 - val_loss: 4.9629\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9578 - val_loss: 4.9608\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9567 - val_loss: 4.9596\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9588\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9586\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9603\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9592\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9565 - val_loss: 4.9588\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9562 - val_loss: 4.9585\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9560 - val_loss: 4.9583\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9581\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9579\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9582\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9578\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9611\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9611 - val_loss: 4.9750\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9579 - val_loss: 4.9735\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9651\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9568 - val_loss: 4.9700\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9583 - val_loss: 4.9794\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9646\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9593\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9586\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9578\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9578\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9635\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9589\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9576\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9572\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9654\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9592\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9595 - val_loss: 4.9935\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9568 - val_loss: 4.9678\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9571\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9570\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9598\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9598\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9616\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9635\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9578\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631852 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -0.99462116\n","The max value of N 0.9989447\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9550604185086943\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 5s 1ms/step - loss: 5.0734 - val_loss: 5.1401\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 5.0019 - val_loss: 5.0111\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9881 - val_loss: 4.9917\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9800 - val_loss: 4.9834\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9748 - val_loss: 4.9802\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9718 - val_loss: 4.9768\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9686 - val_loss: 4.9727\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9665 - val_loss: 4.9717\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9659 - val_loss: 4.9777\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9645 - val_loss: 4.9730\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9631 - val_loss: 4.9694\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9621 - val_loss: 4.9688\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9615 - val_loss: 4.9679\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9613 - val_loss: 4.9682\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9604 - val_loss: 4.9671\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9599 - val_loss: 4.9664\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9602 - val_loss: 4.9867\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9610 - val_loss: 4.9705\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9594 - val_loss: 4.9699\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9588 - val_loss: 4.9653\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9586 - val_loss: 4.9643\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9583 - val_loss: 4.9634\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9588 - val_loss: 4.9686\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9583 - val_loss: 4.9625\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9576 - val_loss: 4.9617\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9576 - val_loss: 4.9627\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9588 - val_loss: 5.0628\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9609 - val_loss: 4.9793\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9591 - val_loss: 4.9715\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9585 - val_loss: 4.9633\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9578 - val_loss: 4.9621\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9574 - val_loss: 4.9606\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9584 - val_loss: 4.9628\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9578 - val_loss: 4.9607\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9573 - val_loss: 4.9611\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9581 - val_loss: 4.9603\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9572 - val_loss: 4.9607\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9568 - val_loss: 4.9603\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9592\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9568 - val_loss: 4.9589\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9564 - val_loss: 4.9590\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9587\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9587\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9591\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9587\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9586\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9570 - val_loss: 4.9627\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9599\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9564 - val_loss: 4.9592\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9552 - val_loss: 4.9591\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9654\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9600\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9615\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9626\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9590\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9593\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9594\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9574\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631868 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9990755\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9578426171529619\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 6s 1ms/step - loss: 5.0598 - val_loss: 5.1782\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 5.0006 - val_loss: 5.0276\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9887 - val_loss: 4.9902\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9809 - val_loss: 4.9809\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9761 - val_loss: 4.9816\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9724 - val_loss: 4.9778\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9695 - val_loss: 4.9755\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9676 - val_loss: 4.9743\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9656 - val_loss: 4.9745\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9661 - val_loss: 4.9753\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9651 - val_loss: 4.9756\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9636 - val_loss: 4.9696\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9629 - val_loss: 4.9694\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9620 - val_loss: 4.9681\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9615 - val_loss: 4.9688\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9611 - val_loss: 4.9663\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9605 - val_loss: 4.9660\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9606 - val_loss: 4.9662\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9599 - val_loss: 4.9658\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9601 - val_loss: 5.0083\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9610 - val_loss: 4.9683\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9607 - val_loss: 4.9703\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9601 - val_loss: 4.9648\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9599 - val_loss: 4.9652\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9588 - val_loss: 4.9631\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9585 - val_loss: 4.9675\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9584 - val_loss: 4.9632\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9583 - val_loss: 4.9627\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9582 - val_loss: 4.9614\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9642\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9617\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9572 - val_loss: 4.9607\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9572 - val_loss: 4.9607\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9577 - val_loss: 4.9620\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9605\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9572 - val_loss: 4.9611\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9571 - val_loss: 4.9604\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9567 - val_loss: 4.9595\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9597\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9594\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9564 - val_loss: 4.9595\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9592\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9576 - val_loss: 4.9630\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9606\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9592\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9561 - val_loss: 4.9586\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9608\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9591\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9563 - val_loss: 4.9601\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9591\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9594\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9593\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9651\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9604 - val_loss: 4.9803\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9574 - val_loss: 4.9687\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9658\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9556 - val_loss: 4.9616\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9773\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9596 - val_loss: 4.9848\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9692\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9629\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9559 - val_loss: 4.9604\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9574 - val_loss: 4.9780\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9570 - val_loss: 4.9685\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9564 - val_loss: 4.9604\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9572 - val_loss: 4.9821\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9681\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9652\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9559 - val_loss: 4.9614\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9592\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9593\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9609\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9599\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9580\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9570\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631861 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -0.9974366\n","The max value of N 0.99301696\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9734983790156204\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 7s 1ms/step - loss: 5.0644 - val_loss: 5.2921\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 5.0029 - val_loss: 5.0359\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9906 - val_loss: 5.0011\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9827 - val_loss: 4.9881\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9773 - val_loss: 4.9827\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9737 - val_loss: 4.9888\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9720 - val_loss: 5.0160\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9701 - val_loss: 4.9864\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9675 - val_loss: 4.9763\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9657 - val_loss: 4.9722\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9647 - val_loss: 4.9741\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9633 - val_loss: 4.9722\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9635 - val_loss: 4.9722\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9621 - val_loss: 4.9677\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9618 - val_loss: 4.9716\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9611 - val_loss: 4.9680\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9610 - val_loss: 4.9679\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9603 - val_loss: 4.9667\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9610 - val_loss: 4.9955\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9605 - val_loss: 4.9791\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9604 - val_loss: 4.9682\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9596 - val_loss: 4.9643\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9591 - val_loss: 4.9651\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9588 - val_loss: 4.9643\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9586 - val_loss: 4.9635\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9623 - val_loss: 5.1036\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9613 - val_loss: 5.0394\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9608 - val_loss: 5.0100\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9597 - val_loss: 4.9718\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9591 - val_loss: 4.9658\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9589 - val_loss: 4.9642\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9583 - val_loss: 4.9628\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9580 - val_loss: 4.9624\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9581 - val_loss: 4.9610\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9575 - val_loss: 4.9607\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9574 - val_loss: 4.9602\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9584 - val_loss: 4.9608\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9577 - val_loss: 4.9607\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9573 - val_loss: 4.9604\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9574 - val_loss: 4.9604\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9572 - val_loss: 4.9603\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9571 - val_loss: 4.9599\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9569 - val_loss: 4.9598\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9570 - val_loss: 4.9599\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9595\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9567 - val_loss: 4.9590\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9564 - val_loss: 4.9589\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9566 - val_loss: 4.9591\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9587\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9588\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9590\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9562 - val_loss: 4.9587\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9589\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9596 - val_loss: 4.9650\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9579 - val_loss: 4.9616\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9594\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9564 - val_loss: 4.9589\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9565 - val_loss: 4.9590\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9566 - val_loss: 4.9587\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9582\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9561 - val_loss: 4.9584\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9586\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9561 - val_loss: 4.9586\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9561 - val_loss: 4.9591\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9566 - val_loss: 4.9601\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9600\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9559 - val_loss: 4.9602\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9581\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9582\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9610\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9603\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9637\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9595\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9595\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9584\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9652\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9592\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9591\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9591\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9592\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9590\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9604\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9592\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9597\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9592\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9586\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 5.0095\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9655\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9605\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9592\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9587\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9606\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9609\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9581\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631776 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9992678\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9708694370763337\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 8s 1ms/step - loss: 5.0774 - val_loss: 5.1623\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 5.0059 - val_loss: 5.0291\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9915 - val_loss: 5.0016\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9836 - val_loss: 4.9860\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9781 - val_loss: 4.9849\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9738 - val_loss: 4.9783\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9704 - val_loss: 4.9761\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9685 - val_loss: 4.9730\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9666 - val_loss: 4.9730\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9660 - val_loss: 4.9739\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9645 - val_loss: 4.9696\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9633 - val_loss: 4.9691\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9626 - val_loss: 4.9689\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9620 - val_loss: 4.9677\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9614 - val_loss: 4.9684\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9609 - val_loss: 4.9696\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9605 - val_loss: 4.9681\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9600 - val_loss: 4.9672\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9598 - val_loss: 4.9675\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9595 - val_loss: 4.9641\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9592 - val_loss: 4.9643\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9590 - val_loss: 4.9652\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9586 - val_loss: 4.9623\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9605 - val_loss: 4.9654\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9591 - val_loss: 4.9621\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9584 - val_loss: 4.9623\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9582 - val_loss: 4.9618\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9586 - val_loss: 4.9612\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9578 - val_loss: 4.9611\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9579 - val_loss: 4.9601\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9578 - val_loss: 4.9606\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9577 - val_loss: 4.9603\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9575 - val_loss: 4.9625\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9571 - val_loss: 4.9599\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9570 - val_loss: 4.9621\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9571 - val_loss: 4.9611\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9615\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9595\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9567 - val_loss: 4.9592\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9567 - val_loss: 4.9595\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9590 - val_loss: 5.0222\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9654 - val_loss: 5.0329\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9609 - val_loss: 4.9990\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9590 - val_loss: 4.9789\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9585 - val_loss: 4.9725\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9579 - val_loss: 4.9655\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9576 - val_loss: 4.9650\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9576 - val_loss: 4.9654\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9572 - val_loss: 4.9610\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9570 - val_loss: 4.9601\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9594\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9592\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9568 - val_loss: 4.9591\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9574 - val_loss: 4.9598\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9568 - val_loss: 4.9605\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9565 - val_loss: 4.9589\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9564 - val_loss: 4.9594\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 308us/step - loss: 4.9562 - val_loss: 4.9585\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9584\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9584\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9582\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9582\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9559 - val_loss: 4.9581\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9588\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9722\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9583\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9557 - val_loss: 4.9583\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9601\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9587\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9594\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9785\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9584\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9601\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9589\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 5.0337\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9625\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9600\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9590\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9580\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631868 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -0.99771035\n","The max value of N 0.99200493\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9719864426760978\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 8s 2ms/step - loss: 5.0661 - val_loss: 5.1751\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 5.0024 - val_loss: 5.0075\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9892 - val_loss: 4.9940\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9824 - val_loss: 4.9949\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9773 - val_loss: 4.9808\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9733 - val_loss: 4.9782\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9707 - val_loss: 4.9737\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9690 - val_loss: 4.9810\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9670 - val_loss: 4.9719\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9667 - val_loss: 4.9757\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9656 - val_loss: 4.9771\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9656 - val_loss: 4.9729\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9643 - val_loss: 4.9712\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9635 - val_loss: 4.9665\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9627 - val_loss: 4.9675\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9616 - val_loss: 4.9653\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9610 - val_loss: 4.9657\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9608 - val_loss: 4.9773\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9609 - val_loss: 4.9679\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9601 - val_loss: 4.9646\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9598 - val_loss: 4.9647\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9594 - val_loss: 4.9642\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9591 - val_loss: 4.9632\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9589 - val_loss: 4.9626\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9601 - val_loss: 4.9674\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9592 - val_loss: 4.9645\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9586 - val_loss: 4.9625\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9582 - val_loss: 4.9619\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9579 - val_loss: 4.9619\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9615\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9575 - val_loss: 4.9613\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9606\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9573 - val_loss: 4.9616\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9581 - val_loss: 4.9779\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9573 - val_loss: 4.9632\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9571 - val_loss: 4.9621\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9571 - val_loss: 4.9603\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9601\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9635 - val_loss: 4.9742\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9592 - val_loss: 4.9685\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9589 - val_loss: 5.0011\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9584 - val_loss: 4.9649\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9574 - val_loss: 4.9605\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9575 - val_loss: 4.9686\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9572 - val_loss: 4.9605\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9569 - val_loss: 4.9597\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9566 - val_loss: 4.9592\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9585\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9564 - val_loss: 4.9587\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9563 - val_loss: 4.9586\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9561 - val_loss: 4.9586\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9561 - val_loss: 4.9588\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9582\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9572 - val_loss: 4.9727\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9587 - val_loss: 4.9677\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9569 - val_loss: 4.9610\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9564 - val_loss: 4.9586\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9582\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9581\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9570 - val_loss: 4.9799\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9565 - val_loss: 4.9612\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9594\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9588\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9582\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9580\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9558 - val_loss: 4.9615\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9563 - val_loss: 4.9590\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9633\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9549 - val_loss: 4.9595\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9624\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9605\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9591\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9606\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9587\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 310us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9578\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9593\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9706\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9587\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9583\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9579\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631838 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -0.99763405\n","The max value of N 0.99927986\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9842469790745654\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 9s 2ms/step - loss: 5.0628 - val_loss: 5.1282\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9999 - val_loss: 5.0167\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9876 - val_loss: 4.9923\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9814 - val_loss: 4.9917\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9763 - val_loss: 4.9876\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9728 - val_loss: 4.9873\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9739 - val_loss: 5.0177\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9711 - val_loss: 4.9832\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9678 - val_loss: 4.9752\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9670 - val_loss: 4.9808\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9664 - val_loss: 4.9717\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9644 - val_loss: 4.9700\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9641 - val_loss: 4.9758\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9626 - val_loss: 4.9736\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9621 - val_loss: 4.9756\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9627 - val_loss: 4.9771\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9635 - val_loss: 4.9785\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9611 - val_loss: 4.9674\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9609 - val_loss: 4.9662\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9600 - val_loss: 4.9649\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9594 - val_loss: 4.9639\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9591 - val_loss: 4.9632\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9588 - val_loss: 4.9641\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9588 - val_loss: 4.9621\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9585 - val_loss: 4.9612\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9591 - val_loss: 4.9827\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9589 - val_loss: 4.9677\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9582 - val_loss: 4.9629\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9579 - val_loss: 4.9614\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9580 - val_loss: 4.9600\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9579 - val_loss: 4.9601\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9578 - val_loss: 4.9619\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9622\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9572 - val_loss: 4.9663\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9570 - val_loss: 4.9666\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9568 - val_loss: 4.9601\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9595\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9567 - val_loss: 4.9590\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9567 - val_loss: 4.9590\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9571 - val_loss: 4.9598\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9568 - val_loss: 4.9624\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9590 - val_loss: 4.9936\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9579 - val_loss: 4.9759\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9582 - val_loss: 4.9648\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9572 - val_loss: 4.9620\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9567 - val_loss: 4.9609\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 308us/step - loss: 4.9567 - val_loss: 4.9604\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9566 - val_loss: 4.9602\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9564 - val_loss: 4.9593\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9561 - val_loss: 4.9591\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9558 - val_loss: 4.9586\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9586\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9580\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9563 - val_loss: 4.9633\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9644 - val_loss: 5.0144\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9596 - val_loss: 4.9760\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9580 - val_loss: 4.9610\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9573 - val_loss: 4.9599\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9567 - val_loss: 4.9587\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9585\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9562 - val_loss: 4.9580\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9560 - val_loss: 4.9581\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9578\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9577\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9557 - val_loss: 4.9577\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9579\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9574\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9556 - val_loss: 4.9576\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9574\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9575\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9574\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9575\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9577\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9553 - val_loss: 4.9575\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9553 - val_loss: 4.9576\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9612\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9594\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9563 - val_loss: 4.9588\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 308us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 308us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9555 - val_loss: 4.9688\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9601\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9555 - val_loss: 4.9701\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9618\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9591\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 310us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9592\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 308us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9572\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631805 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -0.99869734\n","The max value of N 0.993416\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9837606837606838\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  8\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4926, 28, 28, 1)\n","Train Label Shape:  (4926,)\n","Validation Data Shape:  (985, 28, 28, 1)\n","Validation Label Shape:  (985,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5908, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5850, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5317 samples, validate on 591 samples\n","Epoch 1/250\n","5317/5317 [==============================] - 10s 2ms/step - loss: 5.0607 - val_loss: 5.1806\n","Epoch 2/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 5.0015 - val_loss: 5.0263\n","Epoch 3/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9909 - val_loss: 5.0004\n","Epoch 4/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9834 - val_loss: 4.9860\n","Epoch 5/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9782 - val_loss: 4.9815\n","Epoch 6/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9749 - val_loss: 4.9789\n","Epoch 7/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9714 - val_loss: 4.9764\n","Epoch 8/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9687 - val_loss: 4.9755\n","Epoch 9/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9676 - val_loss: 4.9716\n","Epoch 10/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9679 - val_loss: 4.9811\n","Epoch 11/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9689 - val_loss: 4.9922\n","Epoch 12/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9666 - val_loss: 4.9798\n","Epoch 13/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9649 - val_loss: 4.9889\n","Epoch 14/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9638 - val_loss: 4.9718\n","Epoch 15/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9633 - val_loss: 4.9689\n","Epoch 16/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9622 - val_loss: 4.9664\n","Epoch 17/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9614 - val_loss: 4.9661\n","Epoch 18/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9632 - val_loss: 4.9688\n","Epoch 19/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9619 - val_loss: 4.9670\n","Epoch 20/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9611 - val_loss: 4.9668\n","Epoch 21/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9604 - val_loss: 4.9652\n","Epoch 22/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9601 - val_loss: 4.9643\n","Epoch 23/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9596 - val_loss: 4.9634\n","Epoch 24/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9596 - val_loss: 4.9637\n","Epoch 25/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9594 - val_loss: 4.9673\n","Epoch 26/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9590 - val_loss: 4.9650\n","Epoch 27/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9596 - val_loss: 4.9809\n","Epoch 28/250\n","5317/5317 [==============================] - 2s 308us/step - loss: 4.9589 - val_loss: 4.9671\n","Epoch 29/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9586 - val_loss: 4.9637\n","Epoch 30/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9589 - val_loss: 4.9639\n","Epoch 31/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9583 - val_loss: 4.9623\n","Epoch 32/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9581 - val_loss: 4.9614\n","Epoch 33/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9580 - val_loss: 4.9676\n","Epoch 34/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9581 - val_loss: 4.9667\n","Epoch 35/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9578 - val_loss: 4.9620\n","Epoch 36/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9578 - val_loss: 4.9619\n","Epoch 37/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9575 - val_loss: 4.9614\n","Epoch 38/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9572 - val_loss: 4.9611\n","Epoch 39/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9571 - val_loss: 4.9606\n","Epoch 40/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9570 - val_loss: 4.9602\n","Epoch 41/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9568 - val_loss: 4.9608\n","Epoch 42/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9568 - val_loss: 4.9611\n","Epoch 43/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9569 - val_loss: 4.9603\n","Epoch 44/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9567 - val_loss: 4.9600\n","Epoch 45/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9566 - val_loss: 4.9595\n","Epoch 46/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9576 - val_loss: 4.9629\n","Epoch 47/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9568 - val_loss: 4.9603\n","Epoch 48/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9566 - val_loss: 4.9597\n","Epoch 49/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 50/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9568 - val_loss: 4.9630\n","Epoch 51/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9596\n","Epoch 52/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9564 - val_loss: 4.9594\n","Epoch 53/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9564 - val_loss: 4.9591\n","Epoch 54/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9561 - val_loss: 4.9592\n","Epoch 55/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9560 - val_loss: 4.9593\n","Epoch 56/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 57/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 58/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9591\n","Epoch 59/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9588\n","Epoch 60/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 61/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 62/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 63/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9576 - val_loss: 5.0463\n","Epoch 64/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9616 - val_loss: 5.0089\n","Epoch 65/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9587 - val_loss: 4.9943\n","Epoch 66/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9578 - val_loss: 4.9694\n","Epoch 67/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9605 - val_loss: 4.9760\n","Epoch 68/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9580 - val_loss: 4.9651\n","Epoch 69/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9570 - val_loss: 4.9608\n","Epoch 70/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9568 - val_loss: 4.9597\n","Epoch 71/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9567 - val_loss: 4.9588\n","Epoch 72/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9563 - val_loss: 4.9587\n","Epoch 73/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9564 - val_loss: 4.9604\n","Epoch 74/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9563 - val_loss: 4.9589\n","Epoch 75/250\n","5317/5317 [==============================] - 2s 309us/step - loss: 4.9562 - val_loss: 4.9583\n","Epoch 76/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9561 - val_loss: 4.9581\n","Epoch 77/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9558 - val_loss: 4.9582\n","Epoch 78/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9580\n","Epoch 79/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9556 - val_loss: 4.9577\n","Epoch 80/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9556 - val_loss: 4.9579\n","Epoch 81/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9580\n","Epoch 82/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 83/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 84/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 85/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9556 - val_loss: 4.9763\n","Epoch 86/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9622\n","Epoch 87/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9597\n","Epoch 88/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9588\n","Epoch 89/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 90/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 91/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9622\n","Epoch 92/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9598\n","Epoch 93/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 94/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 95/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 96/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 97/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 98/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 99/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 100/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9621\n","Epoch 101/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 102/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 103/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 104/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 105/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9554 - val_loss: 4.9588\n","Epoch 106/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 107/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 108/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 109/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 110/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 111/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 112/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 113/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 114/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 115/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 116/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 117/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 118/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 119/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 120/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 121/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9579\n","Epoch 122/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 123/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 124/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 125/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 126/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 127/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 128/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 129/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 130/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 131/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 132/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 133/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9544 - val_loss: 4.9594\n","Epoch 134/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 135/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 136/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 137/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 138/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 139/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 140/250\n","5317/5317 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 141/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 142/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 143/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 144/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 145/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 146/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 147/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 148/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 149/250\n","5317/5317 [==============================] - 2s 311us/step - loss: 4.9554 - val_loss: 4.9677\n","Epoch 150/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9551 - val_loss: 4.9592\n","Epoch 151/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 152/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 153/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9602\n","Epoch 154/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 155/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 156/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 157/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 158/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 159/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 160/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 161/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 162/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 163/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9591\n","Epoch 164/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 165/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 166/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 167/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 168/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 169/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9606\n","Epoch 170/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 171/250\n","5317/5317 [==============================] - 2s 309us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 172/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9610\n","Epoch 173/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9593\n","Epoch 174/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9587\n","Epoch 175/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9609\n","Epoch 176/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 177/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 178/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 179/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 180/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 181/250\n","5317/5317 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 182/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 183/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 184/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 185/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 186/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 187/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 188/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 189/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 190/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 191/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 192/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 193/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 194/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 195/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 196/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 197/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 198/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 199/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 200/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 201/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 202/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 203/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 204/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 205/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 206/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 207/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 208/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 209/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 210/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 211/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 212/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 213/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 214/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 215/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 216/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 217/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 218/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 219/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 220/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 221/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 222/250\n","5317/5317 [==============================] - 2s 307us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 223/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 224/250\n","5317/5317 [==============================] - 2s 312us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 225/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 226/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9590\n","Epoch 227/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 228/250\n","5317/5317 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 229/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 230/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 231/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 232/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 233/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 234/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 235/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 236/250\n","5317/5317 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 237/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 238/250\n","5317/5317 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 239/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 240/250\n","5317/5317 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 241/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 242/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 243/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 244/250\n","5317/5317 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 245/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 246/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 247/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 248/250\n","5317/5317 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 249/250\n","5317/5317 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 250/250\n","5317/5317 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9573\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5908, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4631829 0.0\n","The shape of N (5908, 784)\n","The minimum value of N  -0.9976369\n","The max value of N 0.9972509\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9688712054229295\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9567698202180961, 0.9758149130562924, 0.9550604185086943, 0.9578426171529619, 0.9734983790156204, 0.9708694370763337, 0.9719864426760978, 0.9842469790745654, 0.9837606837606838, 0.9688712054229295]\n","AUROC ===== 0.9698720895962276 +/- 0.0099510193025969\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmb9AASU6AAAAAElFTkSuQmCC\n","text/plain":["<matplotlib.figure.Figure at 0x7fc241522ac8>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"RG1WecQr-DUb","colab_type":"text"},"cell_type":"markdown","source":["# **DCAE-MNIST 6_Vs_all**"]},{"metadata":{"id":"An25LRa6-ONd","colab_type":"code","outputId":"1aa7402a-bd21-448f-8686-bb796d4de9e0","executionInfo":{"status":"ok","timestamp":1541329909933,"user_tz":-660,"elapsed":2658807,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":65917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 5s 901us/step - loss: 5.0708 - val_loss: 5.1427\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9957 - val_loss: 5.0116\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9823 - val_loss: 4.9871\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9760 - val_loss: 4.9816\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9715 - val_loss: 4.9783\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9679 - val_loss: 4.9766\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9656 - val_loss: 4.9735\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9635 - val_loss: 4.9713\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9624 - val_loss: 4.9700\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9612 - val_loss: 4.9701\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9609 - val_loss: 4.9697\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9599 - val_loss: 4.9672\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9595 - val_loss: 4.9658\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9590 - val_loss: 4.9717\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9587 - val_loss: 4.9660\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9580 - val_loss: 4.9648\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9580 - val_loss: 4.9644\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9575 - val_loss: 4.9640\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9647\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9571 - val_loss: 4.9644\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9642\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9568 - val_loss: 4.9643\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9633\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9637\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9563 - val_loss: 4.9633\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9561 - val_loss: 4.9622\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9561 - val_loss: 4.9623\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9558 - val_loss: 4.9616\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9677\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9629\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9662\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9629\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9557 - val_loss: 4.9614\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9606\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9611\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9551 - val_loss: 4.9604\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9549 - val_loss: 4.9600\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9668\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9613\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9603\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9594\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9591\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9600\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9589\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9590\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9593\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9590\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9587\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9591\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9590\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9594\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9585\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9616\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9585\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9591\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9598\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9586\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9759\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9628\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9592\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9585\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9582\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9581\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 5.0547\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9734\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9628\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9596\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9583\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9590\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9599\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9584\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9579\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9572\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9572\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9571\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663118 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -0.9999999\n","The max value of N 0.9999798\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9993443006849716\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 4s 799us/step - loss: 5.0657 - val_loss: 5.1510\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9927 - val_loss: 5.0154\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9803 - val_loss: 4.9920\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9750 - val_loss: 4.9829\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9706 - val_loss: 4.9785\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9677 - val_loss: 4.9757\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9657 - val_loss: 4.9756\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9640 - val_loss: 4.9739\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9627 - val_loss: 4.9697\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9611 - val_loss: 4.9717\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9604 - val_loss: 4.9679\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9600 - val_loss: 4.9673\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9595 - val_loss: 4.9653\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9586 - val_loss: 4.9662\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9585 - val_loss: 4.9683\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9584 - val_loss: 4.9662\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9576 - val_loss: 4.9644\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9579 - val_loss: 4.9796\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9573 - val_loss: 4.9703\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9583 - val_loss: 5.0014\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9578 - val_loss: 4.9765\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9693\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9671\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9565 - val_loss: 4.9628\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9563 - val_loss: 4.9614\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9623\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9609\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9606\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9618\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9601\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9614\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9617\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9602\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9600\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9596\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9564 - val_loss: 4.9684\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9649\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9620\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9602\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9593\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9599\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9657\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9600\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9594\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9591\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9767\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9642\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9915\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9564 - val_loss: 4.9819\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9693\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9636\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9609\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9594\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9594\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9588\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9591\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9591\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9578\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9583\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9587\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9575\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9582\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9577\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9682\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9589\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9576\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9654\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9655\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9716\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9619\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9580\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9570\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9570\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9578\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9568\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9569\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9522 - val_loss: 4.9570\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9570\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9520 - val_loss: 4.9568\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9571\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9610\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9703\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9584\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9574\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9569\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9568\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9570\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663165 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -0.99999905\n","The max value of N 0.99999064\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9992886833323575\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 5s 940us/step - loss: 5.0724 - val_loss: 5.1535\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9979 - val_loss: 5.0205\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9879 - val_loss: 4.9961\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9830 - val_loss: 4.9886\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9805 - val_loss: 4.9856\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9784 - val_loss: 4.9840\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9772 - val_loss: 4.9844\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9756 - val_loss: 4.9796\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9732 - val_loss: 4.9807\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9697 - val_loss: 4.9801\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9664 - val_loss: 4.9808\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9641 - val_loss: 4.9796\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9623 - val_loss: 4.9804\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9608 - val_loss: 4.9771\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9598 - val_loss: 4.9799\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9593 - val_loss: 4.9744\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9585 - val_loss: 4.9704\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9580 - val_loss: 4.9715\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9577 - val_loss: 4.9691\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9576 - val_loss: 4.9690\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9571 - val_loss: 4.9671\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9637\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9575 - val_loss: 4.9663\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9568 - val_loss: 4.9639\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9564 - val_loss: 4.9623\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9620\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9626\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9618\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9609\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9557 - val_loss: 4.9644\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9680\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9554 - val_loss: 4.9638\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9553 - val_loss: 4.9611\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9604\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9626\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9611\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9607\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9596\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9593\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9604\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9595\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9594\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9597\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9584\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9594\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9573\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9605\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9571\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9571\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9567\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9570\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9568\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4662909 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -0.99893266\n","The max value of N 0.99948263\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9995199344300686\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 6s 1ms/step - loss: 5.0560 - val_loss: 5.1208\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9930 - val_loss: 5.0116\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9812 - val_loss: 4.9916\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9751 - val_loss: 4.9828\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9707 - val_loss: 4.9765\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9677 - val_loss: 4.9723\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9656 - val_loss: 4.9730\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9637 - val_loss: 4.9988\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9620 - val_loss: 4.9733\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9612 - val_loss: 4.9701\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9603 - val_loss: 4.9678\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9596 - val_loss: 4.9674\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9588 - val_loss: 4.9653\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9589 - val_loss: 4.9760\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9590 - val_loss: 4.9716\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9688\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9685\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9646\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9640\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9567 - val_loss: 4.9630\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9565 - val_loss: 4.9624\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9611\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9559 - val_loss: 4.9623\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9615\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9627\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9611\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9601\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9617\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9597\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9614\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9745\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9818\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9620\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9600\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9596\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9594\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9591\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9587\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9554 - val_loss: 4.9631\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9616\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9583\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9587\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9580\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 5.0337\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9594 - val_loss: 5.0534\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9561 - val_loss: 4.9821\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9698\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9645\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9602\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9689\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9606\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9586\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9592\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9564\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9572\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9566\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9563\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9561\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663056 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9952886\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.99746794684152\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 6s 1ms/step - loss: 5.0661 - val_loss: 5.1636\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9951 - val_loss: 5.0127\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9838 - val_loss: 4.9946\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9784 - val_loss: 4.9890\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9747 - val_loss: 4.9865\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9716 - val_loss: 4.9799\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9693 - val_loss: 4.9781\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 293us/step - loss: 4.9672 - val_loss: 4.9765\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9657 - val_loss: 4.9759\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9642 - val_loss: 4.9728\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9627 - val_loss: 4.9742\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9617 - val_loss: 4.9698\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9608 - val_loss: 4.9697\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9601 - val_loss: 4.9678\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9598 - val_loss: 4.9687\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9592 - val_loss: 4.9667\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9590 - val_loss: 4.9678\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9582 - val_loss: 4.9660\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9579 - val_loss: 4.9653\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9575 - val_loss: 4.9641\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9572 - val_loss: 4.9627\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9651\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9570 - val_loss: 4.9642\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9567 - val_loss: 4.9621\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9564 - val_loss: 4.9606\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9646\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9577 - val_loss: 4.9862\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9576 - val_loss: 4.9761\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9724\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9567 - val_loss: 4.9698\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9769\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9752\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9562 - val_loss: 4.9635\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9618\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9847\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9566 - val_loss: 4.9659\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9629\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9571 - val_loss: 4.9624\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9599\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9595\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9591\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9620\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9594\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9598\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9581\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9625\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9588\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9682\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9644\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9599\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9587\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9594\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9574\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9729\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9603\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9590\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9571\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663068 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9967457\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9985539488320355\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 7s 1ms/step - loss: 5.0557 - val_loss: 5.1075\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9930 - val_loss: 5.0131\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9813 - val_loss: 4.9875\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9749 - val_loss: 4.9799\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9707 - val_loss: 4.9775\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9677 - val_loss: 4.9750\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9656 - val_loss: 4.9739\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9641 - val_loss: 4.9708\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9623 - val_loss: 4.9724\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9616 - val_loss: 4.9702\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9608 - val_loss: 4.9766\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9602 - val_loss: 4.9679\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9594 - val_loss: 4.9685\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9593 - val_loss: 4.9672\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9594 - val_loss: 4.9719\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9587 - val_loss: 4.9666\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9583 - val_loss: 4.9650\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9582 - val_loss: 4.9644\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9578 - val_loss: 4.9632\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9575 - val_loss: 4.9623\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9624\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9621 - val_loss: 5.0340\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9613 - val_loss: 5.0186\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9590 - val_loss: 4.9855\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9583 - val_loss: 4.9784\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9587 - val_loss: 4.9699\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9578 - val_loss: 4.9738\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9581 - val_loss: 4.9652\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9639 - val_loss: 4.9813\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9592 - val_loss: 4.9712\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9578 - val_loss: 4.9648\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9571 - val_loss: 4.9619\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9598 - val_loss: 5.0094\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9596 - val_loss: 4.9840\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9580 - val_loss: 4.9743\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9575 - val_loss: 4.9690\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9570 - val_loss: 4.9649\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9565 - val_loss: 4.9636\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9565 - val_loss: 4.9614\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9565 - val_loss: 4.9619\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9607\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9559 - val_loss: 4.9600\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9561 - val_loss: 4.9600\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9597\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9592\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9555 - val_loss: 4.9592\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9593\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9596\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9588\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9591\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9589\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9559 - val_loss: 4.9655\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9558 - val_loss: 4.9601\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9607\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9592\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9586\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9546 - val_loss: 4.9587\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9592\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9586\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9584\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9793\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9570 - val_loss: 4.9668\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9612\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9592\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9581\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9581\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9644\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9637\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9593\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9585\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9582\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9580\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9594\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9586\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9576\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9578\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9576\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663181 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9999917\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9981851179673321\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 8s 1ms/step - loss: 5.0604 - val_loss: 5.1533\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9929 - val_loss: 5.0072\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9821 - val_loss: 4.9905\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9759 - val_loss: 4.9822\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9715 - val_loss: 4.9804\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9685 - val_loss: 4.9770\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9657 - val_loss: 4.9728\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9645 - val_loss: 4.9711\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9632 - val_loss: 4.9706\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9618 - val_loss: 4.9663\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9611 - val_loss: 4.9710\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9599 - val_loss: 4.9665\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9594 - val_loss: 4.9664\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9592 - val_loss: 4.9670\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9588 - val_loss: 4.9662\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9582 - val_loss: 4.9657\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9591 - val_loss: 4.9692\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9581 - val_loss: 4.9659\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9575 - val_loss: 4.9647\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9571 - val_loss: 4.9622\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9634\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9658\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9571 - val_loss: 4.9622\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9614\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9610\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9562 - val_loss: 4.9618\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9607\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9557 - val_loss: 4.9614\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9558 - val_loss: 4.9611\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9641\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9555 - val_loss: 4.9610\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9601\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9555 - val_loss: 4.9600\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9600\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9600\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9549 - val_loss: 4.9598\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9591\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9595\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9595\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9596\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9588\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9601\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9592\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9589\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9582\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9587\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9580\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9545 - val_loss: 4.9605\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 312us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9599\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9584\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9605\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9687\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9584\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9575\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9574\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9581\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9574\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9578\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9567\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663104 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -0.98873\n","The max value of N 0.9992786\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9989871787366079\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 8s 2ms/step - loss: 5.0614 - val_loss: 5.1333\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9918 - val_loss: 5.0090\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9803 - val_loss: 4.9897\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9741 - val_loss: 4.9810\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9700 - val_loss: 4.9794\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9670 - val_loss: 4.9795\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9654 - val_loss: 4.9762\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9634 - val_loss: 4.9734\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9625 - val_loss: 4.9744\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9612 - val_loss: 4.9716\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9603 - val_loss: 4.9720\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9595 - val_loss: 4.9707\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9598 - val_loss: 4.9695\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9589 - val_loss: 4.9689\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9590 - val_loss: 4.9716\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9586 - val_loss: 4.9664\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9583 - val_loss: 4.9690\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9575 - val_loss: 4.9666\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9579 - val_loss: 4.9685\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9782\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9733\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9569 - val_loss: 4.9668\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9566 - val_loss: 4.9629\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9564 - val_loss: 4.9625\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9633\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9567 - val_loss: 4.9627\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9566 - val_loss: 4.9882\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9580 - val_loss: 4.9668\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9565 - val_loss: 4.9639\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9562 - val_loss: 4.9616\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9560 - val_loss: 4.9602\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9557 - val_loss: 4.9612\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9572 - val_loss: 4.9634\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9561 - val_loss: 4.9808\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9559 - val_loss: 4.9664\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9556 - val_loss: 4.9619\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9610\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9552 - val_loss: 4.9608\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9624\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9551 - val_loss: 4.9596\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9547 - val_loss: 4.9593\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9586\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9602\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9558 - val_loss: 4.9813\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9550 - val_loss: 4.9669\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9603\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9546 - val_loss: 4.9592\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9592\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9600\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9592\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9623\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9596\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9593\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9587\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9581\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9554 - val_loss: 4.9814\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9586 - val_loss: 5.0141\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9560 - val_loss: 4.9786\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9716\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9631\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9620\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9594\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9593\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9718\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9622\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9618\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9594\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9578\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9579\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9582\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9579\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9578\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9583\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9580\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9575\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9585\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9578\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9577\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9587\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9580\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9577\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9574\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 5.0207\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9661\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9595\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9586\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9576\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9572\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663143 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99999803\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9974708740706048\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 9s 2ms/step - loss: 5.0604 - val_loss: 5.2095\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9944 - val_loss: 5.0227\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9827 - val_loss: 4.9925\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9767 - val_loss: 4.9822\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9723 - val_loss: 4.9782\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9693 - val_loss: 4.9762\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9669 - val_loss: 4.9766\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9649 - val_loss: 4.9733\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9636 - val_loss: 4.9706\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9627 - val_loss: 4.9698\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9619 - val_loss: 4.9687\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9612 - val_loss: 4.9682\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9609 - val_loss: 4.9682\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9605 - val_loss: 4.9676\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9602 - val_loss: 4.9664\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 314us/step - loss: 4.9593 - val_loss: 4.9648\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9589 - val_loss: 4.9646\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9594 - val_loss: 4.9868\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9590 - val_loss: 4.9670\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9585 - val_loss: 4.9656\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9581 - val_loss: 4.9662\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9581 - val_loss: 4.9670\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9579 - val_loss: 4.9649\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9575 - val_loss: 4.9640\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9574 - val_loss: 4.9630\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9573 - val_loss: 4.9683\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9574 - val_loss: 4.9650\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9570 - val_loss: 4.9637\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9569 - val_loss: 4.9637\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9567 - val_loss: 4.9631\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9572 - val_loss: 4.9759\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9578 - val_loss: 4.9633\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9571 - val_loss: 4.9622\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9569 - val_loss: 4.9622\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9564 - val_loss: 4.9618\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9564 - val_loss: 4.9619\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9609\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9561 - val_loss: 4.9618\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9558 - val_loss: 4.9611\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9605\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9557 - val_loss: 4.9609\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9604\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9558 - val_loss: 4.9697\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9632\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9614\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9611\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9599\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9552 - val_loss: 4.9597\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9648\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9561 - val_loss: 4.9625\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9611\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9594\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9552 - val_loss: 4.9598\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9593\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9543 - val_loss: 4.9590\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9584\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9590\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9638\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9598\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9559 - val_loss: 4.9718\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9660\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9611\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9607\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9589\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9587\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9575 - val_loss: 4.9783\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9563 - val_loss: 4.9682\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 311us/step - loss: 4.9551 - val_loss: 4.9604\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9595\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9546 - val_loss: 4.9588\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9611\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9832\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9643\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9603\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9696\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9552 - val_loss: 4.9608\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9585\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9631\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9618\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9590\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9582\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9581\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9580\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9580\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9575\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663097 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99992514\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.994125051226509\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  6\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False  True]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True False]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4982, 28, 28, 1)\n","Train Label Shape:  (4982,)\n","Validation Data Shape:  (996, 28, 28, 1)\n","Validation Label Shape:  (996,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5948, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5890, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5353 samples, validate on 595 samples\n","Epoch 1/150\n","5353/5353 [==============================] - 10s 2ms/step - loss: 5.0702 - val_loss: 5.1512\n","Epoch 2/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9953 - val_loss: 5.0101\n","Epoch 3/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9821 - val_loss: 4.9897\n","Epoch 4/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9753 - val_loss: 4.9820\n","Epoch 5/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9710 - val_loss: 4.9749\n","Epoch 6/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9680 - val_loss: 4.9731\n","Epoch 7/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9659 - val_loss: 4.9717\n","Epoch 8/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9641 - val_loss: 4.9699\n","Epoch 9/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9628 - val_loss: 4.9682\n","Epoch 10/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9616 - val_loss: 4.9676\n","Epoch 11/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9609 - val_loss: 4.9662\n","Epoch 12/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9601 - val_loss: 4.9658\n","Epoch 13/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9596 - val_loss: 4.9646\n","Epoch 14/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9590 - val_loss: 4.9661\n","Epoch 15/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9587 - val_loss: 4.9671\n","Epoch 16/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9580 - val_loss: 4.9636\n","Epoch 17/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9579 - val_loss: 4.9635\n","Epoch 18/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9575 - val_loss: 4.9643\n","Epoch 19/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9572 - val_loss: 4.9625\n","Epoch 20/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9574 - val_loss: 4.9641\n","Epoch 21/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9572 - val_loss: 4.9632\n","Epoch 22/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9567 - val_loss: 4.9635\n","Epoch 23/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9569 - val_loss: 4.9614\n","Epoch 24/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9566 - val_loss: 4.9619\n","Epoch 25/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9565 - val_loss: 4.9613\n","Epoch 26/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9562 - val_loss: 4.9616\n","Epoch 27/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9560 - val_loss: 4.9691\n","Epoch 28/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9560 - val_loss: 4.9682\n","Epoch 29/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9567 - val_loss: 4.9681\n","Epoch 30/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9564 - val_loss: 4.9657\n","Epoch 31/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9563 - val_loss: 4.9644\n","Epoch 32/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9559 - val_loss: 4.9619\n","Epoch 33/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9556 - val_loss: 4.9610\n","Epoch 34/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9554 - val_loss: 4.9618\n","Epoch 35/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9552 - val_loss: 4.9613\n","Epoch 36/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9592\n","Epoch 37/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9550 - val_loss: 4.9597\n","Epoch 38/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 39/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 40/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9598\n","Epoch 41/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9547 - val_loss: 4.9593\n","Epoch 42/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 43/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 44/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 45/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9546 - val_loss: 4.9600\n","Epoch 46/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9545 - val_loss: 4.9952\n","Epoch 47/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9551 - val_loss: 4.9622\n","Epoch 48/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9551 - val_loss: 4.9638\n","Epoch 49/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9601\n","Epoch 50/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9562 - val_loss: 4.9622\n","Epoch 51/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9552 - val_loss: 4.9595\n","Epoch 52/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9592\n","Epoch 53/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 54/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 55/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 56/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9587\n","Epoch 57/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 58/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 59/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 60/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 61/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 62/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 63/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 64/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 65/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 66/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 67/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9591\n","Epoch 68/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 69/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 70/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 71/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 72/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 73/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9589\n","Epoch 74/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 75/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 76/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 77/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 78/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 79/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 80/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 81/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 82/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 83/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 84/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 85/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 86/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 87/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9577\n","Epoch 88/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 89/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 90/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 91/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 92/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 93/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9598\n","Epoch 94/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 95/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 96/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 97/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 98/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 99/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 100/150\n","5353/5353 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 101/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 102/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 103/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 104/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 105/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 106/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 107/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 108/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 109/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9577\n","Epoch 110/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 111/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 112/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 113/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 114/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 115/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 116/150\n","5353/5353 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 117/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 118/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 119/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9601\n","Epoch 120/150\n","5353/5353 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 121/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 122/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 123/150\n","5353/5353 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 124/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 125/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 126/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9578\n","Epoch 127/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 128/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9575\n","Epoch 129/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 130/150\n","5353/5353 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 131/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 132/150\n","5353/5353 [==============================] - 2s 311us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 133/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 134/150\n","5353/5353 [==============================] - 2s 310us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 135/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 136/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 137/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 138/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 139/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 140/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 141/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9579\n","Epoch 142/150\n","5353/5353 [==============================] - 2s 314us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 143/150\n","5353/5353 [==============================] - 2s 309us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 144/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 145/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 146/150\n","5353/5353 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 147/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 148/150\n","5353/5353 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 149/150\n","5353/5353 [==============================] - 2s 308us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 150/150\n","5353/5353 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9571\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5948, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4663030 0.0\n","The shape of N (5948, 784)\n","The minimum value of N  -0.99999964\n","The max value of N 0.99994516\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9980914466366139\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9993443006849716, 0.9992886833323575, 0.9995199344300686, 0.99746794684152, 0.9985539488320355, 0.9981851179673321, 0.9989871787366079, 0.9974708740706048, 0.994125051226509, 0.9980914466366139]\n","AUROC ===== 0.9981034482758622 +/- 0.0015024423250866894\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7fcc8d2ea400>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"P6zv6Ya8-Gyy","colab_type":"text"},"cell_type":"markdown","source":["# **DCAE-MNIST 5_Vs_all**"]},{"metadata":{"id":"tfqRYOa695b9","colab_type":"code","outputId":"e2456c7a-58ce-4688-a931-e070fd0fd1d7","executionInfo":{"status":"ok","timestamp":1541333719140,"user_tz":-660,"elapsed":2453659,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":65917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":7,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-5-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-5-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 5s 953us/step - loss: 5.0700 - val_loss: 5.1560\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 5.0032 - val_loss: 5.0197\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9881 - val_loss: 4.9988\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9796 - val_loss: 4.9871\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9744 - val_loss: 4.9799\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9712 - val_loss: 4.9768\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9690 - val_loss: 4.9769\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9660 - val_loss: 4.9747\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9644 - val_loss: 4.9718\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9632 - val_loss: 4.9721\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9626 - val_loss: 4.9705\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9615 - val_loss: 4.9718\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9613 - val_loss: 4.9717\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9621 - val_loss: 4.9694\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9607 - val_loss: 4.9664\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9609 - val_loss: 4.9679\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9603 - val_loss: 4.9654\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9595 - val_loss: 4.9661\n","Epoch 19/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9587 - val_loss: 4.9641\n","Epoch 20/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9591 - val_loss: 4.9730\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9596 - val_loss: 4.9775\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9586 - val_loss: 4.9672\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9583 - val_loss: 4.9659\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9593 - val_loss: 4.9997\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9629 - val_loss: 4.9950\n","Epoch 26/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9605 - val_loss: 4.9752\n","Epoch 27/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9591 - val_loss: 4.9663\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9582 - val_loss: 4.9636\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9576 - val_loss: 4.9633\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9574 - val_loss: 4.9613\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9594 - val_loss: 4.9895\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9674 - val_loss: 5.0103\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9623 - val_loss: 4.9819\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9590 - val_loss: 4.9713\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9581 - val_loss: 4.9664\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9579 - val_loss: 4.9634\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9575 - val_loss: 4.9614\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9573 - val_loss: 4.9601\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9573 - val_loss: 4.9601\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9568 - val_loss: 4.9596\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9565 - val_loss: 4.9594\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9563 - val_loss: 4.9588\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9563 - val_loss: 4.9589\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9559 - val_loss: 4.9586\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9560 - val_loss: 4.9585\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9564 - val_loss: 4.9651\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9562 - val_loss: 4.9602\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9577 - val_loss: 4.9613\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9565 - val_loss: 4.9588\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9559 - val_loss: 4.9584\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9552 - val_loss: 4.9950\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9567 - val_loss: 4.9754\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9556 - val_loss: 4.9652\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9551 - val_loss: 4.9620\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9548 - val_loss: 4.9604\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9603\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9591\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9586\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9531 - val_loss: 4.9570\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283704 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99607843\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9925960156089546\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 4s 854us/step - loss: 5.0641 - val_loss: 5.2088\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 5.0005 - val_loss: 5.0330\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9868 - val_loss: 4.9924\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9793 - val_loss: 4.9915\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9752 - val_loss: 4.9913\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9725 - val_loss: 4.9928\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9710 - val_loss: 4.9863\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9687 - val_loss: 4.9762\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9670 - val_loss: 4.9801\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9703 - val_loss: 4.9919\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9667 - val_loss: 4.9934\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9636 - val_loss: 4.9756\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9628 - val_loss: 4.9690\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9623 - val_loss: 4.9700\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9617 - val_loss: 4.9661\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9625 - val_loss: 4.9698\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9609 - val_loss: 4.9665\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9601 - val_loss: 4.9640\n","Epoch 19/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9597 - val_loss: 4.9634\n","Epoch 20/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9594 - val_loss: 4.9637\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9595 - val_loss: 4.9685\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9599 - val_loss: 4.9686\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9592 - val_loss: 4.9677\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9584 - val_loss: 4.9635\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9607 - val_loss: 4.9669\n","Epoch 26/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9593 - val_loss: 4.9648\n","Epoch 27/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9612 - val_loss: 4.9752\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9605 - val_loss: 4.9643\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9585 - val_loss: 4.9616\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9580 - val_loss: 4.9611\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9577 - val_loss: 4.9677\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9576 - val_loss: 4.9640\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9575 - val_loss: 4.9623\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9575 - val_loss: 4.9596\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9571 - val_loss: 4.9591\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9573 - val_loss: 4.9592\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9571 - val_loss: 4.9607\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9594 - val_loss: 4.9656\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9575 - val_loss: 4.9600\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9568 - val_loss: 4.9587\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9567 - val_loss: 4.9598\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9581 - val_loss: 4.9637\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9575 - val_loss: 4.9606\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9568 - val_loss: 4.9591\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9567 - val_loss: 4.9588\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9565 - val_loss: 4.9655\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9571 - val_loss: 4.9604\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9566 - val_loss: 4.9597\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9561 - val_loss: 4.9580\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9556 - val_loss: 4.9579\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9554 - val_loss: 4.9576\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9542 - val_loss: 4.9602\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9602 - val_loss: 4.9780\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9570 - val_loss: 4.9652\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9558 - val_loss: 4.9607\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9588\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 292us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9555 - val_loss: 4.9606\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 291us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9568\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283560 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99998635\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9780618881358253\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 5s 963us/step - loss: 5.0732 - val_loss: 5.2070\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 5.0049 - val_loss: 5.0290\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9896 - val_loss: 4.9978\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9812 - val_loss: 4.9892\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9755 - val_loss: 4.9870\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9719 - val_loss: 4.9814\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9693 - val_loss: 4.9792\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9674 - val_loss: 4.9817\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9657 - val_loss: 4.9741\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9645 - val_loss: 4.9772\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9640 - val_loss: 4.9737\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9636 - val_loss: 4.9751\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9619 - val_loss: 4.9698\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9615 - val_loss: 4.9709\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9611 - val_loss: 4.9691\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9605 - val_loss: 4.9694\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9599 - val_loss: 4.9697\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9595 - val_loss: 4.9726\n","Epoch 19/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9590 - val_loss: 4.9701\n","Epoch 20/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9601 - val_loss: 5.0040\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9598 - val_loss: 4.9899\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9587 - val_loss: 4.9783\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9583 - val_loss: 4.9741\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9582 - val_loss: 4.9700\n","Epoch 25/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9594 - val_loss: 4.9738\n","Epoch 26/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9586 - val_loss: 4.9696\n","Epoch 27/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9578 - val_loss: 4.9663\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9578 - val_loss: 4.9653\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9605 - val_loss: 4.9709\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9582 - val_loss: 4.9661\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9577 - val_loss: 4.9640\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9573 - val_loss: 4.9629\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9569 - val_loss: 4.9627\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9567 - val_loss: 4.9621\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9565 - val_loss: 4.9609\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9564 - val_loss: 4.9602\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9596 - val_loss: 4.9859\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9595 - val_loss: 4.9844\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9576 - val_loss: 4.9716\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9572 - val_loss: 4.9693\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9606 - val_loss: 4.9698\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9582 - val_loss: 4.9635\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9571 - val_loss: 4.9638\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9569 - val_loss: 4.9613\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9564 - val_loss: 4.9600\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9563 - val_loss: 4.9589\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9563 - val_loss: 4.9588\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9561 - val_loss: 4.9582\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9556 - val_loss: 4.9579\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9555 - val_loss: 4.9578\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9553 - val_loss: 4.9579\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9552 - val_loss: 4.9591\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9554 - val_loss: 4.9594\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9549 - val_loss: 4.9628\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9583\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9602\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9594\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9549 - val_loss: 4.9609\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9619\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9584\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9544 - val_loss: 4.9600\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9581\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9531 - val_loss: 4.9568\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283734 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99999994\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9943691380844801\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 6s 1ms/step - loss: 5.0748 - val_loss: 5.1476\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 5.0066 - val_loss: 5.0225\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9914 - val_loss: 4.9950\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9826 - val_loss: 4.9862\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9779 - val_loss: 4.9823\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9731 - val_loss: 4.9790\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9710 - val_loss: 4.9746\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9685 - val_loss: 4.9728\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9673 - val_loss: 4.9875\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9678 - val_loss: 4.9758\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9659 - val_loss: 4.9724\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9658 - val_loss: 4.9749\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 293us/step - loss: 4.9658 - val_loss: 4.9728\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9634 - val_loss: 4.9714\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9627 - val_loss: 4.9694\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9623 - val_loss: 4.9702\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9619 - val_loss: 4.9684\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9616 - val_loss: 4.9672\n","Epoch 19/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9613 - val_loss: 4.9648\n","Epoch 20/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9602 - val_loss: 4.9660\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9598 - val_loss: 4.9650\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9598 - val_loss: 4.9639\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9592 - val_loss: 4.9649\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9659 - val_loss: 5.0044\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9641 - val_loss: 4.9911\n","Epoch 26/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9613 - val_loss: 4.9725\n","Epoch 27/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9607 - val_loss: 4.9661\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9596 - val_loss: 4.9657\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9591 - val_loss: 4.9657\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9587 - val_loss: 4.9655\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9584 - val_loss: 4.9658\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9582 - val_loss: 4.9632\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9581 - val_loss: 4.9623\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9576 - val_loss: 4.9620\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9576 - val_loss: 4.9614\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9592 - val_loss: 4.9632\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9577 - val_loss: 4.9624\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9572 - val_loss: 4.9609\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9574 - val_loss: 4.9606\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9571 - val_loss: 4.9607\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9568 - val_loss: 4.9594\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9569 - val_loss: 4.9600\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9566 - val_loss: 4.9590\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9567 - val_loss: 4.9588\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9566 - val_loss: 4.9595\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9577 - val_loss: 4.9614\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9576 - val_loss: 4.9612\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9567 - val_loss: 4.9591\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9564 - val_loss: 4.9592\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9562 - val_loss: 4.9585\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9562 - val_loss: 4.9584\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9560 - val_loss: 4.9589\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9561 - val_loss: 4.9583\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9560 - val_loss: 4.9586\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9558 - val_loss: 4.9577\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9557 - val_loss: 4.9580\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9559 - val_loss: 4.9597\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9560 - val_loss: 4.9589\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9556 - val_loss: 4.9581\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9558 - val_loss: 4.9756\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9564 - val_loss: 4.9691\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9556 - val_loss: 4.9623\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9555 - val_loss: 4.9609\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9554 - val_loss: 4.9596\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9555 - val_loss: 4.9586\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9625\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 144/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9571\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283509 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9991656\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9947935921133704\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 6s 1ms/step - loss: 5.0676 - val_loss: 5.3919\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9997 - val_loss: 5.0614\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9864 - val_loss: 5.0083\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9786 - val_loss: 4.9850\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9787 - val_loss: 4.9899\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9725 - val_loss: 4.9864\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9693 - val_loss: 4.9830\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9677 - val_loss: 4.9748\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9670 - val_loss: 4.9748\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9693 - val_loss: 4.9811\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9667 - val_loss: 4.9725\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9638 - val_loss: 4.9669\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9632 - val_loss: 4.9700\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9633 - val_loss: 4.9673\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9620 - val_loss: 4.9657\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9629 - val_loss: 4.9744\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9623 - val_loss: 4.9722\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9614 - val_loss: 4.9673\n","Epoch 19/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9606 - val_loss: 4.9661\n","Epoch 20/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9601 - val_loss: 4.9645\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9596 - val_loss: 4.9641\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9595 - val_loss: 4.9640\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9596 - val_loss: 4.9705\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9591 - val_loss: 4.9661\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9587 - val_loss: 4.9651\n","Epoch 26/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9581 - val_loss: 4.9639\n","Epoch 27/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9581 - val_loss: 4.9640\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9575 - val_loss: 4.9620\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9576 - val_loss: 4.9624\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9581 - val_loss: 4.9606\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9575 - val_loss: 4.9610\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9581 - val_loss: 4.9636\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9573 - val_loss: 4.9611\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9568 - val_loss: 4.9599\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9576 - val_loss: 4.9655\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9578 - val_loss: 4.9649\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9572 - val_loss: 4.9628\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9565 - val_loss: 4.9609\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9568 - val_loss: 4.9720\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9600 - val_loss: 4.9689\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9576 - val_loss: 4.9623\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9572 - val_loss: 4.9608\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9565 - val_loss: 4.9592\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9564 - val_loss: 4.9598\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9558 - val_loss: 4.9587\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9561 - val_loss: 4.9583\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9562 - val_loss: 4.9589\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9565 - val_loss: 4.9579\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9558 - val_loss: 4.9582\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9556 - val_loss: 4.9580\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9562 - val_loss: 4.9586\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9556 - val_loss: 4.9592\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9554 - val_loss: 4.9600\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9552 - val_loss: 4.9585\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9552 - val_loss: 4.9576\n","Epoch 62/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9611\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9559 - val_loss: 4.9692\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9558 - val_loss: 4.9626\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9554 - val_loss: 4.9598\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9548 - val_loss: 4.9568\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9546 - val_loss: 4.9610\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9544 - val_loss: 4.9652\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9564 - val_loss: 4.9617\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9552 - val_loss: 4.9596\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9544 - val_loss: 4.9617\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9544 - val_loss: 4.9584\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9582\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9529 - val_loss: 4.9560\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283676 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9995434\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9758471965495996\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 7s 1ms/step - loss: 5.0676 - val_loss: 5.2099\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 5.0004 - val_loss: 5.0423\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9850 - val_loss: 5.0078\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9788 - val_loss: 4.9885\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9749 - val_loss: 4.9777\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9714 - val_loss: 4.9802\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9695 - val_loss: 4.9755\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9669 - val_loss: 4.9711\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9658 - val_loss: 4.9826\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9678 - val_loss: 4.9801\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9654 - val_loss: 4.9756\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9666 - val_loss: 4.9747\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9636 - val_loss: 4.9684\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9626 - val_loss: 4.9669\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9615 - val_loss: 4.9646\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9611 - val_loss: 4.9655\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9605 - val_loss: 4.9651\n","Epoch 18/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9603 - val_loss: 4.9646\n","Epoch 19/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9597 - val_loss: 4.9636\n","Epoch 20/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9595 - val_loss: 4.9635\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9593 - val_loss: 4.9629\n","Epoch 22/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9590 - val_loss: 4.9636\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9585 - val_loss: 4.9637\n","Epoch 24/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9634 - val_loss: 4.9735\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9603 - val_loss: 4.9670\n","Epoch 26/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9591 - val_loss: 4.9655\n","Epoch 27/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9586 - val_loss: 4.9663\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9580 - val_loss: 4.9641\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9575 - val_loss: 4.9634\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9575 - val_loss: 4.9625\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9576 - val_loss: 4.9611\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9572 - val_loss: 4.9614\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9571 - val_loss: 4.9604\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9569 - val_loss: 4.9599\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9569 - val_loss: 4.9602\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9566 - val_loss: 4.9596\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9564 - val_loss: 4.9617\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9564 - val_loss: 4.9604\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9565 - val_loss: 4.9634\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9565 - val_loss: 4.9606\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9562 - val_loss: 4.9598\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9560 - val_loss: 4.9589\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9558 - val_loss: 4.9585\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9564 - val_loss: 4.9636\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9572 - val_loss: 4.9594\n","Epoch 47/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9563 - val_loss: 4.9584\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9559 - val_loss: 4.9591\n","Epoch 49/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9554 - val_loss: 4.9584\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9555 - val_loss: 4.9587\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9552 - val_loss: 4.9577\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9553 - val_loss: 4.9573\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 59/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9550 - val_loss: 4.9592\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9572 - val_loss: 4.9715\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9584 - val_loss: 4.9683\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9565 - val_loss: 4.9628\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9557 - val_loss: 4.9597\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9547 - val_loss: 4.9569\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 295us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9545 - val_loss: 4.9583\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 140/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 149/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9530 - val_loss: 4.9563\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283759 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9990704\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9928698569179161\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 7s 1ms/step - loss: 5.0826 - val_loss: 5.2353\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 5.0072 - val_loss: 5.0209\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9903 - val_loss: 4.9941\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9805 - val_loss: 4.9860\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9744 - val_loss: 4.9806\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9708 - val_loss: 4.9814\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9684 - val_loss: 4.9783\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9675 - val_loss: 4.9754\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9650 - val_loss: 4.9728\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9637 - val_loss: 4.9776\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9634 - val_loss: 4.9807\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9681 - val_loss: 4.9864\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9633 - val_loss: 4.9744\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9637 - val_loss: 4.9763\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9632 - val_loss: 4.9852\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9618 - val_loss: 4.9699\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9610 - val_loss: 4.9689\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9605 - val_loss: 4.9675\n","Epoch 19/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9604 - val_loss: 4.9666\n","Epoch 20/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9593 - val_loss: 4.9639\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9586 - val_loss: 4.9629\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9586 - val_loss: 4.9626\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9609 - val_loss: 4.9734\n","Epoch 24/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9607 - val_loss: 4.9680\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9597 - val_loss: 4.9731\n","Epoch 26/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9590 - val_loss: 4.9742\n","Epoch 27/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9583 - val_loss: 4.9660\n","Epoch 28/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9577 - val_loss: 4.9630\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9573 - val_loss: 4.9622\n","Epoch 30/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9576 - val_loss: 4.9629\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9574 - val_loss: 4.9623\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9569 - val_loss: 4.9605\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9568 - val_loss: 4.9617\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9567 - val_loss: 4.9600\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9565 - val_loss: 4.9600\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9565 - val_loss: 4.9593\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9561 - val_loss: 4.9592\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9568 - val_loss: 4.9602\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9567 - val_loss: 4.9601\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9562 - val_loss: 4.9595\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9560 - val_loss: 4.9587\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9561 - val_loss: 4.9589\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9558 - val_loss: 4.9587\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9558 - val_loss: 4.9589\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9563 - val_loss: 4.9597\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9557 - val_loss: 4.9596\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9561 - val_loss: 4.9597\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9559 - val_loss: 4.9594\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9559 - val_loss: 4.9589\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9553 - val_loss: 4.9594\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9553 - val_loss: 4.9588\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 63/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 64/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9554 - val_loss: 4.9649\n","Epoch 66/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9622\n","Epoch 67/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9555 - val_loss: 4.9585\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9555 - val_loss: 4.9615\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9557 - val_loss: 4.9611\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9551 - val_loss: 4.9603\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9553 - val_loss: 4.9606\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9550 - val_loss: 4.9583\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9546 - val_loss: 4.9585\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9547 - val_loss: 4.9604\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9545 - val_loss: 4.9585\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9544 - val_loss: 4.9587\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9594\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9541 - val_loss: 4.9582\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9551 - val_loss: 4.9626\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9544 - val_loss: 4.9603\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9540 - val_loss: 4.9583\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283560 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9799411241185731\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 8s 2ms/step - loss: 5.0742 - val_loss: 5.2592\n","Epoch 2/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 5.0038 - val_loss: 5.0619\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9883 - val_loss: 4.9983\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9802 - val_loss: 4.9910\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9763 - val_loss: 4.9896\n","Epoch 6/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9726 - val_loss: 4.9833\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9697 - val_loss: 4.9785\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9675 - val_loss: 4.9734\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9666 - val_loss: 4.9772\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9657 - val_loss: 4.9785\n","Epoch 11/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9648 - val_loss: 4.9762\n","Epoch 12/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9643 - val_loss: 4.9734\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9633 - val_loss: 4.9712\n","Epoch 14/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9627 - val_loss: 4.9714\n","Epoch 15/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9620 - val_loss: 4.9691\n","Epoch 16/150\n","4917/4917 [==============================] - 2s 309us/step - loss: 4.9613 - val_loss: 4.9683\n","Epoch 17/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9608 - val_loss: 4.9675\n","Epoch 18/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9606 - val_loss: 4.9677\n","Epoch 19/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9601 - val_loss: 4.9671\n","Epoch 20/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9597 - val_loss: 4.9652\n","Epoch 21/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9596 - val_loss: 4.9646\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9592 - val_loss: 4.9651\n","Epoch 23/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9590 - val_loss: 4.9657\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9587 - val_loss: 4.9653\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9598 - val_loss: 4.9728\n","Epoch 26/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9628 - val_loss: 4.9686\n","Epoch 27/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9597 - val_loss: 4.9664\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9592 - val_loss: 4.9657\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9584 - val_loss: 4.9640\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9583 - val_loss: 4.9644\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9581 - val_loss: 4.9629\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9576 - val_loss: 4.9625\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9574 - val_loss: 4.9624\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9574 - val_loss: 4.9618\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9572 - val_loss: 4.9622\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9571 - val_loss: 4.9608\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9568 - val_loss: 4.9605\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9569 - val_loss: 4.9605\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 296us/step - loss: 4.9564 - val_loss: 4.9594\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9567 - val_loss: 4.9593\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9564 - val_loss: 4.9595\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9564 - val_loss: 4.9593\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9562 - val_loss: 4.9608\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9562 - val_loss: 4.9605\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9560 - val_loss: 4.9591\n","Epoch 47/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9559 - val_loss: 4.9584\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9560 - val_loss: 4.9597\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9560 - val_loss: 4.9601\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9560 - val_loss: 4.9596\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9557 - val_loss: 4.9584\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9555 - val_loss: 4.9590\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9559 - val_loss: 4.9588\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9563 - val_loss: 4.9689\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9561 - val_loss: 4.9632\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9557 - val_loss: 4.9660\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9561 - val_loss: 4.9710\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9558 - val_loss: 4.9631\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9556 - val_loss: 4.9600\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9553 - val_loss: 4.9594\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9571 - val_loss: 4.9606\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9559 - val_loss: 4.9593\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9554 - val_loss: 4.9586\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9552 - val_loss: 4.9593\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 82/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 91/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 98/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 101/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 103/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 107/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 108/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 109/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 110/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9546 - val_loss: 4.9574\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 120/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9541 - val_loss: 4.9578\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9540 - val_loss: 4.9583\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 148/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9536 - val_loss: 4.9572\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283625 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99607843\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9855103717395769\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 9s 2ms/step - loss: 5.0935 - val_loss: 5.2205\n","Epoch 2/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 5.0101 - val_loss: 5.0297\n","Epoch 3/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9920 - val_loss: 4.9948\n","Epoch 4/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9816 - val_loss: 4.9883\n","Epoch 5/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9752 - val_loss: 4.9874\n","Epoch 6/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9711 - val_loss: 4.9819\n","Epoch 7/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9690 - val_loss: 4.9838\n","Epoch 8/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9663 - val_loss: 4.9826\n","Epoch 9/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9650 - val_loss: 4.9791\n","Epoch 10/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9638 - val_loss: 4.9762\n","Epoch 11/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9634 - val_loss: 4.9819\n","Epoch 12/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9623 - val_loss: 4.9765\n","Epoch 13/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9626 - val_loss: 4.9760\n","Epoch 14/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9616 - val_loss: 4.9761\n","Epoch 15/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9608 - val_loss: 4.9743\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9606 - val_loss: 4.9716\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9601 - val_loss: 4.9710\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9595 - val_loss: 4.9668\n","Epoch 19/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9601 - val_loss: 4.9658\n","Epoch 20/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9598 - val_loss: 4.9663\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9592 - val_loss: 4.9655\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9586 - val_loss: 4.9648\n","Epoch 23/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9587 - val_loss: 4.9636\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9583 - val_loss: 4.9649\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9578 - val_loss: 4.9624\n","Epoch 26/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9578 - val_loss: 4.9658\n","Epoch 27/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9584 - val_loss: 4.9657\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9579 - val_loss: 4.9722\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9576 - val_loss: 4.9663\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9570 - val_loss: 4.9648\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9571 - val_loss: 4.9627\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9570 - val_loss: 4.9622\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 294us/step - loss: 4.9567 - val_loss: 4.9619\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9566 - val_loss: 4.9605\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9563 - val_loss: 4.9606\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9564 - val_loss: 4.9651\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9568 - val_loss: 4.9616\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9568 - val_loss: 4.9644\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9566 - val_loss: 4.9614\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9564 - val_loss: 4.9610\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 42/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9559 - val_loss: 4.9615\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9560 - val_loss: 4.9615\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 45/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9557 - val_loss: 4.9593\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9557 - val_loss: 4.9595\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9564 - val_loss: 4.9663\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9572 - val_loss: 4.9623\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9570 - val_loss: 4.9703\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9572 - val_loss: 4.9663\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9563 - val_loss: 4.9632\n","Epoch 55/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9558 - val_loss: 4.9601\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9560 - val_loss: 4.9626\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9555 - val_loss: 4.9607\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9554 - val_loss: 4.9597\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9550 - val_loss: 4.9603\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9549 - val_loss: 4.9593\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9548 - val_loss: 4.9587\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9546 - val_loss: 4.9580\n","Epoch 74/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9546 - val_loss: 4.9576\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 82/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 103/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 104/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 108/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 113/150\n","4917/4917 [==============================] - 1s 297us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 120/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 298us/step - loss: 4.9541 - val_loss: 4.9589\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 142/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 144/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 145/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 146/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 148/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 150/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9533 - val_loss: 4.9572\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283742 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9915828027657974\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  5\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [ True False False ...  True False False]\n","[INFO] : The idx_outlier is:  [False  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False  True False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True False  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4564, 28, 28, 1)\n","Train Label Shape:  (4564,)\n","Validation Data Shape:  (912, 28, 28, 1)\n","Validation Label Shape:  (912,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (54, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5464, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5410, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (54, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 4917 samples, validate on 547 samples\n","Epoch 1/150\n","4917/4917 [==============================] - 10s 2ms/step - loss: 5.0766 - val_loss: 5.2133\n","Epoch 2/150\n","4917/4917 [==============================] - 2s 309us/step - loss: 5.0057 - val_loss: 5.0251\n","Epoch 3/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9898 - val_loss: 4.9944\n","Epoch 4/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9814 - val_loss: 4.9916\n","Epoch 5/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9757 - val_loss: 4.9874\n","Epoch 6/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9708 - val_loss: 4.9816\n","Epoch 7/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9693 - val_loss: 4.9819\n","Epoch 8/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9669 - val_loss: 4.9811\n","Epoch 9/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9654 - val_loss: 4.9801\n","Epoch 10/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9648 - val_loss: 4.9756\n","Epoch 11/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9638 - val_loss: 4.9755\n","Epoch 12/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9624 - val_loss: 4.9743\n","Epoch 13/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9614 - val_loss: 4.9732\n","Epoch 14/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9611 - val_loss: 4.9767\n","Epoch 15/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9606 - val_loss: 4.9722\n","Epoch 16/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9598 - val_loss: 4.9694\n","Epoch 17/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9597 - val_loss: 4.9686\n","Epoch 18/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9600 - val_loss: 4.9670\n","Epoch 19/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9593 - val_loss: 4.9674\n","Epoch 20/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9588 - val_loss: 4.9656\n","Epoch 21/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9583 - val_loss: 4.9679\n","Epoch 22/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9579 - val_loss: 4.9635\n","Epoch 23/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9581 - val_loss: 4.9746\n","Epoch 24/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9578 - val_loss: 4.9672\n","Epoch 25/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9573 - val_loss: 4.9676\n","Epoch 26/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9578 - val_loss: 4.9656\n","Epoch 27/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9573 - val_loss: 4.9629\n","Epoch 28/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9571 - val_loss: 4.9637\n","Epoch 29/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9568 - val_loss: 4.9616\n","Epoch 30/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9568 - val_loss: 4.9614\n","Epoch 31/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9565 - val_loss: 4.9607\n","Epoch 32/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9564 - val_loss: 4.9603\n","Epoch 33/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9564 - val_loss: 4.9598\n","Epoch 34/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9563 - val_loss: 4.9614\n","Epoch 35/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9561 - val_loss: 4.9599\n","Epoch 36/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9559 - val_loss: 4.9594\n","Epoch 37/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9560 - val_loss: 4.9589\n","Epoch 38/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9559 - val_loss: 4.9591\n","Epoch 39/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9557 - val_loss: 4.9590\n","Epoch 40/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9555 - val_loss: 4.9579\n","Epoch 41/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 42/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 43/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 44/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 45/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 46/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9553 - val_loss: 4.9581\n","Epoch 47/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9561 - val_loss: 4.9917\n","Epoch 48/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9584 - val_loss: 4.9863\n","Epoch 49/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9569 - val_loss: 4.9785\n","Epoch 50/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9567 - val_loss: 4.9712\n","Epoch 51/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9562 - val_loss: 4.9655\n","Epoch 52/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9559 - val_loss: 4.9638\n","Epoch 53/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9556 - val_loss: 4.9599\n","Epoch 54/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9588 - val_loss: 4.9686\n","Epoch 55/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9569 - val_loss: 4.9624\n","Epoch 56/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9561 - val_loss: 4.9600\n","Epoch 57/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9557 - val_loss: 4.9581\n","Epoch 58/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9554 - val_loss: 4.9582\n","Epoch 59/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9554 - val_loss: 4.9579\n","Epoch 60/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 61/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9554 - val_loss: 4.9573\n","Epoch 62/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 63/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9551 - val_loss: 4.9573\n","Epoch 64/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9550 - val_loss: 4.9572\n","Epoch 65/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9551 - val_loss: 4.9570\n","Epoch 66/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9567 - val_loss: 4.9594\n","Epoch 67/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9581 - val_loss: 4.9691\n","Epoch 68/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9565 - val_loss: 4.9631\n","Epoch 69/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9555 - val_loss: 4.9604\n","Epoch 70/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 71/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9554 - val_loss: 4.9591\n","Epoch 72/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 73/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 74/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9576\n","Epoch 75/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 76/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9548 - val_loss: 4.9569\n","Epoch 77/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 78/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9547 - val_loss: 4.9569\n","Epoch 79/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 80/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 81/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9547 - val_loss: 4.9569\n","Epoch 82/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 83/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 84/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 85/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 86/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 87/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 88/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 89/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 90/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 91/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 92/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 93/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 94/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 95/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 96/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 97/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 98/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 99/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 100/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 101/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 102/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 103/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 104/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 105/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 106/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 107/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 108/150\n","4917/4917 [==============================] - 2s 308us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 109/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 110/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 111/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 112/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 113/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 114/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 115/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 116/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9540 - val_loss: 4.9661\n","Epoch 117/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9545 - val_loss: 4.9635\n","Epoch 118/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9540 - val_loss: 4.9607\n","Epoch 119/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 120/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 121/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 122/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 123/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 124/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 125/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 126/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 127/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 128/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 129/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 130/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 131/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 132/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 133/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 134/150\n","4917/4917 [==============================] - 1s 299us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 135/150\n","4917/4917 [==============================] - 1s 301us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 136/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 137/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 138/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 139/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 140/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 141/150\n","4917/4917 [==============================] - 1s 300us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 142/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 143/150\n","4917/4917 [==============================] - 1s 304us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 144/150\n","4917/4917 [==============================] - 2s 307us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 145/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 146/150\n","4917/4917 [==============================] - 1s 302us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 147/150\n","4917/4917 [==============================] - 1s 303us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 148/150\n","4917/4917 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 149/150\n","4917/4917 [==============================] - 1s 305us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 150/150\n","4917/4917 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9567\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5464, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4283356 0.0\n","The shape of N (5464, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99600893\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9930820839323613\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9925960156089546, 0.9780618881358253, 0.9943691380844801, 0.9947935921133704, 0.9758471965495996, 0.9928698569179161, 0.9799411241185731, 0.9855103717395769, 0.9915828027657974, 0.9930820839323613]\n","AUROC ===== 0.9878654069966455 +/- 0.006983129728750981\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEk0AAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7fe2ada0a908>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"Am2Oq9gk9xWX","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 4_Vs_all**"]},{"metadata":{"id":"1PVJpi9l9kt9","colab_type":"code","outputId":"3efb7ee6-c1b0-4f15-8099-49acc68c2aa9","executionInfo":{"status":"ok","timestamp":1541337045030,"user_tz":-660,"elapsed":2623575,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":65917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 5s 966us/step - loss: 5.0706 - val_loss: 5.4594\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9995 - val_loss: 5.0788\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9841 - val_loss: 5.0480\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9764 - val_loss: 5.0046\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9797 - val_loss: 5.0097\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9711 - val_loss: 4.9799\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9691 - val_loss: 4.9888\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9690 - val_loss: 4.9748\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9654 - val_loss: 4.9702\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9635 - val_loss: 4.9674\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9626 - val_loss: 4.9677\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9621 - val_loss: 4.9667\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9614 - val_loss: 4.9663\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9610 - val_loss: 4.9657\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9601 - val_loss: 4.9647\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9601 - val_loss: 4.9650\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9601 - val_loss: 4.9681\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9661 - val_loss: 4.9780\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9613 - val_loss: 4.9681\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9599 - val_loss: 4.9663\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9594 - val_loss: 4.9662\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9585 - val_loss: 4.9641\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9582 - val_loss: 4.9638\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9580 - val_loss: 4.9651\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9577 - val_loss: 4.9626\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9573 - val_loss: 4.9620\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9622\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9568 - val_loss: 4.9614\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9607\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9688\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9565 - val_loss: 4.9620\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9610\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9561 - val_loss: 4.9605\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9601\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9597\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9588\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9597\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9590\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9583\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9595\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9587\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9584\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9582\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9613\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9591\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9597\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 290us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9559\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632483 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9782059064468791\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 5s 848us/step - loss: 5.0684 - val_loss: 5.1660\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9960 - val_loss: 5.0118\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9816 - val_loss: 4.9873\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9749 - val_loss: 4.9765\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9701 - val_loss: 4.9748\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9670 - val_loss: 4.9721\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9652 - val_loss: 4.9705\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9644 - val_loss: 4.9685\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9634 - val_loss: 4.9692\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9620 - val_loss: 4.9656\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9609 - val_loss: 4.9650\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9614 - val_loss: 4.9687\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9608 - val_loss: 4.9710\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9596 - val_loss: 4.9715\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9589 - val_loss: 4.9686\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9587 - val_loss: 4.9683\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9583 - val_loss: 4.9647\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9579 - val_loss: 4.9665\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9575 - val_loss: 4.9646\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9583 - val_loss: 4.9620\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9582 - val_loss: 4.9671\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9571 - val_loss: 4.9630\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9566 - val_loss: 4.9608\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9603\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9580 - val_loss: 4.9651\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9576 - val_loss: 4.9615\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9605\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9563 - val_loss: 4.9619\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9558 - val_loss: 4.9602\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9597\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9582\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9602\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9571 - val_loss: 4.9674\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9606\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9572\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9567\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9606\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9596\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9566\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9778\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9596\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9561\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632638 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99953115\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.982101497533578\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 5s 949us/step - loss: 5.0734 - val_loss: 5.1951\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9983 - val_loss: 5.0149\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9830 - val_loss: 4.9952\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9759 - val_loss: 4.9820\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9710 - val_loss: 4.9788\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9678 - val_loss: 4.9739\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9654 - val_loss: 4.9734\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9640 - val_loss: 4.9875\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9633 - val_loss: 4.9716\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9622 - val_loss: 4.9715\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9619 - val_loss: 4.9687\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9608 - val_loss: 4.9723\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9602 - val_loss: 4.9681\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9595 - val_loss: 4.9677\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9589 - val_loss: 4.9668\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9585 - val_loss: 4.9654\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9596 - val_loss: 4.9679\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9587 - val_loss: 4.9673\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9580 - val_loss: 4.9637\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9578 - val_loss: 4.9653\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9573 - val_loss: 4.9638\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9632\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9568 - val_loss: 4.9630\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9564 - val_loss: 4.9628\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9564 - val_loss: 4.9641\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9567 - val_loss: 4.9631\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9590 - val_loss: 4.9997\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9574 - val_loss: 4.9722\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9571 - val_loss: 4.9653\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9565 - val_loss: 4.9617\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9568 - val_loss: 4.9617\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9606\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9596\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9591\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9586\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9640\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9582 - val_loss: 4.9628\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9600\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9590\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9711\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9638\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9553 - val_loss: 4.9600\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9580\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9580\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9591\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9592\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9581\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9608\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9564\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632480 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9994076\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.992907195351222\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 5s 1ms/step - loss: 5.0788 - val_loss: 5.3042\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 5.0017 - val_loss: 5.0311\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9858 - val_loss: 4.9914\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9781 - val_loss: 4.9833\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9730 - val_loss: 4.9759\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9698 - val_loss: 4.9768\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9667 - val_loss: 4.9763\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9648 - val_loss: 4.9726\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9631 - val_loss: 4.9700\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9622 - val_loss: 4.9684\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9612 - val_loss: 4.9672\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9602 - val_loss: 4.9672\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9594 - val_loss: 4.9668\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9590 - val_loss: 4.9652\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9586 - val_loss: 4.9648\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9586 - val_loss: 4.9652\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9578 - val_loss: 4.9631\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9581 - val_loss: 4.9647\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9573 - val_loss: 4.9635\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9643\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9571 - val_loss: 4.9633\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9566 - val_loss: 4.9708\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9566 - val_loss: 4.9617\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9565 - val_loss: 4.9634\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9615\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9620\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9557 - val_loss: 4.9614\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9616\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9558 - val_loss: 4.9604\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9598\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9604\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9613\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9609\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9608\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9602\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9593\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9597\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9585\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9742\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9685\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9658\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9646\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9613\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9598\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9611\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9591\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9583\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9608\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9579\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9604\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9591\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9645\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9590\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9581\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9565\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9564\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9565\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9562\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9562\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632638 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -0.9985002\n","The max value of N 0.99529916\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9914957065989309\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 6s 1ms/step - loss: 5.0804 - val_loss: 5.2348\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 5.0009 - val_loss: 5.0362\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9835 - val_loss: 4.9897\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9754 - val_loss: 4.9831\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9703 - val_loss: 4.9782\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9671 - val_loss: 4.9754\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9653 - val_loss: 4.9728\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9642 - val_loss: 4.9724\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9634 - val_loss: 4.9731\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9621 - val_loss: 4.9920\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9616 - val_loss: 4.9686\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9605 - val_loss: 4.9678\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9597 - val_loss: 4.9674\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9591 - val_loss: 4.9678\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9590 - val_loss: 4.9666\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9592 - val_loss: 4.9715\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9586 - val_loss: 4.9666\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9581 - val_loss: 4.9668\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9576 - val_loss: 4.9656\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9573 - val_loss: 4.9673\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9570 - val_loss: 4.9641\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9646\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9564 - val_loss: 4.9639\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9563 - val_loss: 4.9636\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9621\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9618\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9560 - val_loss: 4.9621\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9559 - val_loss: 4.9619\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9607\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9615\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9559 - val_loss: 4.9608\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9611\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9555 - val_loss: 4.9603\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9599\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9595\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9595\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9600\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9590\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9589\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9589\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9584\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9594\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9589\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9590\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9588\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9582\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9580\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9579\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9583\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9580\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9575\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9573\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9572\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9580\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9572\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9572\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9571\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9574\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9572\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9571\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632656 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -0.9976497\n","The max value of N 0.99979395\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9942008144791047\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 7s 1ms/step - loss: 5.0688 - val_loss: 5.2467\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9976 - val_loss: 5.0302\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9828 - val_loss: 4.9915\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9760 - val_loss: 4.9833\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9717 - val_loss: 4.9807\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9680 - val_loss: 4.9764\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9658 - val_loss: 4.9746\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9647 - val_loss: 4.9720\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9634 - val_loss: 4.9704\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9621 - val_loss: 4.9688\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9617 - val_loss: 4.9701\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9621 - val_loss: 4.9771\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9608 - val_loss: 4.9704\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9598 - val_loss: 4.9693\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9596 - val_loss: 4.9699\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9595 - val_loss: 4.9712\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9592 - val_loss: 4.9667\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9592 - val_loss: 4.9670\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9583 - val_loss: 4.9664\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9577 - val_loss: 4.9691\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9589 - val_loss: 4.9823\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9586 - val_loss: 4.9681\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9585 - val_loss: 4.9639\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9574 - val_loss: 4.9641\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9575 - val_loss: 4.9660\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9570 - val_loss: 4.9632\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9569 - val_loss: 4.9616\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9607\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9634\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9562 - val_loss: 4.9619\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9605\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9598\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9622\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9604\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9595\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9558 - val_loss: 4.9622\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9608\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9597\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9588\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9601\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9604\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9625\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9553 - val_loss: 4.9611\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9589\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9584\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9577\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9598\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9588 - val_loss: 4.9748\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9642\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9602\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9586\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9597\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9573\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9575\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9574\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9572\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9595\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9580\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9585\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9584\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9569\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632570 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99997556\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9905881104909859\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 7s 1ms/step - loss: 5.0775 - val_loss: 5.2484\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9988 - val_loss: 5.0248\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9834 - val_loss: 4.9894\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9758 - val_loss: 4.9795\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 295us/step - loss: 4.9716 - val_loss: 4.9782\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9682 - val_loss: 4.9740\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9657 - val_loss: 4.9715\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9640 - val_loss: 4.9693\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9630 - val_loss: 4.9679\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9620 - val_loss: 4.9709\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9612 - val_loss: 4.9667\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9607 - val_loss: 4.9734\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9612 - val_loss: 4.9697\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9604 - val_loss: 4.9658\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9592 - val_loss: 4.9666\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9591 - val_loss: 4.9648\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9591 - val_loss: 4.9652\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9585 - val_loss: 4.9649\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9581 - val_loss: 4.9640\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9580 - val_loss: 4.9631\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9586 - val_loss: 4.9714\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9576 - val_loss: 4.9699\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9637\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9575 - val_loss: 4.9673\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9568 - val_loss: 4.9627\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9570 - val_loss: 4.9654\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9624\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9563 - val_loss: 4.9614\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9607\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9560 - val_loss: 4.9602\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9601\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9604\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9593\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9598\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9603\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9586\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9579\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9599\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9577\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9578\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9583\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9588\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9583\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9574\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9579\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9572\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9575\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9534 - val_loss: 4.9578\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9568\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9569\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9591\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9566\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9566\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9568\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632500 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -0.9999938\n","The max value of N 0.99877125\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9927038702491174\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 8s 2ms/step - loss: 5.0719 - val_loss: 5.2320\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9981 - val_loss: 5.0132\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9828 - val_loss: 4.9873\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9749 - val_loss: 4.9798\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9707 - val_loss: 4.9765\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9672 - val_loss: 4.9760\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9657 - val_loss: 4.9738\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9652 - val_loss: 4.9724\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9628 - val_loss: 4.9701\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9616 - val_loss: 4.9676\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9612 - val_loss: 4.9669\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9599 - val_loss: 4.9646\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9600 - val_loss: 4.9643\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9593 - val_loss: 4.9649\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9600 - val_loss: 4.9678\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9589 - val_loss: 4.9655\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9581 - val_loss: 4.9655\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9596 - val_loss: 4.9675\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9590 - val_loss: 4.9643\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9580 - val_loss: 4.9645\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9572 - val_loss: 4.9631\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9570 - val_loss: 4.9645\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9578 - val_loss: 4.9715\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9570 - val_loss: 4.9658\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9639\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9566 - val_loss: 4.9622\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9611\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9570 - val_loss: 4.9608\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9562 - val_loss: 4.9607\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9562 - val_loss: 4.9602\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9601\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9608\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9623\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9609\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9626 - val_loss: 4.9964\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9593 - val_loss: 5.0092\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9570 - val_loss: 4.9754\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9564 - val_loss: 4.9747\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9666\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9625\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9601\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9643\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9609\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9574\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9538 - val_loss: 4.9575\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9581\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9599\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9655\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9667\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9537 - val_loss: 4.9578\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9570\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9574\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9564\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632504 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99973524\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.967270552042386\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 9s 2ms/step - loss: 5.0735 - val_loss: 5.5484\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9990 - val_loss: 5.1437\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9861 - val_loss: 5.0072\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9799 - val_loss: 4.9888\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9727 - val_loss: 4.9787\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9697 - val_loss: 4.9720\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9668 - val_loss: 4.9725\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9658 - val_loss: 4.9721\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9659 - val_loss: 4.9708\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9642 - val_loss: 4.9704\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9628 - val_loss: 4.9685\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9625 - val_loss: 4.9685\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9615 - val_loss: 4.9677\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9611 - val_loss: 4.9661\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9608 - val_loss: 4.9651\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9599 - val_loss: 4.9651\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9595 - val_loss: 4.9646\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9593 - val_loss: 4.9642\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9597 - val_loss: 4.9635\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9589 - val_loss: 4.9628\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9587 - val_loss: 4.9633\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9582 - val_loss: 4.9629\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9580 - val_loss: 4.9618\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9622\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9575 - val_loss: 4.9630\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9577 - val_loss: 4.9622\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9572 - val_loss: 4.9631\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9594 - val_loss: 4.9711\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9592 - val_loss: 4.9641\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9578 - val_loss: 4.9624\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9573 - val_loss: 4.9625\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9568 - val_loss: 4.9617\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9566 - val_loss: 4.9616\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9563 - val_loss: 4.9617\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9566 - val_loss: 4.9614\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9603\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9560 - val_loss: 4.9605\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9558 - val_loss: 4.9599\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9603\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9594\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9555 - val_loss: 4.9596\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9553 - val_loss: 4.9592\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9607\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9554 - val_loss: 4.9598\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9590\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9545 - val_loss: 4.9581\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9577\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9583\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9579\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9595\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9577\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9587\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9568 - val_loss: 4.9868\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9660\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9593\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9623\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9574\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9587\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9572 - val_loss: 4.9769\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9628\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9576\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9563\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9563\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632540 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99999\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9922147113078225\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  4\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ...  True False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ... False  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (4919, 28, 28, 1)\n","Train Label Shape:  (4919,)\n","Validation Data Shape:  (983, 28, 28, 1)\n","Validation Label Shape:  (983,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (58, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (5909, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5851, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (58, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5318 samples, validate on 591 samples\n","Epoch 1/150\n","5318/5318 [==============================] - 10s 2ms/step - loss: 5.0724 - val_loss: 5.2884\n","Epoch 2/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9988 - val_loss: 5.0232\n","Epoch 3/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9817 - val_loss: 4.9878\n","Epoch 4/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9744 - val_loss: 4.9802\n","Epoch 5/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9714 - val_loss: 4.9858\n","Epoch 6/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9684 - val_loss: 4.9796\n","Epoch 7/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9661 - val_loss: 4.9724\n","Epoch 8/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9654 - val_loss: 4.9733\n","Epoch 9/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9640 - val_loss: 4.9695\n","Epoch 10/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9623 - val_loss: 4.9688\n","Epoch 11/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9618 - val_loss: 4.9708\n","Epoch 12/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9621 - val_loss: 4.9687\n","Epoch 13/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9607 - val_loss: 4.9661\n","Epoch 14/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9602 - val_loss: 4.9672\n","Epoch 15/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9598 - val_loss: 4.9661\n","Epoch 16/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9594 - val_loss: 4.9650\n","Epoch 17/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9590 - val_loss: 4.9646\n","Epoch 18/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9586 - val_loss: 4.9661\n","Epoch 19/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9586 - val_loss: 4.9671\n","Epoch 20/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9580 - val_loss: 4.9639\n","Epoch 21/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9577 - val_loss: 4.9637\n","Epoch 22/150\n","5318/5318 [==============================] - 2s 308us/step - loss: 4.9573 - val_loss: 4.9634\n","Epoch 23/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9573 - val_loss: 4.9626\n","Epoch 24/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9573 - val_loss: 4.9627\n","Epoch 25/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9569 - val_loss: 4.9626\n","Epoch 26/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9567 - val_loss: 4.9644\n","Epoch 27/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9593 - val_loss: 4.9679\n","Epoch 28/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9573 - val_loss: 4.9648\n","Epoch 29/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9568 - val_loss: 4.9628\n","Epoch 30/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9566 - val_loss: 4.9621\n","Epoch 31/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9562 - val_loss: 4.9615\n","Epoch 32/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9561 - val_loss: 4.9612\n","Epoch 33/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9561 - val_loss: 4.9611\n","Epoch 34/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9558 - val_loss: 4.9610\n","Epoch 35/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9558 - val_loss: 4.9610\n","Epoch 36/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9604\n","Epoch 37/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9554 - val_loss: 4.9605\n","Epoch 38/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9553 - val_loss: 4.9605\n","Epoch 39/150\n","5318/5318 [==============================] - 2s 308us/step - loss: 4.9560 - val_loss: 4.9651\n","Epoch 40/150\n","5318/5318 [==============================] - 2s 306us/step - loss: 4.9556 - val_loss: 4.9602\n","Epoch 41/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9552 - val_loss: 4.9601\n","Epoch 42/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9596\n","Epoch 43/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9597\n","Epoch 44/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9592\n","Epoch 45/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9592\n","Epoch 46/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9549 - val_loss: 4.9594\n","Epoch 47/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 48/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9587\n","Epoch 49/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9590\n","Epoch 50/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9594\n","Epoch 51/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 52/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 53/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9586\n","Epoch 54/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9586\n","Epoch 55/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9585\n","Epoch 56/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9584\n","Epoch 57/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 58/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9604\n","Epoch 59/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9588\n","Epoch 60/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 61/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9586\n","Epoch 62/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9589\n","Epoch 63/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9545 - val_loss: 4.9587\n","Epoch 64/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9579\n","Epoch 65/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 66/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 67/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9577\n","Epoch 68/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 69/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 70/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9577\n","Epoch 71/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9535 - val_loss: 4.9578\n","Epoch 72/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 73/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 74/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9579\n","Epoch 75/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9579\n","Epoch 76/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 77/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 78/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 79/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 80/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 81/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 82/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9576\n","Epoch 83/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9573\n","Epoch 84/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9575\n","Epoch 85/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9594\n","Epoch 86/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9576\n","Epoch 87/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 88/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9577\n","Epoch 89/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 90/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 91/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 92/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 93/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 94/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9573\n","Epoch 95/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 96/150\n","5318/5318 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 97/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 98/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 99/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 100/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9570\n","Epoch 101/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 102/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 103/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 104/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 105/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9569\n","Epoch 106/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 107/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9576\n","Epoch 108/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 109/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 110/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9568\n","Epoch 111/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9610\n","Epoch 112/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9576\n","Epoch 113/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9573\n","Epoch 114/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9575\n","Epoch 115/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 116/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9529 - val_loss: 4.9572\n","Epoch 117/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 118/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 119/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 120/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 121/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 122/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9610\n","Epoch 123/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9532 - val_loss: 4.9577\n","Epoch 124/150\n","5318/5318 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 125/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9571\n","Epoch 126/150\n","5318/5318 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 127/150\n","5318/5318 [==============================] - 2s 307us/step - loss: 4.9528 - val_loss: 4.9573\n","Epoch 128/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9570\n","Epoch 129/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 130/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 131/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9571\n","Epoch 132/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9573\n","Epoch 133/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9573\n","Epoch 134/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 135/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9568\n","Epoch 136/150\n","5318/5318 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9570\n","Epoch 137/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 138/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9569\n","Epoch 139/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 140/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 141/150\n","5318/5318 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 142/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9569\n","Epoch 143/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9570\n","Epoch 144/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9568\n","Epoch 145/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9571\n","Epoch 146/150\n","5318/5318 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9569\n","Epoch 147/150\n","5318/5318 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9568\n","Epoch 148/150\n","5318/5318 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9567\n","Epoch 149/150\n","5318/5318 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9569\n","Epoch 150/150\n","5318/5318 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9568\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (5909, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4632533 0.0\n","The shape of N (5909, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9965618\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9907295540402761\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9782059064468791, 0.982101497533578, 0.992907195351222, 0.9914957065989309, 0.9942008144791047, 0.9905881104909859, 0.9927038702491174, 0.967270552042386, 0.9922147113078225, 0.9907295540402761]\n","AUROC ===== 0.9872417918540304 +/- 0.008267208812660083\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYAAAAEVCAYAAADpbDJPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<matplotlib.figure.Figure at 0x7fe98b244b38>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"AmmHmYSaWE-T","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 3_Vs_all**"]},{"metadata":{"id":"h38EeKY1WJKu","colab_type":"code","outputId":"c02d26a6-3d3f-4602-dfa4-1f8a1aa14931","executionInfo":{"status":"ok","timestamp":1541339873999,"user_tz":-660,"elapsed":2723679,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":65917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 5s 892us/step - loss: 5.0724 - val_loss: 5.1504\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9998 - val_loss: 5.0064\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9840 - val_loss: 4.9857\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9756 - val_loss: 4.9830\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9704 - val_loss: 4.9760\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9677 - val_loss: 4.9718\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9656 - val_loss: 4.9703\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9637 - val_loss: 4.9683\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9624 - val_loss: 4.9678\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9609 - val_loss: 4.9665\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9600 - val_loss: 4.9788\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9598 - val_loss: 4.9722\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9594 - val_loss: 4.9718\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9585 - val_loss: 4.9674\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9585 - val_loss: 4.9699\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9583 - val_loss: 4.9652\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9574 - val_loss: 4.9632\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9568 - val_loss: 4.9627\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9573 - val_loss: 4.9768\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9579 - val_loss: 4.9682\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9568 - val_loss: 4.9616\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9565 - val_loss: 4.9633\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9563 - val_loss: 4.9606\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9593\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9556 - val_loss: 4.9591\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9598\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9554 - val_loss: 4.9720\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9559 - val_loss: 5.0210\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9600 - val_loss: 5.0386\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9575 - val_loss: 4.9912\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9565 - val_loss: 4.9719\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9645\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9604\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9580\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9612\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9614\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9591\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9570\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9581\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 288us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9726\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9577\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9561\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9571\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9602\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9589\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9577\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9559\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845871 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9980561\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9865504125147326\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 4s 774us/step - loss: 5.0650 - val_loss: 5.0748\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9971 - val_loss: 4.9975\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9830 - val_loss: 4.9838\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9757 - val_loss: 4.9776\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9717 - val_loss: 4.9735\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9680 - val_loss: 4.9703\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9664 - val_loss: 4.9846\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9650 - val_loss: 4.9724\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9633 - val_loss: 4.9689\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9633 - val_loss: 4.9692\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9616 - val_loss: 4.9671\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9606 - val_loss: 4.9672\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9603 - val_loss: 4.9678\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9610 - val_loss: 4.9778\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9597 - val_loss: 4.9688\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9587 - val_loss: 4.9634\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9582 - val_loss: 4.9639\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9579 - val_loss: 4.9629\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9577 - val_loss: 4.9632\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9572 - val_loss: 4.9617\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9569 - val_loss: 4.9617\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9572 - val_loss: 4.9618\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9571 - val_loss: 4.9614\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9606\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9599\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9563 - val_loss: 4.9607\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9600\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9561 - val_loss: 4.9631\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9561 - val_loss: 4.9622\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9603 - val_loss: 5.0185\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9606 - val_loss: 4.9887\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9579 - val_loss: 4.9679\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9631\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9561 - val_loss: 4.9598\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9596\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9588\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9560 - val_loss: 4.9584\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9582\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9562 - val_loss: 4.9676\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9569 - val_loss: 4.9621\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9558 - val_loss: 4.9583\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9553 - val_loss: 4.9570\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9575\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9586\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9571\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9564\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9546 - val_loss: 4.9562\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9564\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9575\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9581\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9560\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9602\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9564\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9561\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9562\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9559\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9559\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9558\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9595\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9584\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9557 - val_loss: 4.9633\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9580\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9554\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9554\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9554\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9552 - val_loss: 4.9708\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9621\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9589\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9586\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9590\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 289us/step - loss: 4.9529 - val_loss: 4.9557\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4844890 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9890362155791278\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 5s 879us/step - loss: 5.0778 - val_loss: 5.1349\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9998 - val_loss: 5.0083\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9840 - val_loss: 4.9865\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9757 - val_loss: 4.9799\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9714 - val_loss: 4.9789\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9678 - val_loss: 4.9745\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9651 - val_loss: 4.9718\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9633 - val_loss: 4.9718\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9621 - val_loss: 4.9694\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9611 - val_loss: 4.9692\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9612 - val_loss: 4.9706\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9602 - val_loss: 4.9689\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9596 - val_loss: 4.9671\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9588 - val_loss: 4.9669\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9586 - val_loss: 4.9732\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9581 - val_loss: 4.9659\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9577 - val_loss: 4.9653\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9573 - val_loss: 4.9645\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9624\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9620\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9620\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9560 - val_loss: 4.9606\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9556 - val_loss: 4.9601\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9603\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9555 - val_loss: 4.9601\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9604\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9553 - val_loss: 4.9613\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9594\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9548 - val_loss: 4.9600\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9550 - val_loss: 4.9631\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9603\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9615\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9579\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9590\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9691\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9594\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9528 - val_loss: 4.9560\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9527 - val_loss: 4.9559\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9647\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9556\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9523 - val_loss: 4.9557\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9555\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9565\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9522 - val_loss: 4.9556\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 290us/step - loss: 4.9521 - val_loss: 4.9552\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845769 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9999817\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9872790099646416\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 6s 1ms/step - loss: 5.0632 - val_loss: 5.1870\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9970 - val_loss: 5.0183\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9834 - val_loss: 4.9856\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9770 - val_loss: 4.9819\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9722 - val_loss: 4.9788\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9683 - val_loss: 4.9740\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9661 - val_loss: 4.9713\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9648 - val_loss: 4.9700\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9637 - val_loss: 4.9743\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9624 - val_loss: 4.9688\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9618 - val_loss: 4.9708\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9611 - val_loss: 4.9722\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9608 - val_loss: 4.9865\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9601 - val_loss: 4.9718\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9593 - val_loss: 4.9698\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9591 - val_loss: 4.9701\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9587 - val_loss: 4.9677\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9582 - val_loss: 4.9666\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9590 - val_loss: 5.0038\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9586 - val_loss: 4.9907\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9586 - val_loss: 4.9684\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9594 - val_loss: 4.9624\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9580 - val_loss: 4.9618\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9575 - val_loss: 4.9706\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9573 - val_loss: 4.9668\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9569 - val_loss: 4.9655\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9565 - val_loss: 4.9635\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9567 - val_loss: 4.9609\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9563 - val_loss: 4.9597\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9597\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9598\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9614 - val_loss: 5.0223\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9590 - val_loss: 4.9768\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9573 - val_loss: 4.9655\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9566 - val_loss: 4.9619\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9564 - val_loss: 4.9591\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9562 - val_loss: 4.9585\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9560 - val_loss: 4.9578\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9561 - val_loss: 4.9574\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9582 - val_loss: 4.9811\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9581 - val_loss: 4.9621\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9596\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9559 - val_loss: 4.9587\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9576\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9575\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9615\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9611\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9570\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9571\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9570\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9589\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9564\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9563\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9567\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9592\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9577\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9543 - val_loss: 4.9563\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9582\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9543 - val_loss: 4.9562\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9561\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9542 - val_loss: 4.9561\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9559\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9762\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9574\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9549 - val_loss: 4.9599\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9578\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9532 - val_loss: 4.9557\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845359 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.98483070823958\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 6s 1ms/step - loss: 5.0735 - val_loss: 5.1033\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9986 - val_loss: 5.0119\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9839 - val_loss: 4.9854\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9763 - val_loss: 4.9797\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9710 - val_loss: 4.9756\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9681 - val_loss: 4.9730\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9664 - val_loss: 4.9732\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9647 - val_loss: 4.9700\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9629 - val_loss: 4.9685\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9622 - val_loss: 4.9680\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9619 - val_loss: 4.9682\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9612 - val_loss: 4.9701\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9600 - val_loss: 4.9666\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9595 - val_loss: 4.9665\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9593 - val_loss: 4.9648\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9588 - val_loss: 4.9644\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9582 - val_loss: 4.9635\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9581 - val_loss: 4.9624\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9577 - val_loss: 4.9625\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9575 - val_loss: 4.9673\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9570 - val_loss: 4.9639\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9567 - val_loss: 4.9799\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9569 - val_loss: 4.9702\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9572 - val_loss: 5.0317\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9596 - val_loss: 5.0031\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9579 - val_loss: 4.9804\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9573 - val_loss: 4.9703\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9641\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9631\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9612\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9569 - val_loss: 4.9613\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9563 - val_loss: 4.9594\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9576\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9573\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9562 - val_loss: 4.9597\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9556 - val_loss: 4.9574\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9576\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9740\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9625\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9588\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9616\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9601\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9590\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9578\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9566\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9565\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9544 - val_loss: 4.9563\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9544 - val_loss: 4.9607\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9557\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9557\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9561\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9584\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9605\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9704\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9613\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9853\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9651\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9610\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9677\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9558 - val_loss: 4.9598\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9629\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9576\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9553\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9590\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9612\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9605\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9576\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9570\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9553\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845374 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.983890496089146\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 7s 1ms/step - loss: 5.0573 - val_loss: 5.2266\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9962 - val_loss: 5.0143\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9834 - val_loss: 4.9883\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9760 - val_loss: 4.9793\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9724 - val_loss: 4.9764\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9690 - val_loss: 4.9740\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9664 - val_loss: 4.9705\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9648 - val_loss: 4.9725\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9646 - val_loss: 4.9710\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9629 - val_loss: 4.9778\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9617 - val_loss: 4.9695\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9611 - val_loss: 4.9704\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9600 - val_loss: 4.9682\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9610 - val_loss: 4.9714\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9594 - val_loss: 4.9697\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9588 - val_loss: 4.9650\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9602 - val_loss: 4.9676\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9591 - val_loss: 4.9644\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9584 - val_loss: 4.9625\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9578 - val_loss: 4.9615\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9572 - val_loss: 4.9613\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9570 - val_loss: 4.9607\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9569 - val_loss: 4.9598\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9623\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9568 - val_loss: 4.9594\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9565 - val_loss: 4.9605\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9567 - val_loss: 4.9723\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9635\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9565 - val_loss: 4.9600\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9561 - val_loss: 4.9609\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9579 - val_loss: 4.9731\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9590 - val_loss: 4.9724\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9569 - val_loss: 4.9645\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9562 - val_loss: 4.9608\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9560 - val_loss: 4.9588\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9558 - val_loss: 4.9584\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9600\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9581\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9572\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9570\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9571\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9566\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9564\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9565\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9546 - val_loss: 4.9579\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9580\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9568\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9563\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9563\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9564\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9544 - val_loss: 4.9561\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9606 - val_loss: 5.1684\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9567 - val_loss: 4.9818\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9557 - val_loss: 4.9625\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9591\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9572\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9569\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9568\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9568\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9561\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9561\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9559\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9541 - val_loss: 4.9584\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9559\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9558\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9557\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9557\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9558\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9556\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9557\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9556\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9617\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9606 - val_loss: 4.9947\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9564 - val_loss: 4.9703\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9605\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9589\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9560\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9558\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9558\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9555\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9555\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9594\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9607\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9579\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9556\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9554\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9553\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9554\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9554\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9554\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9553\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9554\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9554\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9553\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9553\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9553\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9553\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9551\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9552\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9551\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9677\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9598\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9572\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4843678 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9661443265830922\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 8s 1ms/step - loss: 5.0614 - val_loss: 5.1022\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9982 - val_loss: 5.0017\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9843 - val_loss: 4.9857\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9770 - val_loss: 4.9784\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9723 - val_loss: 4.9768\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9686 - val_loss: 4.9741\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9666 - val_loss: 4.9710\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9643 - val_loss: 4.9707\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9632 - val_loss: 4.9691\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9623 - val_loss: 4.9683\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9609 - val_loss: 4.9657\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9600 - val_loss: 4.9678\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9596 - val_loss: 5.0003\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9590 - val_loss: 4.9682\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9596 - val_loss: 4.9763\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9588 - val_loss: 4.9712\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9582 - val_loss: 4.9693\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9579 - val_loss: 4.9709\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9658\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9570 - val_loss: 4.9618\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9574 - val_loss: 4.9667\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9570 - val_loss: 4.9613\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9568 - val_loss: 4.9738\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9570 - val_loss: 4.9634\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9615\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9561 - val_loss: 4.9618\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9558 - val_loss: 4.9603\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9557 - val_loss: 4.9593\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9592\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9657\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9555 - val_loss: 4.9589\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9604\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9550 - val_loss: 4.9585\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9577\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9583\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9578\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9561 - val_loss: 4.9636\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9643\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9579\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9581\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9593\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9555\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9649\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9556\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9667\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9557 - val_loss: 4.9588\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9576\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9577\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9578\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9573\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9562\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9551\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9527 - val_loss: 4.9570\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9552\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9562\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9579\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9550\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9552\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9561 - val_loss: 4.9917\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9623\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9533 - val_loss: 4.9583\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9592\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9551\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9553\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845788 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9898424943747991\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 8s 1ms/step - loss: 5.0785 - val_loss: 5.1416\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 5.0020 - val_loss: 5.0093\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9846 - val_loss: 4.9871\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9755 - val_loss: 4.9824\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9700 - val_loss: 4.9760\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9669 - val_loss: 4.9698\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9652 - val_loss: 4.9687\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9635 - val_loss: 4.9695\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9624 - val_loss: 4.9718\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9615 - val_loss: 4.9677\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9608 - val_loss: 4.9663\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9601 - val_loss: 4.9649\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9598 - val_loss: 4.9770\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9595 - val_loss: 4.9664\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9588 - val_loss: 4.9648\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9584 - val_loss: 4.9635\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9579 - val_loss: 4.9631\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9575 - val_loss: 4.9625\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9574 - val_loss: 4.9618\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9571 - val_loss: 4.9608\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9568 - val_loss: 4.9615\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9567 - val_loss: 4.9604\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9565 - val_loss: 4.9612\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9562 - val_loss: 4.9600\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9611\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9601\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9594\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9592\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9596\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9584\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9550 - val_loss: 4.9582\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9576\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9547 - val_loss: 4.9574\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9595\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9611\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9562\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9619\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9587\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9582\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9559\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9556\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9560\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9576\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9558\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9557\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9558\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9564\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9556\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9556\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845847 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -0.98751265\n","The max value of N 0.98985445\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9888808528876031\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 9s 2ms/step - loss: 5.0671 - val_loss: 5.2082\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9975 - val_loss: 5.0211\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9830 - val_loss: 4.9900\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9760 - val_loss: 4.9856\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9708 - val_loss: 4.9827\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9684 - val_loss: 4.9798\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9650 - val_loss: 4.9729\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9632 - val_loss: 4.9702\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9641 - val_loss: 4.9813\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9622 - val_loss: 4.9737\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9606 - val_loss: 4.9685\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9597 - val_loss: 4.9677\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9599 - val_loss: 4.9793\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9590 - val_loss: 4.9851\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9589 - val_loss: 4.9770\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9582 - val_loss: 4.9852\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9577 - val_loss: 4.9698\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9570 - val_loss: 4.9645\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9648\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9565 - val_loss: 4.9630\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9637\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9560 - val_loss: 4.9613\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9608\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9555 - val_loss: 4.9595\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9555 - val_loss: 4.9618\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9601\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9589\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9576 - val_loss: 4.9656\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9564 - val_loss: 4.9601\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9559 - val_loss: 4.9602\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9550 - val_loss: 4.9586\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9584\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9627 - val_loss: 5.0418\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9566 - val_loss: 4.9732\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9629\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9565\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9560\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9561\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9560\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9579\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9585\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9598\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9555\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9609\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9593\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9553\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9558\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9553\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9551\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9622\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9589\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9529 - val_loss: 4.9550\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9528 - val_loss: 4.9569\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9527 - val_loss: 4.9610\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9562\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9563\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9556\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9524 - val_loss: 4.9548\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9549\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9523 - val_loss: 4.9549\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9523 - val_loss: 4.9550\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9550\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9691\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9574\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9558\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9548\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 309us/step - loss: 4.9522 - val_loss: 4.9568\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9560\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9550\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9576\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9662\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9551\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9522 - val_loss: 4.9548\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9522 - val_loss: 4.9564\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9521 - val_loss: 4.9554\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9521 - val_loss: 4.9548\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9522 - val_loss: 4.9562\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9570\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9521 - val_loss: 4.9549\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9522 - val_loss: 4.9549\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845143 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 1.0\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9804644808743168\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  3\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5162, 28, 28, 1)\n","Train Label Shape:  (5162,)\n","Validation Data Shape:  (1031, 28, 28, 1)\n","Validation Label Shape:  (1031,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (61, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6181, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6120, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (61, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5562 samples, validate on 619 samples\n","Epoch 1/150\n","5562/5562 [==============================] - 10s 2ms/step - loss: 5.0612 - val_loss: 5.1205\n","Epoch 2/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9963 - val_loss: 5.0021\n","Epoch 3/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9830 - val_loss: 4.9870\n","Epoch 4/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9756 - val_loss: 4.9808\n","Epoch 5/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9718 - val_loss: 4.9782\n","Epoch 6/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9682 - val_loss: 4.9735\n","Epoch 7/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9659 - val_loss: 4.9712\n","Epoch 8/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9651 - val_loss: 4.9704\n","Epoch 9/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9632 - val_loss: 4.9693\n","Epoch 10/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9640 - val_loss: 4.9766\n","Epoch 11/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9633 - val_loss: 4.9900\n","Epoch 12/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9619 - val_loss: 4.9778\n","Epoch 13/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9621 - val_loss: 4.9750\n","Epoch 14/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9602 - val_loss: 4.9745\n","Epoch 15/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9592 - val_loss: 4.9689\n","Epoch 16/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9585 - val_loss: 4.9691\n","Epoch 17/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9584 - val_loss: 4.9676\n","Epoch 18/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9583 - val_loss: 4.9653\n","Epoch 19/150\n","5562/5562 [==============================] - 2s 307us/step - loss: 4.9579 - val_loss: 4.9645\n","Epoch 20/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9576 - val_loss: 4.9639\n","Epoch 21/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9570 - val_loss: 4.9610\n","Epoch 22/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9570 - val_loss: 4.9602\n","Epoch 23/150\n","5562/5562 [==============================] - 2s 307us/step - loss: 4.9569 - val_loss: 4.9617\n","Epoch 24/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9575 - val_loss: 4.9614\n","Epoch 25/150\n","5562/5562 [==============================] - 2s 311us/step - loss: 4.9566 - val_loss: 4.9601\n","Epoch 26/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9563 - val_loss: 4.9607\n","Epoch 27/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9562 - val_loss: 4.9597\n","Epoch 28/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9560 - val_loss: 4.9602\n","Epoch 29/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9559 - val_loss: 4.9595\n","Epoch 30/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9559 - val_loss: 4.9597\n","Epoch 31/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9593\n","Epoch 32/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9592\n","Epoch 33/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9592\n","Epoch 34/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9554 - val_loss: 4.9594\n","Epoch 35/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 36/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9592\n","Epoch 37/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 38/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 39/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 40/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 41/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9585\n","Epoch 42/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9579\n","Epoch 43/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 44/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 45/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9559 - val_loss: 4.9670\n","Epoch 46/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9575 - val_loss: 4.9709\n","Epoch 47/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9560 - val_loss: 4.9649\n","Epoch 48/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9602\n","Epoch 49/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 50/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 51/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9578\n","Epoch 52/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9573\n","Epoch 53/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9582\n","Epoch 54/150\n","5562/5562 [==============================] - 2s 308us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 55/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9568\n","Epoch 56/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9558 - val_loss: 4.9611\n","Epoch 57/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9553 - val_loss: 4.9587\n","Epoch 58/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9606\n","Epoch 59/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 60/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9549 - val_loss: 4.9591\n","Epoch 61/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9604\n","Epoch 62/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 63/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 64/150\n","5562/5562 [==============================] - 2s 307us/step - loss: 4.9545 - val_loss: 4.9565\n","Epoch 65/150\n","5562/5562 [==============================] - 2s 309us/step - loss: 4.9546 - val_loss: 4.9564\n","Epoch 66/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 67/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9562\n","Epoch 68/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 69/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9543 - val_loss: 4.9563\n","Epoch 70/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9559\n","Epoch 71/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 72/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9563\n","Epoch 73/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9560\n","Epoch 74/150\n","5562/5562 [==============================] - 2s 306us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 75/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 76/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9558\n","Epoch 77/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9872\n","Epoch 78/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9584 - val_loss: 4.9706\n","Epoch 79/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9632\n","Epoch 80/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9554 - val_loss: 4.9615\n","Epoch 81/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9588\n","Epoch 82/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9628\n","Epoch 83/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9597\n","Epoch 84/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 85/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 86/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 87/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9562\n","Epoch 88/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 89/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9563\n","Epoch 90/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 91/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 92/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9563\n","Epoch 93/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9548 - val_loss: 4.9562\n","Epoch 94/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9550 - val_loss: 4.9917\n","Epoch 95/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9755\n","Epoch 96/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9668\n","Epoch 97/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9606\n","Epoch 98/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9578\n","Epoch 99/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 100/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 101/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 102/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9563\n","Epoch 103/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 104/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9559\n","Epoch 105/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 106/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9560\n","Epoch 107/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9562\n","Epoch 108/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 109/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 110/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 111/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9561\n","Epoch 112/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 113/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9558\n","Epoch 114/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9557\n","Epoch 115/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 116/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9556\n","Epoch 117/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 118/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 119/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9559\n","Epoch 120/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 121/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9535 - val_loss: 4.9557\n","Epoch 122/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9536 - val_loss: 4.9557\n","Epoch 123/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 124/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 125/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 126/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 127/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 128/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9670\n","Epoch 129/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 130/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 131/150\n","5562/5562 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 132/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9558\n","Epoch 133/150\n","5562/5562 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 134/150\n","5562/5562 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9593\n","Epoch 135/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 136/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 137/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 138/150\n","5562/5562 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 139/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 140/150\n","5562/5562 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 141/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 142/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9557\n","Epoch 143/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9560\n","Epoch 144/150\n","5562/5562 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 145/150\n","5562/5562 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 146/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 147/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9554\n","Epoch 148/150\n","5562/5562 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 149/150\n","5562/5562 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 150/150\n","5562/5562 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9560\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6181, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4845894 0.0\n","The shape of N (6181, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9984367\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9858164577306332\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9865504125147326, 0.9890362155791278, 0.9872790099646416, 0.98483070823958, 0.983890496089146, 0.9661443265830922, 0.9898424943747991, 0.9888808528876031, 0.9804644808743168, 0.9858164577306332]\n","AUROC ===== 0.9842735454837674 +/- 0.006596633432656195\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAIABJREFUeJzsvXmYHHd57/up6m16m1WzSbJkyZJL\ntoXxim2MsSGsJziBBHKOTXJykkASCMt5Em6SmxtucE4eCNkgOScJSwKESwDbYTPggAHLC3i3bEvW\nUlpGs+8zPb0vtd0/qrunZ9VsrWlNv5/n0aOe6qrqt7u6f996l9/7UxzHQRAEQRAA1M02QBAEQagd\nRBQEQRCEMiIKgiAIQhkRBUEQBKGMiIIgCIJQRkRBEARBKCOiIAjrQNO0f9E07WPn2ed/aJr245Vu\nF4TNRERBEARBKOPdbAME4UKhadqlwJPAp4DfAhTgvwMfBa4Bfqjr+m8W930X8Ge4v5Fh4L26rp/V\nNK0N+BqwHzgOZIDB4jFXAv8MdAN54Dd0XX9uhba1Ap8BXglYwL/puv7J4nN/AbyraO8g8Ku6rg8v\ntX2tn48ggHgKQv2xDRjVdV0DjgD3Ar8OXA3crWnaZZqm7QI+D7xd1/UDwPeBzxaP/yNgQtf1PcDv\nAW8G0DRNBb4NfFnX9cuB3wW+o2naSm+8Pg7Eina9Bni/pmmv0TTtKuBXgIPF834LeMNS29f+sQiC\ni4iCUG94gfuLj48Cz+q6Pqnr+hQwAmwH3ggc0nX9THG/fwFeVxzgXwvcB6Drei/waHGfA0AH8IXi\ncz8DJoBXr9Cunwf+qXjsNPBN4E3ADNAOvFvTtBZd1/+3rutfXma7IKwLEQWh3rB0Xc+WHgOpyucA\nD+5gGytt1HU9jhui2Qa0AvGKY0r7NQMh4ISmaSc1TTuJKxJtK7RrzmsWH3fouj4E/BJumKhf07Tv\na5p2yVLbV/hagrAkklMQhIWMAbeU/tA0rQWwgUncwbqpYt92oAc375AohpvmoGna/1jha7YB/cW/\n24rb0HX9EHBI07Qw8DfAXwLvXmr7it+lICyCeAqCsJAfAa/VNG1v8e/fBR7Sdd3ETVS/A0DTtMtw\n4/8AfcCgpmnvLD63TdO0rxUH7JXwPeC3S8fiegHf1zTtTZqm/aOmaaqu62ngJcBZavt637ggiCgI\nwjx0XR8E3oObKD6Jm0f4neLTnwB2a5p2DvjfuLF/dF13gP8GfKB4zGPAT4oD9kr4U6Cl4ti/1HX9\nmeLjEHBK07RjwH8F/t9ltgvCulBkPQVBEAShhHgKgiAIQhkRBUEQBKGMiIIgCIJQRkRBEARBKHPR\nz1OYmEiuOVPe0hIiFstspDkbjti4MdS6jbVuH4iNG0Wt2NjeHlUW217XnoLX69lsE86L2Lgx1LqN\ntW4fiI0bRa3bWNeiIAiCIMxFREEQBEEoI6IgCIIglBFREARBEMqIKAiCIAhlRBQEQRCEMiIKgiAI\nQpmLfvLaWjkWS+HPF9gf8G+2KYIgCDVD3XoKPxma4j9ODm22GYIgCAA88shPVrTf3//93zI8XL2x\nq25FAcCyZS0JQRA2n5GRYX784x+uaN8Pf/gP2L59R9VsqdvwkaooWLLAkCAINcDf/d0nOXHiGLfd\ndiNvetNbGRkZ5tOf/ic+8Yk/Z2JinGw2y2/+5m9z66238YEP/Da///t/yKFDPyGdTtHf38fQ0CAf\n+tAfcMstt67blroVBQWQVecEQZjPfQ+f4dmT4xt6zhsPdPArr9+35PN33fVrfPOb97Fnz2X09/fy\nT//0L8Ri07zqVTfz1re+jaGhQT760T/m1ltvm3Pc+PgYf/M3/8BTTz3Bd77zDRGF9aAqCvZmGyEI\ngjCPK664CoBotJETJ47xwAPfRFFUEon4gn2vvvoaADo6OkilUhvy+nUsCpJTEARhIb/y+n3L3tVX\nG5/PB8CPfvQDEokE//iP/0IikeA97/m1Bft6PLMdVzcq8lG3iWZVUXCQEJIgCJuPqqpYljVn28zM\nDN3d21FVlUcffRjDMC6MLRfkVWqQ0uoSEkISBGGz2b17D7p+knR6NgR0xx2v54knHufDH34fwWCQ\njo4OvvjFz1fdFuViv1Ne68prX9CHOJPI8OfXX4ZXrV1tbG+PMjGR3GwzlkVsXD+1bh+IjRtFrdgo\nK6/NQy1+HJJWEARBmKV+RaEYQLIRVRAEQShRv6JQ9BQu8uiZIAjChlK3opCz3BSzzGoWBEGYpW5F\nYSSTB5DgkSAIQgV1KwolMbDFUxAEQShTt6JQnqcgmiAIQg2w0tbZJV588TCx2PSG21G1Nheapt0B\n3A8cK246quv6Byuefx3wCcACdOA9uq7bmqZ9CrgZ92b+w7quP1sN+xRJNAuCUCOUWmffccfPrfiY\n73//Ae6661dpaWndUFuq3fvoUV3X37nEc58DXqfr+qCmafcDb9E0LQ3s13X9Fk3TrgC+ANxSDcMU\nKUkVBKFGKLXO/sIXPkdPzxmSySSWZfE//+f/xb59+/nKV77Eo48eQlVVbr31Nq644koef/wRzp3r\n4S/+4q/o6uraMFs2syHe9bquJ4qPJ4A2XA/h2wC6rp/QNK1F07TGiv02DJm8JgjCYnzzzPd4Yfzo\nhp7z2o5X8Ev73rbk86XW2aqqctNNr+bOO9/OuXM9/P3f/w2f/vQ/8fWvf4Vvf/sHeDwevv3tb3Dj\njTezb9/l/P7v/+GGCgJUXxSu1DTtAaAVuEfX9R+VnigN9JqmdQNvAj6KG056vuL4CaALWFIUWlpC\neL2epZ5eEo+qAhaNTUHam0KrPv5C0t4e3WwTzovYuH5q3T6oDxtDQ3486qIdINZ+zqB/jl3zbWxu\nDhEI+ND1Y0xPT3Po0EMA5PNZ2tujvOUtb+EP//BDvO1tb+Puu99FJBLB7/fS0hLe8GtSTVE4DdwD\n3AfsBQ5pmrZP1/VCaQdN0zqA7wLv13V9StO0+ec475WJxTJrMs4puggT02kaCtZ59t48aqVPynKI\njeun1u2D+rHxLTvexFt2vGmDLJqlZNdiNs7MZMjnDRxH4YMf/AMOHrx6znEf+MBH6Ovr5eGHf8Rd\nd93N5z73bxQKJrFYes3vdykxqVr1ka7rQ7qu36vruqPr+llgFCgvLKppWiPwn8Cf6rr+UHHzMK5n\nUGI7MFIN+0o3AqYjfVIFQdhcSq2zr7zyII899ggA58718PWvf4VUKsUXv/h5du++lN/4jfcSjTaR\nyaQXbbe9IbZs+BmLaJr2bk3TPlJ83AV0AkMVu/wt8Cld139Qse0h4J3FY64DhnVdr8qtiVIsP7JF\nEwRB2GRKrbNnZmIMDQ3w/ve/h09+8i+45prriEQizMzEeO97/zsf+tDvctVVB2lsbOKaa67jT//0\nj+jpObuhtlStdbamaVHgq0Az4McNJXUAceCHQAx4suKQr+q6/jlN0/4SeC3uUge/p+v6S8u9zlpb\nZ//tkV6m8ga/vn87WnN4Lae4INSLy15tat3GWrcPxMaNolZsXKp1dtVyCsU7/DuX2SWwxHF/XB2L\n5lIKH0nvI0EQhFnqdkazWgwfiSgIgiDMUreiUPKbTJmoIAiCUKbuRUFmNAuCIMxSt6IwlZsCwBJP\nQRAEoUzdioJpG4DkFARBECqpW1EorahgyTwFQRCEMnUrCqXW2ZbkFARBEMrUrygU/xdPQRAEYZY6\nFoVi+Eh6HwmCIJSpY1FwkUSzIAjCLPUrCoorBqZ0xBMEQShTt6JQeuMSPhIEQZilbkVBKa+nIOEj\nQRCEEnUrCqU3LuEjQRCEWepWFEo5BVs8BUEQhDJ1LArSOlsQBGE+dSsKnuL/kmjeWsQLJl87O8J0\n3thsUwThoqRuRWF28tomGyJsKD2JDEenU5yJZzbbFEG4KKlbUfCoEj7aipRyRLJOhiCsjboVhfIi\nO7KewpaiFAyUyyoIa6NuRcFTVAWpPtpalMRArqsgrI06FgUJH21FyuEjua6CsCbqVhTUkqewuWYI\nG0xJCiR8JAhro25FoTRPQe4otxaSaBaE9VG3ouAti8ImGyJsKLM5hc21QxAuVupWFEqJZkc8hS2F\ng+QUBGE91K8oFN+5hBm2FpZ4CoKwLrzVOrGmaXcA9wPHipuO6rr+wYrnG4DPAlfpun7DSo7ZSFTF\nVQUZPLYWjlQfCcK6qJooFHlU1/V3LvHcXwMvAlet4pgNo5RTkKFja1ESeWlfIghrYzPDR38CfGuz\nXnwsMwrIHeVWoxQOlLCgIKyNansKV2qa9gDQCtyj6/qPSk/oup7UNK1tNccsRktLCK/Xs9wuizKa\nGQWvhqIqtLdHV338haTW7YPasbFhKg5AIOBbYFOt2LgUtW4fiI0bRS3bWE1ROA3cA9wH7AUOaZq2\nT9f1wkYeE4utsRtmsWW2ZTlMTCTXdo4LQHt7tKbtg9qyMZ12vyrpbGGOTbVk42LUun0gNm4UtWLj\nUsJUNVHQdX0IuLf451lN00aBHcC5jTxmrXgAC8kpbDVmG+LJlRWEtVC1nIKmae/WNO0jxcddQCcw\ntNHHrBW1uBynjB1bi5IYyHUVhLVRzfDRA8BXNU37RcAPvA+4W9O0uK7r39I07X7gEkDTNO0R4HOL\nHXOecNOaUaX6aEtSrj6SKysIa6Ka4aMkcOcyz79riaeWPGYj8aUayDeJKGw1ZrukbrIhgnCRUrcz\nmkM9nYC0udhqzHZJlesqCGuhbkVBsSV8tBWR9RQEYX3UryhIonlLIl1SBWF91LEobLYFQjWQGc2C\nsD7qVhRsqT7akoinIAjro25FwXJ8gIjCVkO6pArC+qhbUSjHjxyJI20lxFMQhPVRv6JQv+98S2PL\nymuCsC6q3SW1ZlEURW4ntyAlLbCX300QhCWo2/tlRUUSClsQS3IKgrAu6lYUUBUUGTi2HDKjWRDW\nR12LAg7iLWwxpPeRIKyPuhUFRQXFcXCQ6qOtxGz1kaiCIKyFuhUFVRUx2IqIpyAI66NuRcHymDhW\ndrPNEDaYck5B4oKCsCbqVhSGGp8hYT642WYIG4x4CoKwPupWFGzVwEE8ha2G5BQEYX3UrSgoqDhY\n0i51izE7o3mTDRGEi5S6FgWZ97r1KHsKklMQhDVRt6Kg4gEcHEeEYSshXVIFYX3UrygopbduyzrN\nW4iSxEv4SBDWhoiCYksQaQtR8hAcxFsQhLVQx6LgAcDBksFjC1HpIYi3IAirp25FwVMUBTevsKmm\nCBtIpcCL2AvC6qlbUVDLomBh2RJA2ipUyoBUIAnC6qlbUfCUw0c2powdW4ZK70AcBUFYPXUvCjg2\nppSlbhkq8wiWqIIgrJqqLcepadodwP3AseKmo7quf7Di+Qbgs8BVuq7fULH9U8DNuJGAD+u6/mw1\n7DOTFgQBLEzRhC2DQ2VOYRMNEYSLlGqv0fyoruvvXOK5vwZeBK4qbdA07XZgv67rt2iadgXwBeCW\nahg2Y48DbvjIEk9hyzC3+khUQRBWy2aGj/4E+Na8bT8HfBtA1/UTQIumaY3VeHFbtdwHjiU5hS2C\n7TjzEs2CIKyWansKV2qa9gDQCtyj6/qPSk/oup7UNK1t3v5dwPMVf08UtyWWeoGWlhBer2epp5dE\nLa645mATbQzS3hpZ9TkuFO3t0c024bzUgo3mvHhRc0uI9nBD+e9asHE5at0+EBs3ilq2cdWioGla\nAOjQdX3gPLueBu4B7gP2Aoc0Tdun63phFS933hamsVhmFaerPHHp1BaT02marNp0F9rbo0xMJDfb\njGWpFRuNeaXFk1Np1IwB1I6NS1Hr9oHYuFHUio1LCdOKREHTtP8bSAH/CjwHJDVNe0jX9Y8udYyu\n60PAvcU/z2qaNgrsAM4t81LDuJ5Bie3AyEpsXC1Kqc2FVB9tGeanECSnIAirZ6U5hTuB/wO8C/iu\nrus3Abcud4Cmae/WNO0jxcddQCcwdJ7XeQh4Z/GY64BhXderIqmV4SNTJq9tCeaLgIiCIKyelYqC\noeu6A7yVYiIYOF8g/wHgdk3THge+A7wPuFvTtHcAaJp2P/B196H2iKZpd+u6/gTwvKZpTwD/APze\n6t7Oyil7ClgYtlWtlxEuIPOlXaReEFbPSnMKM5qmfR/Yqev6k5qmvY3z/OaKd/h3LvP8u5bY/scr\ntGldVE5eMyR8tCUQT0EQ1s9KReFu4I3Az4p/54Bfr4pFF4hS62wHC8MSUdgKzJ+sJpPXBGH1rDR8\n1A5M6Lo+oWnae4G7gHD1zKo+XqWkh5Jo3iqIpyAI62elovBFoKBp2rXAe4Bv4Mb8L1q8aqn6yJJE\n8xZhvgSIpyAIq2elouAUexC9A/g/uq4/yArmENQyXtUHSPXRVmKBpyCtswVh1aw0pxDRNO1G3HLR\n24sT2FqqZ1b18Xm8YIGEj7YOC3MKIgqCsFpW6in8LfB54LO6rk8AHwO+Wi2jLgQ+1dVDBwtL4gxb\ngvmegVxWQVg9K/IUdF2/F7hX07RWTdNagD8pzlu4aPEVw0cyo3nrMF8EZD0FQVg9K/IUNE27VdO0\ns8BJ3J5GJzRNu+E8h9U0AU9RFMRT2DI4RRHwKm66Sy6rIKyelYaPPgH8oq7rHbqub8MtSf276plV\nffyqHyitpyCjx1agJAJetdTCRK6rIKyWlYqCpev6y6U/dF1/ATCrY9KFofuJ4oJujoUporAlKOUU\nPOIpCMKaWWn1ka1p2i8DpfUQ3kKxdudipWFiCoiArLy2ZXDmeQpSfSQIq2elnsLvAu8FenFbX/86\n8DtVsumCoKizXVLljnJrYM3LKdToEhmCUNMs6ykUO5yWfloKcKz4uBH4EvDaqllWZZSSn+NYklPY\nIpSuokc8BUFYM+cLH/3pBbFiE1AzpZSIjSUzmrcEJRHwKSIKgrBWlhUFXdcfvVCGXGjU4ngh1Udb\nh1IYsOwpbKItgnCxstKcwtZDAcVR3PCReApbglIJqlc8BUFYM3UrCoqioNiq6ylIpnlLMH+eglxW\nQVg9dSsKDVeEcccOmdG8VbAXzGiW6yoIq6V+RWFfCK8HcGxsEYUtwQJPYRNtEYSLlboVBUwHFcXt\nkip3lFsCW3IKgrBu6lYUDFMptkMQT2GrsKD6SC6rIKyauhWFU/FtWKYKMqN5y7CwS6pcWEFYLXUr\nCo+P7SWd8+E4FtL6aGtQuoyl9bdFFARh9dStKICDY3sAGxk7tgYLq4820xpBuDipW1HwKjaOrYIk\nmrcMC2c0y3UVhNVSv6Kg2uAU376IwpZgYe+jzbRGEC5O6lYUPKoDTrF9tiQVtgTSJVUQ1s9KF9lZ\nNZqm3QHcz2y77aO6rn+w4vk3AB/HXaznQV3X/9f5jtlIPMqspyCDx9ag5Bn4pCRVENZM1UShyKO6\nrr9zief+AXgzMAQ8qmnaN1ZwzIbhUZxiTgGk/GhrUBJ3j5SkCsKa2ZTwkaZpe4FpXdcHdF23gQeB\nn7uQNqjKbPjIloYIW4JySaoiiWZBWCvV9hSu1DTtAaAVuEfX9dIaz13ARMV+48BlwNFljlmUlpYQ\nXq9n1YZ5VWYTzdi0t0dXfY4LRS3bVqIWbAzGkgC0NocA8Pq8c+yqBRuXo9btA7Fxo6hlG6spCqeB\ne4D7gL3AIU3T9um6XlhkX2UNxwAQi2XWZJyqAsXwkePYTEwk13SeatPeHq1Z20rUio2ptPs1SSdz\nAOTyRtmuWrFxKWrdPhAbN4pasXEpYaqaKOi6PgTcW/zzrKZpo8AO4BwwjOstlNgBDJ/nmA3F7ZBa\nFAUJH20JypPXJNEsCGumajkFTdPerWnaR4qPu4BO3KQyuq73Ao2apl2qaZoXeBvw0HLHbDQ+rwfH\nKTkoVjVeQrjAzM8pyKREQVg91Uw0PwDcrmna48B3gPcBd2ua9o7i8+8DvgY8Dtyr6/qpxY5ZLnS0\nHgI+r3gKW4xSQzxZo1kQ1k41w0dJ4M5lnn8MuGU1x2wkwQYfJGYTzcLKsR2HR0ZiXNUSpjMY2Gxz\nylhFx0BVFFRFSlIFYS3U7YzmcDAwO6NZShdXxUSuwI+HpnhqLL7ZpsyhdB1VcBdQkssqCKumjkUh\nWFGSWh85BdM2mcxOr/s8heItecGuLQ+r5BmIpyAIa6duRaGpKVqe0VwvOYUf9z/GPU/9FVPrFAbT\nqVVRcP9XFVcYRBQEYfXUrSi0NkcruqTW1uBWLXrivdiOzVRufaJgFMXAqLGaT7scPnI9hfrw/wRh\nY6lfUWiJlHMK9ZJoHk2PAZAxc+s6j1kUg4JVW59byTFQFFcYxFMQhNVTt6IQCoZmS1KV2hrcqkHB\nKjCdmwEgu05RKHkIhVrzFOZUHykyeU0Q1kDdioLXE2C2Y9LFJQqpRI5kfHUD+1hmolydk1u3p1AK\nH9XW5zabaEYSzYKwRupWFFA8eIqD5MWWaP7ht4/xvfuOrOqYkWLoCCBrZtf1+oZTo55C8X83pyCe\ngiCshWp3Sa1ZFEXBQykZeXGJQjqRJ5MuYNsOqqqc/wBgLD1efrxR4SPxFARh61G/ngIViqhcXHUq\nhmHhOJDNrLwDyEhm40RhNtFcW4PugkTz5pojCBcldS0KHuXiCx85joNRcEUsk1q5KIymx1EV93Kv\n31NwPy/TcWrqbnx+SWot2SYIFwt1LQre4pjhXETzFCzTLt8Rr1QUTNtkIjvJzkg3sP6cglkRrK+l\nuQoyeU0Q1k9di0I50excPOEjw5i1NZ3Kr+iYiewUtmOzI7Idn+pbv6dQMdjW0qzmhW0uNtkgQbgI\nqWtR8JZFoXYGtvNRCh3Byj2F0WKSuSvcQdDbQNrI8Mjgz7DX+L5r1lMo/q9Q9BSk0aEgrJo6F4US\nF5EoVHoK6ZWKgluO2hVyRSFRSHL/qe/QE+9bmw0V3kEtzWp2HAcVt7KsVJLqSAhJEFZFXYtCKdFs\nO/ZFE3+e4ykkVxY+msrFAGgPthH0BjFtE4CUkV6TDTXrKTiuIMDsF7t2rBOEi4P6FoXSiOFYZMyL\nI69gVngKmRV6Cplxm+3nDtLgCRL0NpRnNmeNtSWcK4Wg1nIKpWkbHkUpbxMEYeXUtSiUwkcOFomC\nsam2rJRKT2GliWajv4HWiV1kpi0avA3l7WutQqpMNNfSBDYHtxwV3LkKIMlmQVgt9S0KSqkm1WYq\nn9lcY1aIYcwOwtm0saKYuVV0KAoZi6BnVhTW2i3VrMwp1NCoaztOWQxU8RQEYU3UtygU377jWExl\nLxJRqPAUbNshmzm/h2Ob7sCYSRcI+mZFYa2N8ebkFGoo0Ww7s2Igq28Lwtqob1FQS+v42kzn11e7\nf6EoiUK0MQBAZiUhJFMp7xv0BMubM2sNH83JKdTOnbiNU/5Ci6cgCGujrkXB7/GAo+JgMZ03N9uc\nFVEqSW1qDQGQXslcBcu9zJl0geCcnMIGeAo1lFOY4ylITkEQ1kRdi0KwIeCuvubYxPMXR/VRyVNo\nanHv+M83gc2yLRTTXTkinZovCmtNNNdmTsGR6iNBWDd1LQqhYAPYKmCzgtB8TVAqSZ0VheXDR1kz\nh2q7dVbZeZ7CRoSPatdTUMrbBEFYOXUtCs3NjcUlOW0M++L4KMqeQqsrCueb1ZyzcqjWUp7C6sNH\njuNg2g5Bj/t51VL7bBuH0uoS5fCRTF8ThFVxcYyEVWLbthYcRwXFwnIujo+ilFNoLuYUzhc+Suez\nqI4rCtl0AZ/iKz+3lvCR5bjzAUJe95y15ym4j0vzFSwJHwnCqrg4RsIqsaO7C2wFsHFUddPjz2kj\ng2EtH8earT5qQFWV805gS1WU2pqmXa5EAsiZ+VU3xSvNUSiJQi3lFNwZzTJ5TRDWQ9WW49Q07Q7g\nfuBYcdNRXdc/WPH8G4CP466I+aCu6/+ruP1TwM24N6Qf1nX92WrZuK1zmxs+UgxQFDKmRcS3OSuU\nFiyDe578K67pOMjdB9655H6GYaGqCh6vSijiP6+nkMrODRHlc7NVVg4OOTNPyBecf9jSr18UzpC3\nGD6qJU+B2bscSTQLwtqo9gj4qK7rS41w/wC8GRgCHtU07RtAO7Bf1/VbNE27AvgCcEu1jItGI0VR\ncAe2lLF5opAoJEmbGY5N6cvuZxQsvD73Lj0cDTAxklx2reZsbq4o5LJzS2+zZnZVolAqR50NH9XO\noOtUeAqlj0M0QRBWx6aEjzRN2wtM67o+oOu6DTwI/Fzx37cBdF0/AbRomtZYLTtU1YNSCIBqYdsZ\npnMbN4EtbxX4xunvkigkV7R/utixdCYfZyYfX3I/07Dw+d0BOdrUgG07y1YgZfLuc2pR65KZuXmE\n1ba6KImAT1XwqUpNtc52u6S6j0s5BUk0C8LqqPZt8ZWapj0AtAL36Lr+o+L2LmCiYr9x4DJgG/B8\nxfaJ4r6JpV6gpSWEt3jXuhbUXASHCSx7iriyg/b26JrPVclP+57l4YHH2d7azi8ceON59x8yZwev\naSbY376z/HelTZZpEwz5aW+P0tndyJnj7trLS9vtnjfa6ic+XsB05noKgYiyqvecjbs5img4QCCe\nwS7ekm/U57YeHBwCfi/t7VEiMVeMo00h2lsjQG3YuBy1bh+IjRtFLdtYTVE4DdwD3AfsBQ5pmrZP\n1/XFguCLxz6W3l4mFlt7z6L29iieXAgTsKwpeibiTLSs7M7+fAxNuZo3Mj3JxMT5zzk0OVl+/NKA\nzt7AvrKNlcfn8yahSICJiSRen+voDfRNE4r6Fz1vPJ4BGghGfcTHC0xOz/VCRian6FBW/p7Hk66n\nYeZMvEC24IrMSt5jtbEdsAyLiYkkuYz7NZuOpZmwnAWfY61R6/aB2LhR1IqNSwlT1URB1/Uh4N7i\nn2c1TRsFdgDngGFcD6DEjuK2wrzt24GRatkI4M27omDbU0wmV7Y+wUpIFlIAJIr/n4+0MStuvYn+\nRfexbQfTsPEVxSDa5M45SM4sHQLKF9t3NLU2MHo2TTqfhcDs86sOHxWD9F5VwaeqNbMOhe24q0Qo\nCyavSfhIEFZD1XIKmqa9W9O0jxQfdwGduElldF3vBRo1TbtU0zQv8DbgoeK/dxaPuQ4Y1nW9qpIa\ndDw4phfLmiKZ27gBriQKKWNlolBaBU1BoS8xiGUvtKU0m7kypwCQjC89sBvFO/mW1jAAmcLcfVc7\nV8GsyCn4VaVmqo9KY7+n3Dplh9VhAAAgAElEQVTb/b+G8uAXlKfGZ9Bn1raynlDfVDPR/ABwu6Zp\njwPfAd4H3K1p2juKz78P+BrwOHCvruundF1/Anhe07QncKuTfq+K9gGwLVjAzjRi23EK5sb1ukgW\nE8wrTTSXRGFP0y4M22A4Pbpgn/miECl2Sk0sIwqF4jGN0TD+gIfsAlFYbaLZFQGvquLzqBi2UxN3\n46WEskKpdXb9JppN2+G7fRP8aGhqs00RLkKqGT5KAncu8/xjLFJuquv6H1fLpsW4tC3MiXQUGqcx\nWLrqZ7UkC+ni/6sLH13VdgU98T56E/1cEt0xZ5/SbOZSSarX6yEU8S/rKVgFGxUIBgIEw37GjLmV\nSuv1FKA2ylJLJqjiKZA2LRxgprCw829PvBfbcdjXvOfCG7YFOTp5nO3hbtqCLZttyoZR1zOaAW65\n5VXYGbfq1VRiaz6P4zgMpUbKK6EljVL4KL2iWcPpoohc1XYAgHPxhXmF0mzmkqcAbggpncxjLxHG\nsQzXnkDASzjsp2DNzZtkjdV5CmYpp6C4OQWAQg3kFUqf+8KGePWnCmnDFYOMaS0oGf7/jt/H549+\neUUr9gnLM5OP85kjX+J753642aZsKHUvCp0dnYQMt3LHIMZUbm3J5mdGD/PxZz7F0cnjOI5TDh/Z\njk3GOP/deNrMEPQ2sCPShVf1MpoeX7BPWRR8c0XBth3SSyTJS10zfH7Xq7A9cwfw1XoKxiKeQi3k\nFUoWiKcAqQqRjs/zFmbycVJGmnhhySpvYYUk8u5vfDq39pvJWqTuRQGgHRvHVrDMSb7ZO76mu6hT\nsbMAnJg+Rd7KY9izP8aV5BVShTRhXxhVUWltaGYqN71gH8NY3FOApZPNpWkJPr+HUDiArc4dJFbb\nPruUU/CpKv6ip5A3a0AUnHk5hRryFAzLYDK78HpWi7SxuCgUrAIF271LGEotzFkJq6OUB1xp3vBi\nQUQB2OY3cLIRbCdGTyLBc5Orv4vqTQ4AcDbeu6AM9XwVSI7jkDYzhH1u59O2hlZSRpqcOTf+bxSK\nA3KFp9C4jCjYjg2me4n9fi/R5oY5noKqqKtONJdyCt7ijGaAfA3Mal6YUyglmjefH/Yd4s+f+mum\nLpAwVHoKM4XZ4onKsufhVFUrveuCsijkRRS2HJd1t2LFOkGxMPInebB/ksH0/J5BBgPnFv9RZ80c\nY8Vwz3BqlKncbNWHogR4eKTAWNYd4HOWxVdOD3M2MfsDzVsFTNsk4nPLRtsa3KTVfLd0OU9hsQqk\nvJV311JQHDxelabmILZaTFYrHhSU9YWPymsqbP7Qa8/PKczbvpmMpMewHIu+5OAFeb1KT6Ey2Zys\nuDlZrLqtklMvj/L9+49g1cC1rVVKIpuz8uStjZvjtNmIKAC33n4H9sROsFWMzBFylsEX9CGGK4Th\nmcfP8b17jzAxuvCuoD8xiIODV/Xi4HB2prf8XMB/Lf1pH4eGXUE5MpXi+EyaZydmK51KX65ZUWgF\nWBBCMpdINMPinkJ51TWvOzA2Njdge9xBoj24zc13rLH3kVdVy55CLYhCaegvfaFrKXxUyi+NXKCQ\nTcqcFYLK8FG6MHsjcj5bzpwYp//sNLHJtXcM2OqUPAWAeH7r5GhEFIBQOEKH38Ca2IHpy+AdeJGc\nZfOv+hAjGfcOf2zIvejD/TMLju9LuKGjV3Ve6/5dvCNUlAb8visAODGTJm/ZvDRdHCAys6GhUjO8\ncvioWN42lV3cU/BWJpoblxaFnJlHtbyoRVGINjdgqxaq46Ez3F5snZ1d1ZoKZnFfr6KUcwq1IAoL\nPIUaSjSXYs4j6bEL8npzcwqz4aNKT2EkM77oBMnyOYot2eOxtS3ZWg9UisJWyiuIKBTZE4phjO4B\nBzL+MzQne8gWhWE4mWV6wv0CjA65d/gvT6f4xIs9TOYK5XzC6y65DQWF0eKPP+C7GkXx4lNyGLbD\nM+Nxeou9gyZzRjlpmyqLQtFTCC7uKSxWfeTxqoSXmKuQNd2lOFWfO0J6vR4cn41qeWgPbgPcO+y8\ntfxCPZWUcgpfOf41RtPDQG3lFEpdUr3lORSbb1tprsrwBRKFlGHhUxVCXs+c8FHJIw16GzBtk4ns\n0pPbSos3JWZEFJaiVEYOIgpbkjffdhPkg/jj28iFE4wnn2NPKEbGtPjCqeFSvpaRwTiO4/DCVIKk\nYfH0eJy+xACN/ihhX4jucCexXBzw4/dfiW2naXCeA+BHQ1M4QNCj4gBjxaZtpR9rZaIZYKqYU/ju\n11/iJ989sWhOASDaHCSVyFHIz6ssMjJ4bC8eX0VfQY+FYnnYFthWsd/KQ0il8FFP4iy9iV6gRjwF\n5noKLX532dHp/MbNUl8LBcsgVxTdiezknKq0apE2LcJeD81+LzN5s1xNlyqK0/7my4Cl8wq2bZNN\nu5+beApLI+GjLc6uy69gT2Sa+JlriaajZKNxjgx/i8sbHTK2Tbo7hMejkE0bzMSynCve8T8/McJM\nPk5zoIk/+dlf0BxoxMHB4+lCUXzkCy8TSw/QHfRjOu7C8rd1ueGhkWLyufTlKuUUIr4wftXHVHaa\nbKbAYG+M08fHSCfd/eeLwo5dzTgODPXNDW1l8zkUR8Xrn73MtsdCtTxEzebZ/VaRbC55Co5jkci7\nnkwteQqld9rW4IrCZG5zRSFZcQdpOzbjmYll9l4/juMUF4tyRcF0HDLmXI/08paiKCxRgZRJz35m\nIgpLU1nNJZ7CFuVgKA62h66xXTROdWN7LF4YuR/VcUjtjLDvyk4Ajg/EyFk2CpDMu3dbGdNdvaxU\nj25b09h2Dk8iRsbI0BxzvYI90QD7mlyPoJRXKH25nht9gRcnXkZRFFqDrUzlYoyNuHcgjgP9Pe65\nK8NHAJfsdT2L+dVR6bzrAVSKiKkYqLYXfz5U3raauQpGOf9gEs+7nkxtiMJcT8GnqjT7vUxtsiiU\nypMDHneC5HCVk80F28F0HMJeD01Fb6lUlrpQFBa3pXLRploMH83k4zUxYSxlpPEo7m9rK5WliihU\n8MZ3/TdC3gJn4l10Te3EY/gxrTh2bhgz5CVyRRsAp2PuD/2mjiYM021HkXM6ABjPTqKg4JAik/ke\nDXEFR7WZ7j+BYQ6QzR+mM+hHoVIU3B/ri5Mv860z38dxHLY1tJA1swwOzMZ9F2tzAdC5PYo/4KG/\nZ3rOxLtscXa23++2uLJsCxs3p2AkHbyKu31sFXevbvjI/WfarujUQvjImTdPAVxvIW6Ym2pfyVPY\n17wXOH8p6HpJFVtcRHxemgPu9S1VIKWMNAoK3eFOwt4QQ0t4CumKdb9TiXy5EWOt8LmjX+bThz+7\nqTY4jkPayNAZagfYUjPERRQqCEcjXOfvI2P4mDG9dPUfAAUS5iM4jsPJQh6vT2WkODnoyaFDGGYP\nqhJF8WhE/aXkbRi/70osJ0asqx8U6G16kUz2B5yceoL+RD/bGvyMZgvlL1eJyewUg6mRcrJ5YNAV\nBVVVSDSPkmgem1N95D6nsvPSFpLx3Bx3P5tzRScQcO8Y3VpqFdX2kJjJ0R5yX+NU7MyKPyPTdlCw\n6e69ikDa9TZqQRTmd0kF2BZw786nNjGvUEoyay3uoknVrkBKF7+bpZwCzM5VSBXShHxBVEVlT9Mu\nJnPTjGcmF5yj5CmUbj6W68J7obEdm6HkMFO56Tkx/QtNzspjORatDS00eAISPtrK/OLbf57dkRh9\nqSbsVCvBZAsOGTLZhzgRj5M80Ewy6sW2U+QNCzDx+TRU1cO+1tfSFmjB5+2mIfBqOidvp3NAAwcK\noRTgDlLfPPM9ukN+8pZNrGAu+HK/OH6E1uIEtonRJIoCe6/YxuBlLzG49yWcRebpXrK3lXzUx2Nn\nZ3smZfPuHV9DURSeHJ+hKfpbqF2v5wgG3aHtgNuiY6WtPQzbAduibXw3rWPu8aUGbJvJ/BnNUJlX\nqM7EoqdHnueh3kPL7lMaLLaHu4j4wlWfq5Aq3tW7OQX3/ccrwkcRn7s06TXtrwDgxYmjC85R8hS6\ndja5x9dQXmEmH8d03Pe4WH+wC0W6Ig/YGIhKonkr07ZrJ29VTxHxF+jNB2g980p8hTCm1U8q820G\nm3QsJ0Zw2oMaPwcOBH17cByHvrSfWD6On10oikJ0Ikr7yGU0pN0urB61laB/L72JftIFt5yzL5ks\nf7kVFHyqj8MTR9wKJAcSkwWa20JELnWwPRa21+To1PEFvXR27WkldqCZpzDKTf2yOfcOLxhw113Q\nS/Fhb4jBZh+q51LAHSxWGkIybRun6BlEpiJ4FJMz06nzisrLkyfK/aGqwfycAsC2hqKnUKW8wnd7\nfsgDPT9Y0I6kklJOoTEQZXu4i8ncdFVnv1Z6Ck0lTyFvYjs2aSNDpFjh9or2K1EVlRfGj5YXYiqR\nKYpCd1EUEusQBcdxOH18bEFl3FqZrCijHctsniiUy8j9IRr9UdJGZtl5HxcTIgqL8Ir/+ju8o/sI\nDjDseNh79Bb8+RZsJ06+8CypzDeIKQ+QjcQJJ7ZRyPagKAoWUUKBn8fr7UIxbJTij7917FIALHua\nvOHeOb409jgA951+tByP7Ap1cLDtAOOZSRzHwVcIYhvQ1h4hHZkVgX8/8R984plPM5GZ/YH4wn6M\nqHtneGTCPd940j0mGgpj2Q6jWQvLmiI87pbIZop5EGDFA3bBtlFKs5otH35jhumcwcQyA+/p2Fk+\nc+RLfPbIv626rcZKmd8lFWBbyVPILz8IpwwTc5XzGbJmjlh+BgeH4fTSfYRKOYWoP8L2iLvSbGmy\nYzWo9BQiPg8exS3LzZhZHBwi/kjx+TCXN19Gf3KQL3/p0TntLErho+2XrN9T6O+Z5scPnODZn/au\n+RxZ0yJnue+r8ju/mZ5CqjDrKTT53YrD5ApXWax1RBQWIdjWSpfdzR1dZ8mYPqbDWS4/8ioOHH4D\nHaM3EDB3YvjcgbdtbDftx1Uo1voragh8IQIzeaav3o7RYNM004m7MHIB23HvyC17mHzhGLayG3Bj\ntwFPgGs7XLe+LzFAQ8ZdWLu1PUx/enYgyZhZclaOfz32lXLde38qW565dXhkhlhuhkwxp+D3exjO\n5DEdMK1RIl7wz+QZyji0B3cCcGJaX9FnY1g2auUNUcytAjmTWLwdQqqQ5ovHvubOnrZyPD741Ipe\nZ7WU11OoyCm0+H2oLF+WmjEt/vZIH9/rXxhbX46RioTxQHJ4yf0ShRQKChFfmFe2XwW4YadqURKF\nsM+LqijsCDUwkskzlXXFqeQpALyy/SAAw75+RgYq2q6kCnh9Kts63e9fpSg4jkMqsfIcw/iI+7o9\n+sSaug87jsM/nxjgy6dd4a2ccDe6iZ5CZWuaxkDxc6piCMl2nHIRQbURUViCA3f/KjfE0+wLTdCf\niNIXzOJg0+ZT8aaztA1dRiTWjtcIEE2003hqFCwb1euGivyJPGbISyGq4rF8NGZeQbCwB5/vcnze\ny1FQyeWfIJt/BJ93P+BWpuwK7mLviVs482SCBqWBTDhGqMVDT7yvfFcC0BFqZyA5xNdOfoOMkSnP\nm1BMmynF4chELx7LDR/4/F76Uu7zljVGJBgiPOp+qXc33wqAHjtz3h9tbCpNwbJRTQXVvQnHO+IO\nEKfjiyf9vnryP4gXErxp9+to8DTw8MDjFKyND+fMn9EM4FEVWgK+ZcNHfaksedvm5VhyVX2SKss5\nB5NDS+6XNJJEii3R9zXvpa2hlcPjL83pTpu3Cvz47E8xN2BiW7rY9yjidW80tOYwNnAy5g6mpVnz\nAHv9l7khytZRzp2aFcV0Kk84EiivwVEpCs/9rI+v/PNTjI+sbAAs9QpLJfJlgVgN8YLJZM6gP5ml\nYNllUfCpXsbS65/zUbBspnMxvnz83hWvkghzuxCUfpPVTDY/Phrjky/1lhtrVhMRhSVQFIUDH/gQ\nv5g+zcGmEcYzIU7YCmf6g9xoHOCX33Qjr7xyL+O7T5FqnKRp1MeOx0ZpPD5A05k4kfEc2386Srj4\nvd31cgfXj95A2PMaQsHbiYTvwuu9FMsaxbQGAA8FW+Xew/9JKNmCGg8y0dxP3+XPczj3DLH8DHub\nLi0PHG/Z/Xq2h7t4evR5/p8nPs7zE+7AtN9wL+lPB0Cx3IHBH/DQm8ximiMYZi9d3U1Ep/NgO8wY\nrSgo5K3CnLvfxThzchxUFdVSiHR4UUIWkekI24IeepLZBSGYmXyclyaPsadxF3fufTOv3XkLSSPF\nUyPPbeCVcpk/o7nEtgYfadNaMhk+kHIH54xplx+vhMqWFQOppT2FZCFFxB8hlptBVVRu6b6Rgm1w\nePyl8j6PDvyMzz337zw+tH4vquQphEqiUJwTczbhenTRClHIT0M42UomGuN4fw+O45RnM4cibj6m\nqThb3rJsTMPi5ecHcRw4cWTp70rlzcXk2OxA2aOvfhAfKpZt27gl3JPZKfyqj0sbdzGdi63rBuPF\nqQR/fvgs3zv3OE+PPs/Php9e8bGVE04b/a6nUM25Cidn0liOw/MT1U9oiygsg+LxcOUf/Ql37Qny\nhl1nQFVIGEH+syfKfd8cYM9oH29sPkjLZTsJ3+gn0zpF04hKY1+K19x8KW//pVdwx89rdOxw7yRm\nRtN0PzFGaDiNL+OjOX0rAf8NOE4asIA8Ov30HTzC4N6X8Xi2Y/u9PBlzv6ztwVbSpnuHfy7Rzx9c\n/37ese/nCXrDJAw/ijPDwd3uJU37wngbdgPg9an0pXLkC08BFmPmGDfeuIvgVI6JvMmOqBtG+NLx\nry+bLOs96+YoFMvBMi0inR68lp8WxcCwHfoqBtXpXIxvnP4uQHnxoDt2vgav6uWhvkMLlgVdL+V5\nCvO2txWTzePpxe+wBio64Z5cwttZjOHUCAoKXeFORlKji35uhmWQNXPkrRx/9uQnGU2Pc3P39Sgo\nPDn8bHk/vVgS/Mzo4RW//lKkTYugRy33fuoOBYj6PAym3dnulZ7C5FiKtmK+qz+qMzmWKs9mDhdF\nobkthONA35kpjr04TC7riuvZE+OLttXuTWb56PNnOBVPk0kXSCcL7Ly0Ba9PXVMIaaji+gymc0xk\nJ9kWbKMz3IGDs6o5NvN5diKBDZyYOgW4xRArpbKJZTl8VKW5CqbtMFT8/r40vTqPdi2IKJwHRVXp\nfssv8fY3vI6PXPsEv3XTS1zePs1IIsrnn76Grz7RwBMvZHjk2RwnJ5sxL2tBu/kSduxvw9/cwIGD\nXfzyr13H23/1GprbQnhMh7YTM2x/epyul2LsfaqTJuUXCHiuxaO0YttxkqFBCsEMDf7riIbfhcfj\nln6WKo68qpfnxl7k9EwPb9h1O79x1e8BHnLGIF/t/3HZ9kLnbhwF+ifTxPMjmJYbIjgdO8vVN+6g\nMe/+qE3lJsDPUGqE/zj9wKKfQzqZZ2zc/SEotkNXtIfuLneAUKfdAed03BWsjJHhL5/5ew6PHwHg\n2NRJJjJTNAWivG7na4jlZ/hB78MbeJXAWqT6CGaTzWOLiILtOAym8rT4vXgVBX1mZaLgOA7D6VHa\ngq1c1rQb07EWjW9P5dxBIllIYzkWT48+T0tDM1e0Xs65RD8P9R4ia+Y4Gz8HQH9ykC8d+9q6WmGk\nDItwxTwWRVG4vDFErpjgj8wThWisk2a1mXjbMMdO9ZWTzKGIW7F29Q078XgUHv3hKZ585KxbHq1t\nI58z6T87xcRokkMPniw30Ds8mcB24LmJRDl01LWjkd2XtZGYyTE1vrq5BUMV160vlSZvFWgPbaMr\n5BZJrLUCKWmY9Caz2HaORME9R29iYMUhpFRFTqEUPopXKXw0ksmXW+QkDWvOWizVwFvVs28hQh37\n2fv6PyPw/L/T2fUiLyebOFnYTveuAt0tafpGw5yc2MYLZ+GFs1PwVB8Al3f4+eW9k1xy5fXc9d5X\nEY9lOPSgTiKRIZXKoZoqlzztAN1AN4kuh4nd02SdZ8hkfkCw4bUEG96IkTnMy2PnUBU/jhIka6b5\nyrFBfL4c6fwTFJwpXtF+LedSbqtuHAcr6CW1I8yjLw9T2HUMAI/hJ0GSzzzyKIkON8mcdRR2Nv4S\ng4mv89jQk3SGOrjjklvL771gGfzg2SdxiovqBP1ZDmwfIJWbQeeVJF6ME3h1G89MxHFeHGeq7Qxp\nM4NH8dDgDZA2MnzpxPe57ZJfwKt68Kk+Huo7REugidt23gK4ycx0Ms/2XbM9mZYjZaQ5OnGc6ztf\niV1QcBxwHBNFAdsqoBbbSoSSJjseGeahaYPfesOB8sJAABO5Annb5qpomJRpcSqeYSZvkLdt/KpK\nS3F+x3wShRRpI8O+pj3sjLiCPZAcYkeke85+D/a7s91NpwEweHb0Be7c+2b+y5430p8c5Ds9/8lj\nQ09g2CbbQq1MZqZ5duwFzsyc4w+ufz8tDSv7LEqYtkPGtGhv8OHYJigqiqLSMJnDcVxRmBkuQLEX\n4uRYisC2EK/a+Roe6v8eT00+w+5ut0Kq5Cm0toe56fa9PPHwWXIZgz37t3HdLbvp0Sd54akBZqYz\n5HMmtuXwurcd4ERRWE/F0+wrlkRv64rSsi2M3jvN80/28aZfvBJlnngvhuM4DKZztAS8ZAybwbT7\nHrYFW+kKu6Kw1gqk47GUG3C03dBf1BclaSQ5PqVzU/f15z0+VXBnh/efmMFUDBQUTk2foWAV8Be/\nextFKR/4qo4mnh6P8+JUkv1N4fMctXY8H/vYx6p28gtBJlP42FqPDYcDZDIrD2MoikLj9qtp2f86\n2raH6R59hh09Z+hszXL5gSw37Rmlw45RyCq0B1IEPSbn4j6eGfJwtvckLx9+CZ+Z57VvvY4DWpZ9\nl57hwFVeQi2tBENN5HBQJ01aB0M0pKPEW4cx7HMUjKOYjOKoBVAD2HYKFJuCc46C1Y/pjINjMJ2c\nwVZt8rnnyBUOYzsZsq0WqaZpFNvBVwjT2XsJibZRJtMTxON9FNTD2HacAkFCyhUU7BGOTbxMos8k\n3NTFz8ZneKjvEcZemsFnN5K6JEKHMsV+fz9+b4Epr4/8SAt5f4psJML0VIbT9sMoXgcLi5u7rydp\n5BlN9TF+xOHMxAkMNYftsXh56iSpQopd/kv55pcPc+zwMN07m2hsDi57HRzH4XNH/o2HBx/n8MDL\nHH1whpc9zzCV+zGp7BiRkQeJBKJkPGHuf+oBersOo4zkGfS3sicY4Cs/0SnEc8QaHPSZLK/qaKIt\n4OdUPEODR+G+njGeHJ8ha9rsDDfgU+c61L2Jfp4ZPcy1HVezr3kvT448S1tDC1e2aeV90obFN3tO\nUDDPEvBfTdTnZyY/itayj8uaL+XW7a8ibxXKoaPdzTuZzEwXl0jNcmxK52DbFYR8i38WE6NJ/H4v\nngqRe2EywfGZNFe1hAgPfpFc8gxK4AA/e+AEE12j2PY0+f7d3HhgJ9mMwbNP9DF2QzsDhTBW7gRx\n/yR2X5jCtIL2ii7aOtzy1c7tjQwPxEnGc9z2pv107WjkrD7B5FgKy7QJhf2MDiUIXRLlcCqDqoDp\ngGc6T34sTedN23ksm2Gwq4GxgoFnIMHuPW3n/b3F8iaPjcbY3xQi4FEZzRrkC0e5qeta9jbt5uGB\nx4n6I1zXcXX5mJX8po9P6TzQN4JFA9t8p4nlxri6840Mp06jqp4551uKH/YdwrJsko80MXB6Bstj\nMNkwTNbMcnDbFQv2f2LoGfqSg1wS3bHqceenozOM5wrcdVk3J2ZSDKXz3NzZhFddX6AnHA7cs9h2\nEYVVXJwSiqIQbu6i87rXErjhdk4lmhg6nSZgZ7jkkhyv3DPNK3ZPc+3ucVqCOXpnmhhKNzGYjXJ0\nOMsLz7/Ecy8O8cIZlRPDOaazY3h9PfiD4xTCJrY/hMfXSQO7KOQBpYDXaiaq3k4wcDP+0DWoShTH\nmMEhi0/pQPGEsJRpLHsChww4BpY9jmn1YdpDWM44pmeGRFsxQRjIowZToBSw7QkM8zR5WwcKoDr0\nc46nR56lNznCTC5OOjpJui2O4c3iU/qZNBKcMwzGfQmmIhk8k3nyUZt0U4w8IzRkGjB9ebaHdpA8\nFSQdGaUh4WNH3yvYNroXO5ojG0jQlxzksf4nyTkZcBTO9o4y1TjDaG6A4fQoqUKK41Mn6Qp3krcK\neFUvP+x9nCdGnkJFJe2kmO7sJ2VPAT5ihQmeLxgcGjvJjwceZcY/ge01STWNk+/z8mTay2STjzOZ\nLGde+hFZjnOgvZ0rWrt5YmyGwZRbuhvyejibzHIslqJ5pkAmnqOpJYiiKPxs+BnOxnu5bfvN7GvZ\ny0N9h/CoHm7pvrH8HXl6PM6J6bOYVj8+737CwRvJJEYZHjC58dJ9BH0BDm47wDMjh8mYWSYz0/hV\nP6ZjsrvxEoZSI/x0+GkMy6A12IJpWwQ8fhRF4cRLIzz4H0cZ6pvh8oOdqKqC7Tjc2zNK3rK5s3Ua\nK34EszBD/7k0A+f8zLT2YXpTGB2v5sRYimjB4UQiQ6oziINKR6iZWP40w/5eQskWrj14WVmcFUVh\n7+XbOHjtDrZ1RVAUBdtyGDg3za1v2McVr+zm9LExJqYyTLU38NruFvpSOQoZAyXo5WmPRdwwafR6\nSDd4GEzmGH68n4Gz09i2Q1t7eFHP4XQiw8uxFNe3NdLg9dCfymGag7Sf3MXg8RTD4bPkyXFdx9WY\ntoXf41v2N/3SswP88IGjHJr5KWb4Khx7kmz+BQoWNIVeh2OdZTg1yht2vRZVWXrAPRU7w8MDj2M5\nNtGZDpoug+70HsZ8A/Tketge6qI70lne/7nRF/nyyXt5eeoEp2fOcdOuV2LmV5YXcByH7w9MEPCo\nvHFHG3nb5nQiS08iy1UtkQU3LKthKVFQ1lI7XEtMTCTX/Aba26NMTGxcHDBvWCSSCaYmzjA6OEpy\nZIxG7xSNzQYT2QinZrbRk2hlJttwnjM5RJwcESuDFwuvB1AcbFUlh5+UGsaDRbs3SWPY5LqRI6RD\nrRzdf5Ccz+Y1Hp3WaEx4AKIAABRDSURBVIEj0yF6Ez5Mx8AM+nAaI2T8Jo7igGNTamynKAHAg+2k\ncOw0iuIDFExrFFhjmaSjAAqXDN+JlbQZNbL4Cj66HYsGxcZr+Uk1xoi1D5BsGsX2zn0d1WnAwcRR\nzPL5fEYjquklH5wGHHaeuYZ8KMdk9xka0wcwO/ZgGGcwrAEUxYuihPB7LyPoaSKW/k8cwO/X8Nhh\n8EUJT3lpOpsj1WlS6CyghqIUlA4UJUR30CFnpJkywGs0EBkq0GUa0DHOsfTLWD6DaDDMqy+5nsdG\nnyBhJLi8+TL2Nu0lPtzCSV+EtPki+cLzNPgO4nFuZPr5Mey8TVMT3H6rj/3b2/nMkS/RFGgkHAhy\nx/bb+MaZB8gW8vjMAIYvR8W0C4KeBgJqA85okG3Dewnkw3Qc9HH9qy/l5RmFx0cGCCvn6LROs99j\noyoKKcum7/gV9Hf1kW1I0pG7i/y2ABZJVCWConqI+jwkDItL8z28lH8YFLdZYnuojYgvgkdV2d+0\nhxsuPYi3ECTiC2PZNtl0geamCGnD5N8ffI4++xThTDev3b6Xx/wmKcfEVuOgTHFje4icGWcg1caM\n1YzX8BMZTBMay9LVGGTb/hZ0bwpP1iDaYxKNBJnZ18Rx2+C3Lt9B2rL4+tlRvKM9dB8L4CgwowWZ\n6chgOVNY1jivaG3hA7e+m2zczZXN5AucS8bYEQ7Qe2yc5348hILCzJ4Iyb1NZDI/xbBOEPLtwtfw\nZlqSP6OX49ygvppfu/XOBT3GAMbS43zq8GdmJ6o5CigOrb5WWsYv4WzTS6DCFeEreMeVb8HjCfDx\np/8ay5ktRIj4QtyuvpmIEqU3fILs/9/euQdZdtR1/NN9HvfeeexONlmS7OaxhMAvhgCSSBIrEMLD\n0hisFEkUq3gkCkYIUCioVSqPoFZhoVRAQMsXoVAskBIxiEEMkAhoMGpVEIUOCQlJ9pnd2dmZuY/z\n6G7/6DN3787O7CZLZu+ktj9VM/fcPn3P+Z4+5/Sv3z/V45LTLuTZJ5/HYtFjb7fL7hr6dcV9s3ez\nvbwQ5bZzZvsHXHz6RXxv4WTunV1kczvjpvPPopUcm2HYvHl6xTa8aBSeRKOwGnOLBWVlsXWf3mPf\nZfaR+1icm8WnHiYzKhIODNrs67ZZLHK6ZcaexUkG9cpdPgpH6HY6/J7mumayVTGR1yTakWqH9mEW\nsvYOXVnqvmfgc9zMFDZvkfuapLb0sw511kKloFIopzuQ7wercXWKUgM26j2crXfy6PdPY0PmOe+c\nR5l3lod7KUWR0e2cQrc1T8UjaHsmdvuPMdh1aMdYvikjm26RtlISwLc8rrMHr2dBV3i/gHV78b4i\nU1tRepLKPThsFwfIFi+mfmwrWQJZqrCTOT5L0O2EiUFFe7Yg7zsWt0KxeSO120WvfwdhlNdRcBqU\nOyR5FTmJ6+B9iU0O6lAuRZGDcjg1GIbpdAZf93C6x6nfv5Tt+0+isp5spkU1V6AUTOUJk50UtTGD\nDS2ywpItlGSlw6UJ3TMzuu0HqOr7cO7AcpWrkDXXePjIoMS22bLzKhYGltmiwmrNTEexuajYf9FW\nlHX4+fsp/P3USZdaL4R0OALatdB0cPRxqkS7nMQFX+BWD1gtvZVTJHWLtJog8R1QGq8ceNAu9A8V\n0x6fprT317jEY2c2Q9VDD7rYdhufgKcGb0FpNB105VBliUssVTaHpxw5Z4qmg08U3lfheVIws/Bc\n/JZLsNVjLPY/D8qSlyeRldNh9r6yWO1QKqPM56nTA0z0tpD3ppiffhjXWmkoc0KiZkjKDGVrfKLD\nO1jVJHWGVh1s6qjyAV470qqDpo1tt/B5KJQBUFvSQY0qS3RV49QAlxTYzOOThOuf8yp+dMu2x/Ng\nHEY0CitwvIzCajjv2T3bZcfOh5hMZtmQHcAV+xkszqGqLnNdRd+lWJ1AorFeMZFVbJ7qUdQpD+yb\nYffCJAtFTq/MqJ1mUCd0eyn9OsOqw0s6xw1dg9fgNZ22Z9sZBSUZj+zIKHtHymg8iSZM8lYaZwHn\n0YkjmVqEvMLWYGc3sZJRPNJxVVLS6syRTcxBVlN1cnxSg6qABK1baN9D6QVwmrSYwAPVRI3PCkj6\nQIKqZ8DnoAu8KkAXgEK5DUAGeh6vFkFZdJXRv/dynMvYnCsmfmSGuUqx+OA8tn+UWpgK7laTVKGm\n+5B5lPaozl5o7wY0uDYoj3IlSlX4tMarBEU77Fc13tY4W2P3nkq96+kr3y4Nup2iWwk606AVKqmh\nswPS/SgXbid5AbpCKQ04fNIF1QffRvk2Xg1ADYAc5TsotwntN4cb6iu8XsTrebzqA3286gGrvcLq\nCPtGb23zHKhD42o1RV7M4PIUlzicXwiGwGvSQpGXOdM+48ydz2DfzMnsO2MKl0O/uJu6fmjV06Xp\nNibaL2+a0SyOBZzbg/d9vK9xbh5Xz2KZYyXjfMzXedivJnm2v5g3vezKJ/xbGJNREJEO8G3gd40x\nHx8Jvxp4J1AAnzLGfERErgA+A/xvE+1/jDFvPdo5nspG4fGwpNG5knqwn6rYhy3nqKoeRdGnriuc\nK7HW0S8UCwuaxbkM7QfYqTbgyeb3w2KXyofMNW87dK6wKJxWWK1xWuPQ1C4Yn7LW9MqMXpXRSi2d\nrKYuoZh1uO2DUONQmlJnlEmGncyxWYpN0lDTSBSbN/Y5//R9nDGzcMiaRAtFxq75SRaKFpXVDOqU\nbpmxWGR0y5x+lVJZjUfRSiyJdpQ2oagTSpugled5W/bw3C17mCsm2NOfouMHeBQH+i0ODHKs0ziv\ncF5hm8/aavb32vTrQ0cVJcph/RqMzm5K2dN5xRXnPszzT9uFPVDTtW0eSbcy18/ZdyCDfkVZQte3\nGahWqAV6oKxC2tgcyw9n4FNlmc5LMu3YkBc8c2qWmXqeBxZm2FHP0PUtujandCuPuFozlEO1mpqk\n0yF/1BZsii9boB2q1Q/VB6fBabxLgoVyGlxC+JGFrESlFaDwdYaqWhzMbMMDGFp7mnr2SGVbseRR\nUIWwvDmn16G/w4NKapSqob8BrxKsUxycnnIwG1KEc4BDTfSDsSpbgA6FncSiVIX2IVx58O0ClZZo\nH2p5yod1vHwKPkkBj0eDa+FdC2Vz0hTecPnTeM55FxxT0q9mFNZ6SOo7gUOW8xQRDXwEuBDYB9wu\nIp9rdt9ljLlujTU9JdE6J584lXzi1KNHPkacq/C2AJ2jCCVB5yqc7eGqHipp4/o9vK1ROPTEJDpt\nsWE6Yc+uXSx0F+j2usxMWBLlccUEvpjElyWudHjrcR6mbM1ZSUmdW2pbYjMHHY93YT/Kk+oarSzg\ncQ6qHtQlpC1Pkoe4ah9szbqc3dqDB5yF/qKiuwDWKXSqyCahNQV525HlHp1CpVNIFFniSBNHoqGy\nirmiTWFTaq+pbULlNKl2TGQVWnkKm2CdbibKhWxkads2Rqeymso1GUbfkS/0uSB/lPa8x+U5yUzK\nRl2zMfkBapOCrUe/L95DaRNqp7BOY53C+ubTqaYpMVxz5RrtNhhFD8x0Cs7YuECiDy8/CT3g4Izs\nyioGdTpynmBQE+XxwKBKGdRpKDyM6PE0w4JRzZIjS99Htj3BODcFD+tCIaR2mtqGT6U8adPsmepg\nrIs6pLvCQ5Nnq6ZW4JyisAm1DUbdL/1vFdA0HR1y1X7ZZt+Ge5hoVN5cB8E4eK9xTfqH5yvDk4fa\nGJY8t3SyKjzrw2tUwzRY2vZe41t1E2cpXhbSI2l69qo2vuyE8/iDabl0zKXjj9aO86Rmx3/ce8xG\nYTXWzCiIyHnA+cAXlu06BZgzJsx5F5EvAy8HHlorLZHHh9YZw0WNlsKSHLJJWOobnzj8d1Mz0/Sr\nTUwffZTh2HgitcJQew6d8d678En4dK5pr3c1zltwIYPwzjZNKuGl1WmHrL05NDG4Cm8HeO/w3obj\nOYdOWygdRhRt2tThsT2z2GoenMV7i7Mlzi1N3vKE5f6ScG5b4p1t4objeVfjXE1tPVUNCov2JWiw\nqQ5LVQwq6tpT6RTvExQe7RzY0FellCPBorUnUQfTwnnQiUZ7T0urJuPyKOXReJRy4feAVh6lFUrp\nJo0qlPLBsGhCLOvANouTKJqc16G8Q3mPI6FSGTiFtnXoF1MetAflgz4dPlPdeAK0wUi4ZiEshx4x\n3EGXXtLhFLO7pynaLbx1+MqifE2rU6ESG5orU0hS1zwTKlyrZnj+6Y4lzRVNRSIw0oS1ZKCGHmx9\nWKPLDzP45jEbPncj34PbEsq+wpaKrA1JyzdDQ5p/XvHMy9/4uJ7pJ8KaNR+JyBeAtwDXAw8tNR+J\niAIeBH6CYAhuA+4Evgn8MXA/sAl4rzHmX452nrq2Pk3H2HYeiUQiT02OX/ORiLwO+HdjzIMicsg+\nY4wXkeuBjwEHCAZCAd8D3gv8LXAO8FUROdcYc8SJBPv3H/uU76dSn8J6Jmr84Vnv+iBqfLJYLxo3\nb55eMXytmo+uAs4RkVcAZwCFiDxqjLkDwBhzF/AiABF5H6EmsR34dPP7B0RkF6HF9cE10hiJRCKR\nZayJUTDGvGppW0RuJmT6d4yE3U5oVuoCPwN8QEReDZxujPlDETkNOBVYfaH6SCQSiTzpHLdVUkXk\nBhF5ZfP1z4EvAV8H3meM2UvoW3ixiHwN+AfgTUdrOopEIpHIk8uar5JqjLl5hbDPAp9dFrZAqDVE\nIpFIZExEfwqRSCQSGRKNQiQSiUSGRKMQiUQikSFP+QXxIpFIJPLkEWsKkUgkEhkSjUIkEolEhkSj\nEIlEIpEh0ShEIpFIZEg0CpFIJBIZEo1CJBKJRIZEoxCJRCKRIWu+9tF6RURuAS4l+DB6mzHmnjFL\nAkBE3k9YVjwF3gfcA/wVwbvrTuC1xphi9SOsPaO+t4Evs/70vRr4DaAG3g18i3WkUUSmgE8AJwEt\ngh+RXcCfEJ7Hbxlj3jRGfRcQFqW8pfGffiYrpF+Tzr9C8E7/Z8aYvxyjvluBDKiA1xhjdo1L30oa\nR8J/EviiMUY138emcTVOyJqCiLwYeKYx5seB1wN/NGZJAIjIS4ALGl0/BXwQ+B3go8aYFxG80v3i\nGCUuMep7e13pE5GTgfcALwReAVzNOtMI3AAYY8xLgOuADxHu9duMMZcBG0XkynEIE5FJ4MMEY7/E\nYenXxHs3wZXuFcCvisimMen7PUKG+mLg74G3j0vfETQiIm3gNwmGlXFqPBInpFEAXgZ8DsAY8x3g\nJBHZMF5JAPwr8LPN9hwwSXhYbmvCPk94gMbGCr63r2Ad6WvOf4cxZsEYs9MYcyPrT+NeYMmj9UkE\nA/v0kdrqODUWwE8DO0bCruDw9LsEuMcYc8AY0we+AVw2Jn03AX/XbD9GSNtx6VtNI8BvAR8FllwC\njFPjqpyoRuE0wsOzxGNN2FgxxlhjTLf5+nrgn4DJkaaOPcDpYxF3kA8Abx/5vt70bQMmROQ2Efma\niLyMdabRGPMp4CwRuZ9QEPg1YP9IlLFpNMbUTQY1ykrpt/wdOi6aV9JnjOkaY6yIJMCbgb8Zl77V\nNIrIs4DnGWM+MxI8No1H4kQ1CstZ0YH1uBCRqwlG4S3Ldo1V56jv7VWirId0VISS4jWEZppbOVTX\n2DWKyGuAh40x5wIvBf56WZSxazwCq2kb97OZEPo9vmKM+fIKUcadprdwaGFqJcatEThxjcIODq0Z\nbKFp5xs3TUfUbwNXGmMOAItNxy4En9XLq6THk6uAq0XkbuANwLtYX/oAdgP/1pTWHgAWgIV1pvEy\n4J8BjDH3Ah3glJH960HjKCvd4+Xv0Lg13wp8zxjz3ub7utEnIluB84BPNu/O6SJyF+tI4ygnqlH4\nEqGDDxG5ENjReH4bKyKyEfgD4BXGmKWO3DuAa5vta4EvjkMbBN/bxpgXGGMuBf6CMPpo3ehr+BLw\nUhHRTafzFOtP4/2E9mRE5GyC4fqOiLyw2X8N49c4ykrp903gBSIy04ymugz42jjENSN4SmPMe0aC\n140+Y8x2Y8wzjDGXNu/OzqZTfN1oHOWEXTpbRH4fuJwwFOzNTYltrIjIjcDNwH0jwdcTMuA28APg\nF4wx1fFXdygicjPwEKHE+wnWkT4R+WVC8xuEkSn3sI40NhnAx4BTCUOP30UYkvqnhILaN40xR2tq\nWCttFxH6jbYRhnduB14NfJxl6Sci1wG/ThhG+2FjzCfHpO9pwACYb6L9nzHmpnHoO4LGa5YKeiLy\nkDFmW7M9Fo1H4oQ1CpFIJBI5nBO1+SgSiUQiKxCNQiQSiUSGRKMQiUQikSHRKEQikUhkSDQKkUgk\nEhkSjUIkMkZE5AYRWT6jORIZG9EoRCKRSGRInKcQiTwOROStwM8RJpt9F3g/8I/A7cDzmmg/b4zZ\nLiJXEZZE7jV/NzbhlxCWyC4JK6O+jjBD+BrCxKvzCZPDrjHGxBczMhZiTSESOQoicjHwSuDyxtfF\nHGH56HOAWxs/A3cC7xCRCcIM9Gsbfwm3E2ZVQ1j47peaJQ7uIqwlBfBs4EbgIuAC4MLjcV2RyEqc\nsJ7XIpEnwBXAucBXRQSCn4utwD5jzH81cb5B8KD1LGC3MebRJvxO4I0icgowY4z5NoAx5oMQ+hQI\na+r3mu/bgZm1v6RIZGWiUYhEjk4B3GaMGS5lLiLbgP8eiaMI69csb/YZDV+tZl6v8JtIZCzE5qNI\n5Oh8A7iyWcgOEbmJ4AzlJBF5fhPnhQRf0PcBTxORs5rwlwN3G2P2AXtF5AXNMd7RHCcSWVdEoxCJ\nHAVjzH8S3CjeKSJfJzQnHSCsfnmDiHyFsOzxLY3HrdcDnxaROwmuX9/ZHOq1wIeatfQv53DnOpHI\n2ImjjyKRY6BpPvq6MeaMcWuJRJ5MYk0hEolEIkNiTSESiUQiQ2JNIRKJRCJDolGIRCKRyJBoFCKR\nSCQyJBqFSCQSiQyJRiESiUQiQ/4f9vjcMal1Gv0AAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7fc9af9b2978>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"t4uDUpjMV8lm","colab_type":"text"},"cell_type":"markdown","source":["# **RCAE-MNIST 2_Vs_all**"]},{"metadata":{"id":"sRLgcDCmWBM7","colab_type":"code","outputId":"21b60028-a103-41ca-c56d-487136b2743c","executionInfo":{"status":"ok","timestamp":1541369113470,"user_tz":-660,"elapsed":2690900,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":65917}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-3-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 6s 1ms/step - loss: 5.0781 - val_loss: 5.1985\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 5.0037 - val_loss: 5.0224\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9880 - val_loss: 4.9956\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9806 - val_loss: 4.9853\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9758 - val_loss: 4.9797\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9724 - val_loss: 4.9775\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9702 - val_loss: 4.9760\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9688 - val_loss: 4.9753\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9672 - val_loss: 4.9732\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9663 - val_loss: 4.9711\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9650 - val_loss: 4.9713\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9641 - val_loss: 4.9713\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9633 - val_loss: 4.9715\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9624 - val_loss: 4.9681\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9616 - val_loss: 4.9702\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9609 - val_loss: 4.9677\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9605 - val_loss: 4.9670\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9600 - val_loss: 4.9641\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9595 - val_loss: 4.9650\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9592 - val_loss: 4.9646\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9587 - val_loss: 4.9658\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9582 - val_loss: 4.9636\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9582 - val_loss: 4.9627\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9578 - val_loss: 4.9633\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9574 - val_loss: 4.9624\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9571 - val_loss: 4.9619\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9571 - val_loss: 4.9629\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9568 - val_loss: 4.9612\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9566 - val_loss: 4.9603\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9565 - val_loss: 4.9612\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9598\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9563 - val_loss: 4.9597\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9591\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9571 - val_loss: 4.9606\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9565 - val_loss: 4.9592\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 286us/step - loss: 4.9559 - val_loss: 4.9586\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9559 - val_loss: 4.9588\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9559 - val_loss: 4.9596\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9556 - val_loss: 4.9588\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9588\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9553 - val_loss: 4.9583\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9555 - val_loss: 4.9597\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9569 - val_loss: 4.9635\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9558 - val_loss: 4.9600\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9574 - val_loss: 4.9835\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9563 - val_loss: 4.9716\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9556 - val_loss: 4.9615\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9586\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9547 - val_loss: 4.9575\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9548 - val_loss: 4.9571\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 286us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9574\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9570\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9536 - val_loss: 4.9616\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9562\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747601 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.99403524\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9818694658063495\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   56\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 4s 715us/step - loss: 5.0687 - val_loss: 5.1799\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9974 - val_loss: 5.0306\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9844 - val_loss: 4.9949\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9779 - val_loss: 4.9849\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9740 - val_loss: 4.9805\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9714 - val_loss: 4.9786\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9694 - val_loss: 4.9744\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9684 - val_loss: 4.9747\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9674 - val_loss: 4.9724\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9664 - val_loss: 4.9726\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9653 - val_loss: 4.9701\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9650 - val_loss: 4.9712\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9643 - val_loss: 4.9695\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9638 - val_loss: 4.9699\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9635 - val_loss: 4.9685\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9628 - val_loss: 4.9671\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9623 - val_loss: 4.9659\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9616 - val_loss: 4.9658\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9613 - val_loss: 4.9656\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9603 - val_loss: 4.9651\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9596 - val_loss: 4.9653\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9589 - val_loss: 4.9642\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9584 - val_loss: 4.9634\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9580 - val_loss: 4.9613\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9574 - val_loss: 4.9626\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9572 - val_loss: 4.9619\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9567 - val_loss: 4.9608\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9563 - val_loss: 4.9609\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9562 - val_loss: 4.9604\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9562 - val_loss: 4.9608\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9557 - val_loss: 4.9595\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9554 - val_loss: 4.9588\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9554 - val_loss: 4.9599\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9586\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9555 - val_loss: 4.9599\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9587\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9587\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9548 - val_loss: 4.9585\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9596\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9543 - val_loss: 4.9574\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9574\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9581\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9536 - val_loss: 4.9661\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9556\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9531 - val_loss: 4.9556\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9531 - val_loss: 4.9557\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9559\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9529 - val_loss: 4.9560\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9526 - val_loss: 4.9554\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9525 - val_loss: 4.9551\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9524 - val_loss: 4.9559\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9524 - val_loss: 4.9561\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9529 - val_loss: 4.9561\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9526 - val_loss: 4.9559\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9524 - val_loss: 4.9560\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9524 - val_loss: 4.9555\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9555\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9523 - val_loss: 4.9556\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9554\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9554\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9522 - val_loss: 4.9551\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9522 - val_loss: 4.9557\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9523 - val_loss: 4.9553\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9552\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9552\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9522 - val_loss: 4.9554\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9521 - val_loss: 4.9553\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9521 - val_loss: 4.9556\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9586\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9530 - val_loss: 4.9569\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9525 - val_loss: 4.9557\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9522 - val_loss: 4.9553\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9524 - val_loss: 4.9552\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9523 - val_loss: 4.9551\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9523 - val_loss: 4.9552\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9551\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9521 - val_loss: 4.9552\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747412 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -0.9999999\n","The max value of N 0.9989035\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9796141008357286\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   81\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 4s 796us/step - loss: 5.0674 - val_loss: 5.1964\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9994 - val_loss: 5.0226\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9859 - val_loss: 4.9913\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9789 - val_loss: 4.9825\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9749 - val_loss: 4.9804\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9724 - val_loss: 4.9756\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9708 - val_loss: 4.9796\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9689 - val_loss: 4.9745\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9679 - val_loss: 4.9740\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9667 - val_loss: 4.9748\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9657 - val_loss: 4.9728\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9651 - val_loss: 4.9713\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9643 - val_loss: 4.9726\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9634 - val_loss: 4.9732\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9625 - val_loss: 4.9729\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9617 - val_loss: 4.9713\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9614 - val_loss: 4.9689\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9607 - val_loss: 4.9682\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9602 - val_loss: 4.9663\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9595 - val_loss: 4.9647\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9589 - val_loss: 4.9631\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9585 - val_loss: 4.9635\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9583 - val_loss: 4.9632\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9577 - val_loss: 4.9625\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9576 - val_loss: 4.9621\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9574 - val_loss: 4.9630\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9569 - val_loss: 4.9615\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9567 - val_loss: 4.9616\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9566 - val_loss: 4.9613\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9566 - val_loss: 4.9618\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9564 - val_loss: 4.9612\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9560 - val_loss: 4.9612\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9598\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9561 - val_loss: 4.9594\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9558 - val_loss: 4.9598\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9557 - val_loss: 4.9594\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9557 - val_loss: 4.9585\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9556 - val_loss: 4.9589\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9591\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9551 - val_loss: 4.9588\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9581\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9565 - val_loss: 4.9606\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9554 - val_loss: 4.9590\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9552 - val_loss: 4.9586\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9581\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9549 - val_loss: 4.9570\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9557 - val_loss: 4.9575\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 287us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9570\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9569\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9555 - val_loss: 4.9720\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9567\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9608\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9585 - val_loss: 4.9932\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9562 - val_loss: 4.9666\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9553 - val_loss: 4.9612\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9550 - val_loss: 4.9593\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9547 - val_loss: 4.9581\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9549 - val_loss: 4.9579\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9541 - val_loss: 4.9565\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9538 - val_loss: 4.9567\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9564\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9537 - val_loss: 4.9565\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9562\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9532 - val_loss: 4.9565\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747351 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9996891\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9213137642267462\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   67\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 5s 918us/step - loss: 5.0714 - val_loss: 5.1144\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9980 - val_loss: 5.0182\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9839 - val_loss: 4.9924\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9774 - val_loss: 4.9831\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9736 - val_loss: 4.9782\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9712 - val_loss: 4.9754\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9695 - val_loss: 4.9733\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9684 - val_loss: 4.9733\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9673 - val_loss: 4.9724\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 288us/step - loss: 4.9664 - val_loss: 4.9704\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9655 - val_loss: 4.9716\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9649 - val_loss: 4.9703\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9644 - val_loss: 4.9726\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9637 - val_loss: 4.9702\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9629 - val_loss: 4.9681\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9622 - val_loss: 4.9665\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9614 - val_loss: 4.9660\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9608 - val_loss: 4.9649\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9600 - val_loss: 4.9646\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9591 - val_loss: 4.9638\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9587 - val_loss: 4.9635\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9585 - val_loss: 4.9631\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9579 - val_loss: 4.9634\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9576 - val_loss: 4.9620\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9572 - val_loss: 4.9622\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9569 - val_loss: 4.9612\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9566 - val_loss: 4.9618\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9565 - val_loss: 4.9616\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9564 - val_loss: 4.9602\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9561 - val_loss: 4.9612\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9605\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9559 - val_loss: 4.9592\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9558 - val_loss: 4.9598\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9554 - val_loss: 4.9587\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9555 - val_loss: 4.9593\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9591\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9552 - val_loss: 4.9584\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9551 - val_loss: 4.9589\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9552 - val_loss: 4.9590\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9551 - val_loss: 4.9587\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9547 - val_loss: 4.9582\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9545 - val_loss: 4.9578\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9543 - val_loss: 4.9583\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9542 - val_loss: 4.9578\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9582\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9576\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9537 - val_loss: 4.9571\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 289us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9569\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9568\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9566\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9568\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9567\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9529 - val_loss: 4.9565\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9528 - val_loss: 4.9565\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9576\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9528 - val_loss: 4.9563\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9567\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9528 - val_loss: 4.9564\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9561\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9526 - val_loss: 4.9563\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9527 - val_loss: 4.9564\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9528 - val_loss: 4.9566\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9527 - val_loss: 4.9567\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9527 - val_loss: 4.9565\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9527 - val_loss: 4.9562\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747902 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -0.993473\n","The max value of N 0.99796695\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9807870036713272\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   33\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 6s 1ms/step - loss: 5.0774 - val_loss: 5.2047\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 5.0042 - val_loss: 5.0171\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9877 - val_loss: 4.9935\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9798 - val_loss: 4.9855\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9752 - val_loss: 4.9794\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9723 - val_loss: 4.9775\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9706 - val_loss: 4.9741\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9690 - val_loss: 4.9735\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9679 - val_loss: 4.9728\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9669 - val_loss: 4.9707\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9664 - val_loss: 4.9697\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9652 - val_loss: 4.9716\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9645 - val_loss: 4.9694\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9635 - val_loss: 4.9687\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9623 - val_loss: 4.9687\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9616 - val_loss: 4.9677\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9608 - val_loss: 4.9668\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9604 - val_loss: 4.9653\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9595 - val_loss: 4.9648\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9593 - val_loss: 4.9647\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9587 - val_loss: 4.9657\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9584 - val_loss: 4.9654\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9583 - val_loss: 4.9646\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9577 - val_loss: 4.9634\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9572 - val_loss: 4.9618\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9571 - val_loss: 4.9624\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9619\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9569 - val_loss: 4.9615\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9569 - val_loss: 4.9617\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9569 - val_loss: 4.9621\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9566 - val_loss: 4.9600\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9563 - val_loss: 4.9603\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9562 - val_loss: 4.9600\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9561 - val_loss: 4.9596\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9599\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9557 - val_loss: 4.9599\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9558 - val_loss: 4.9607\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9595\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9596\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9553 - val_loss: 4.9589\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9554 - val_loss: 4.9607\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9591\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9551 - val_loss: 4.9591\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9581\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9587\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9549 - val_loss: 4.9585\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9582\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9547 - val_loss: 4.9578\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9581\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9554 - val_loss: 4.9586\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9548 - val_loss: 4.9577\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9548 - val_loss: 4.9583\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9580\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9545 - val_loss: 4.9582\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9544 - val_loss: 4.9572\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9543 - val_loss: 4.9570\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9544 - val_loss: 4.9576\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9577\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9573\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9542 - val_loss: 4.9581\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9623\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9573\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9540 - val_loss: 4.9580\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9575\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9575\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 290us/step - loss: 4.9538 - val_loss: 4.9582\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9575\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 292us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9537 - val_loss: 4.9570\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9572\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9569\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9571\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9575\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9571\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9532 - val_loss: 4.9571\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9569\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9571\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9570\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9571\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9565\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9530 - val_loss: 4.9564\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9564\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9531 - val_loss: 4.9567\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9531 - val_loss: 4.9570\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9565\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9566\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9563\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9588\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9572\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9570\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747673 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -0.9975274\n","The max value of N 0.99681276\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9885422937457429\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   25\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 7s 1ms/step - loss: 5.0696 - val_loss: 5.1787\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 5.0014 - val_loss: 5.0331\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9868 - val_loss: 4.9950\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9795 - val_loss: 4.9865\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9756 - val_loss: 4.9810\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9726 - val_loss: 4.9786\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9704 - val_loss: 4.9742\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9686 - val_loss: 4.9749\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9673 - val_loss: 4.9728\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9661 - val_loss: 4.9700\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9648 - val_loss: 4.9690\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9637 - val_loss: 4.9695\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9629 - val_loss: 4.9693\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9622 - val_loss: 4.9677\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9617 - val_loss: 4.9674\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9611 - val_loss: 4.9661\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9605 - val_loss: 4.9658\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9598 - val_loss: 4.9646\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9595 - val_loss: 4.9659\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9594 - val_loss: 4.9682\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9588 - val_loss: 4.9648\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9587 - val_loss: 4.9661\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9582 - val_loss: 4.9642\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9581 - val_loss: 4.9643\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9580 - val_loss: 4.9643\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9578 - val_loss: 4.9630\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9576 - val_loss: 4.9617\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9636\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9574 - val_loss: 4.9616\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9569 - val_loss: 4.9611\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9567 - val_loss: 4.9622\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9566 - val_loss: 4.9622\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9567 - val_loss: 4.9618\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9565 - val_loss: 4.9620\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9565 - val_loss: 4.9618\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9600\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9599\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9562 - val_loss: 4.9597\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9608\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9591\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9592\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9563 - val_loss: 4.9691\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9561 - val_loss: 4.9596\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9562 - val_loss: 4.9596\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 291us/step - loss: 4.9560 - val_loss: 4.9589\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9592\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9556 - val_loss: 4.9584\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9555 - val_loss: 4.9591\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9555 - val_loss: 4.9581\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9554 - val_loss: 4.9580\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9553 - val_loss: 4.9575\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9552 - val_loss: 4.9572\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9551 - val_loss: 4.9580\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9574\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9577\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9553 - val_loss: 4.9585\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9579\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9597\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9570 - val_loss: 4.9617\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9581\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9580\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9573\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9556 - val_loss: 4.9576\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9552 - val_loss: 4.9573\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9571\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9574\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9576\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9570\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9546 - val_loss: 4.9573\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9546 - val_loss: 4.9578\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9546 - val_loss: 4.9570\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9546 - val_loss: 4.9571\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9605\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9588\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9548 - val_loss: 4.9587\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9572\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9544 - val_loss: 4.9569\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9576\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9568\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9542 - val_loss: 4.9571\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9573\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9542 - val_loss: 4.9569\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9541 - val_loss: 4.9571\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9567\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9573\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9574\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9569\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9572\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9574\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9596\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9539 - val_loss: 4.9566\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9569\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9538 - val_loss: 4.9566\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747603 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9989847\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9508991784027607\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   90\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 7s 1ms/step - loss: 5.0745 - val_loss: 5.1811\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9994 - val_loss: 5.0261\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9844 - val_loss: 4.9956\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9776 - val_loss: 4.9863\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9739 - val_loss: 4.9814\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9710 - val_loss: 4.9775\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9690 - val_loss: 4.9740\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9673 - val_loss: 4.9738\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9660 - val_loss: 4.9729\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9651 - val_loss: 4.9707\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9640 - val_loss: 4.9719\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9632 - val_loss: 4.9685\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9622 - val_loss: 4.9677\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9614 - val_loss: 4.9687\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9605 - val_loss: 4.9668\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9601 - val_loss: 4.9673\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9596 - val_loss: 4.9643\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9590 - val_loss: 4.9644\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9584 - val_loss: 4.9638\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9581 - val_loss: 4.9636\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9576 - val_loss: 4.9619\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9571 - val_loss: 4.9617\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9570 - val_loss: 4.9606\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9567 - val_loss: 4.9611\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9565 - val_loss: 4.9610\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9563 - val_loss: 4.9605\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9561 - val_loss: 4.9601\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9558 - val_loss: 4.9599\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9556 - val_loss: 4.9590\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9699\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9578 - val_loss: 4.9687\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9612\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9603\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9555 - val_loss: 4.9591\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9555 - val_loss: 4.9576\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9551 - val_loss: 4.9572\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9549 - val_loss: 4.9574\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9575\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9567\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9567\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9575\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9559 - val_loss: 4.9585\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9571\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9543 - val_loss: 4.9569\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9563\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9560\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9542 - val_loss: 4.9561\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9561\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9541 - val_loss: 4.9562\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9540 - val_loss: 4.9564\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9559\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9572\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9573\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9560\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9538 - val_loss: 4.9559\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9534 - val_loss: 4.9560\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9539 - val_loss: 4.9565\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9564\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9554\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9556\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9557\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9555\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9555\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9556\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9554\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9554\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9554\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9555\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9553\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9529 - val_loss: 4.9556\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9555\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9558\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9593\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9555\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9556\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9553\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9527 - val_loss: 4.9555\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9528 - val_loss: 4.9561\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9528 - val_loss: 4.9554\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9528 - val_loss: 4.9557\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9528 - val_loss: 4.9553\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9552\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9527 - val_loss: 4.9554\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9551\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9526 - val_loss: 4.9556\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9526 - val_loss: 4.9557\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9553\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9526 - val_loss: 4.9558\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9529 - val_loss: 4.9557\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9555\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9525 - val_loss: 4.9555\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9525 - val_loss: 4.9560\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9553\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9526 - val_loss: 4.9560\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9525 - val_loss: 4.9554\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9524 - val_loss: 4.9553\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9524 - val_loss: 4.9552\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747488 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9996426\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9583944514630197\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   77\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 8s 1ms/step - loss: 5.0624 - val_loss: 5.0910\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9994 - val_loss: 5.0135\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9862 - val_loss: 4.9932\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9793 - val_loss: 4.9819\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9750 - val_loss: 4.9784\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9717 - val_loss: 4.9749\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9693 - val_loss: 4.9721\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9674 - val_loss: 4.9711\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9664 - val_loss: 4.9696\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9645 - val_loss: 4.9693\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9632 - val_loss: 4.9682\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9630 - val_loss: 4.9672\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9636 - val_loss: 4.9694\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9616 - val_loss: 4.9698\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9605 - val_loss: 4.9681\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9596 - val_loss: 4.9647\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9593 - val_loss: 4.9668\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9610 - val_loss: 4.9695\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9593 - val_loss: 4.9700\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9587 - val_loss: 4.9679\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9584 - val_loss: 4.9661\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9577 - val_loss: 4.9646\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9574 - val_loss: 4.9626\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9575 - val_loss: 4.9616\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9583 - val_loss: 4.9626\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9572 - val_loss: 4.9618\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9570 - val_loss: 4.9608\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9567 - val_loss: 4.9615\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9605\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9567 - val_loss: 4.9603\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9564 - val_loss: 4.9593\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9596\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9562 - val_loss: 4.9605\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9560 - val_loss: 4.9590\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9586\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9556 - val_loss: 4.9583\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9577\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9555 - val_loss: 4.9576\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9559 - val_loss: 4.9579\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9554 - val_loss: 4.9578\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9553 - val_loss: 4.9582\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9552 - val_loss: 4.9602\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9588\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9559 - val_loss: 4.9590\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9553 - val_loss: 4.9578\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 309us/step - loss: 4.9550 - val_loss: 4.9575\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9626\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9577\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9577\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9546 - val_loss: 4.9567\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9545 - val_loss: 4.9569\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9568\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9571\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9557 - val_loss: 4.9589\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9574\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9547 - val_loss: 4.9571\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9566\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9543 - val_loss: 4.9567\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9544 - val_loss: 4.9566\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9566\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9580\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9547 - val_loss: 4.9613\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9568\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9564\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9540 - val_loss: 4.9562\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9564\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9545 - val_loss: 4.9570\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9539 - val_loss: 4.9563\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9560\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9538 - val_loss: 4.9562\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9561\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 293us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9561\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9562\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9536 - val_loss: 4.9573\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9535 - val_loss: 4.9562\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9560\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9561\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9558\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9557\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9558\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9559\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9559\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9561\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9562\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9559\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9532 - val_loss: 4.9560\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9560\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9532 - val_loss: 4.9558\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9559\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9557\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9529 - val_loss: 4.9559\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9531 - val_loss: 4.9558\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9558\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9530 - val_loss: 4.9561\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9529 - val_loss: 4.9562\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9530 - val_loss: 4.9559\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9568\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 294us/step - loss: 4.9530 - val_loss: 4.9560\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9529 - val_loss: 4.9561\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747498 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9998649\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9632302026719574\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   15\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 9s 2ms/step - loss: 5.0773 - val_loss: 5.1189\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9997 - val_loss: 5.0077\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9848 - val_loss: 4.9882\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9778 - val_loss: 4.9818\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9742 - val_loss: 4.9794\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9719 - val_loss: 4.9808\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9699 - val_loss: 4.9762\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9686 - val_loss: 4.9767\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9674 - val_loss: 4.9743\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9668 - val_loss: 4.9794\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9659 - val_loss: 4.9720\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9651 - val_loss: 4.9730\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9648 - val_loss: 4.9706\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9638 - val_loss: 4.9697\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9628 - val_loss: 4.9693\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9626 - val_loss: 4.9681\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9619 - val_loss: 4.9686\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9609 - val_loss: 4.9690\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9605 - val_loss: 4.9660\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9597 - val_loss: 4.9669\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9591 - val_loss: 4.9637\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9586 - val_loss: 4.9633\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9581 - val_loss: 4.9638\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9579 - val_loss: 4.9636\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9576 - val_loss: 4.9626\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9574 - val_loss: 4.9624\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9571 - val_loss: 4.9622\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9568 - val_loss: 4.9616\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9565 - val_loss: 4.9608\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9565 - val_loss: 4.9600\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9564 - val_loss: 4.9594\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9592\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9559 - val_loss: 4.9600\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9558 - val_loss: 4.9591\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9556 - val_loss: 4.9586\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9585\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9578\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9557 - val_loss: 4.9587\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9552 - val_loss: 4.9588\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9551 - val_loss: 4.9584\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9573\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9557 - val_loss: 4.9715\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9570 - val_loss: 4.9648\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9559 - val_loss: 4.9616\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9556 - val_loss: 4.9595\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9551 - val_loss: 4.9593\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9549 - val_loss: 4.9580\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9548 - val_loss: 4.9572\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9547 - val_loss: 4.9576\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9546 - val_loss: 4.9569\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9544 - val_loss: 4.9568\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9546 - val_loss: 4.9572\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9543 - val_loss: 4.9575\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9544 - val_loss: 4.9567\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9565\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9567\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9542 - val_loss: 4.9566\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9566\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9542 - val_loss: 4.9565\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 310us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9574\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9541 - val_loss: 4.9567\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9565\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9577\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9566\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9539 - val_loss: 4.9564\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9538 - val_loss: 4.9566\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9564\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9564\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9565\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9563\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9537 - val_loss: 4.9562\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9564\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9536 - val_loss: 4.9563\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9564\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9563\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9563\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9534 - val_loss: 4.9565\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9534 - val_loss: 4.9562\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9532 - val_loss: 4.9561\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9565\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 295us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9534 - val_loss: 4.9563\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9572\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9533 - val_loss: 4.9563\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9563\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9566\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9533 - val_loss: 4.9561\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9532 - val_loss: 4.9567\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9531 - val_loss: 4.9561\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9531 - val_loss: 4.9562\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9530 - val_loss: 4.9563\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9532 - val_loss: 4.9574\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9531 - val_loss: 4.9566\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9530 - val_loss: 4.9567\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9531 - val_loss: 4.9564\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9565\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9531 - val_loss: 4.9563\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9529 - val_loss: 4.9564\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9530 - val_loss: 4.9562\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9530 - val_loss: 4.9562\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747790 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -1.0\n","The max value of N 0.9996653\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9814144360315751\n","=======================\n","Saved model to disk....\n","========================================================================\n","RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  2\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   11\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False  True False ... False False False]\n","[INFO] : The idx_outlier is:  [ True False  True ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5016, 28, 28, 1)\n","Train Label Shape:  (5016,)\n","Validation Data Shape:  (1003, 28, 28, 1)\n","Validation Label Shape:  (1003,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (59, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6056, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (5997, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (59, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 5450 samples, validate on 606 samples\n","Epoch 1/150\n","5450/5450 [==============================] - 10s 2ms/step - loss: 5.0686 - val_loss: 5.1315\n","Epoch 2/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 5.0008 - val_loss: 5.0101\n","Epoch 3/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9869 - val_loss: 4.9903\n","Epoch 4/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9795 - val_loss: 4.9813\n","Epoch 5/150\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9754 - val_loss: 4.9774\n","Epoch 6/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9722 - val_loss: 4.9748\n","Epoch 7/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9701 - val_loss: 4.9731\n","Epoch 8/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9684 - val_loss: 4.9721\n","Epoch 9/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9671 - val_loss: 4.9717\n","Epoch 10/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9654 - val_loss: 4.9697\n","Epoch 11/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9643 - val_loss: 4.9687\n","Epoch 12/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9635 - val_loss: 4.9679\n","Epoch 13/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9626 - val_loss: 4.9673\n","Epoch 14/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9619 - val_loss: 4.9667\n","Epoch 15/150\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9612 - val_loss: 4.9653\n","Epoch 16/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9606 - val_loss: 4.9643\n","Epoch 17/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9602 - val_loss: 4.9639\n","Epoch 18/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9595 - val_loss: 4.9637\n","Epoch 19/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9592 - val_loss: 4.9636\n","Epoch 20/150\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9587 - val_loss: 4.9635\n","Epoch 21/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9585 - val_loss: 4.9638\n","Epoch 22/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9580 - val_loss: 4.9638\n","Epoch 23/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9578 - val_loss: 4.9638\n","Epoch 24/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9576 - val_loss: 4.9635\n","Epoch 25/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9574 - val_loss: 4.9621\n","Epoch 26/150\n","5450/5450 [==============================] - 2s 311us/step - loss: 4.9569 - val_loss: 4.9614\n","Epoch 27/150\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9570 - val_loss: 4.9621\n","Epoch 28/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9569 - val_loss: 4.9614\n","Epoch 29/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9566 - val_loss: 4.9602\n","Epoch 30/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9565 - val_loss: 4.9609\n","Epoch 31/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9570 - val_loss: 4.9621\n","Epoch 32/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9566 - val_loss: 4.9605\n","Epoch 33/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9563 - val_loss: 4.9601\n","Epoch 34/150\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9563 - val_loss: 4.9594\n","Epoch 35/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9562 - val_loss: 4.9591\n","Epoch 36/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9560 - val_loss: 4.9595\n","Epoch 37/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9559 - val_loss: 4.9598\n","Epoch 38/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9557 - val_loss: 4.9593\n","Epoch 39/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9558 - val_loss: 4.9592\n","Epoch 40/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9558 - val_loss: 4.9596\n","Epoch 41/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9587\n","Epoch 42/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9556 - val_loss: 4.9593\n","Epoch 43/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9554 - val_loss: 4.9585\n","Epoch 44/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9554 - val_loss: 4.9589\n","Epoch 45/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9555 - val_loss: 4.9583\n","Epoch 46/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9553 - val_loss: 4.9590\n","Epoch 47/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9552 - val_loss: 4.9579\n","Epoch 48/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9552 - val_loss: 4.9583\n","Epoch 49/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9551 - val_loss: 4.9583\n","Epoch 50/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9551 - val_loss: 4.9590\n","Epoch 51/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9552 - val_loss: 4.9582\n","Epoch 52/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9549 - val_loss: 4.9578\n","Epoch 53/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9550 - val_loss: 4.9580\n","Epoch 54/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 55/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9549 - val_loss: 4.9586\n","Epoch 56/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9548 - val_loss: 4.9579\n","Epoch 57/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9551 - val_loss: 4.9638\n","Epoch 58/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 59/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9551 - val_loss: 4.9582\n","Epoch 60/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9550 - val_loss: 4.9581\n","Epoch 61/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9573\n","Epoch 62/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9550 - val_loss: 4.9578\n","Epoch 63/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9548 - val_loss: 4.9575\n","Epoch 64/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9547 - val_loss: 4.9572\n","Epoch 65/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9575\n","Epoch 66/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 67/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9545 - val_loss: 4.9571\n","Epoch 68/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9574\n","Epoch 69/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 70/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9571\n","Epoch 71/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 72/150\n","5450/5450 [==============================] - 2s 306us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 73/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9544 - val_loss: 4.9573\n","Epoch 74/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 75/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 76/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9577\n","Epoch 77/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 78/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9545 - val_loss: 4.9625\n","Epoch 79/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9544 - val_loss: 4.9588\n","Epoch 80/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9576\n","Epoch 81/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9541 - val_loss: 4.9570\n","Epoch 82/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9572\n","Epoch 83/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9572\n","Epoch 84/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9578\n","Epoch 85/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9554 - val_loss: 4.9583\n","Epoch 86/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9573\n","Epoch 87/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9543 - val_loss: 4.9576\n","Epoch 88/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 89/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9542 - val_loss: 4.9575\n","Epoch 90/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 91/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 92/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9540 - val_loss: 4.9570\n","Epoch 93/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9568\n","Epoch 94/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 95/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9545 - val_loss: 4.9579\n","Epoch 96/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9572\n","Epoch 97/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9543 - val_loss: 4.9573\n","Epoch 98/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9541 - val_loss: 4.9568\n","Epoch 99/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9540 - val_loss: 4.9571\n","Epoch 100/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 101/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9540 - val_loss: 4.9572\n","Epoch 102/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9538 - val_loss: 4.9571\n","Epoch 103/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9538 - val_loss: 4.9568\n","Epoch 104/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 105/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 106/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9538 - val_loss: 4.9570\n","Epoch 107/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9538 - val_loss: 4.9609\n","Epoch 108/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9540 - val_loss: 4.9574\n","Epoch 109/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 110/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 111/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9539 - val_loss: 4.9570\n","Epoch 112/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9540 - val_loss: 4.9568\n","Epoch 113/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9571\n","Epoch 114/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9539 - val_loss: 4.9569\n","Epoch 115/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9537 - val_loss: 4.9567\n","Epoch 116/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 117/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9566\n","Epoch 118/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9537 - val_loss: 4.9568\n","Epoch 119/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9537 - val_loss: 4.9569\n","Epoch 120/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9565\n","Epoch 121/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 122/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 123/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9567\n","Epoch 124/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9534 - val_loss: 4.9569\n","Epoch 125/150\n","5450/5450 [==============================] - 2s 296us/step - loss: 4.9535 - val_loss: 4.9569\n","Epoch 126/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9537 - val_loss: 4.9566\n","Epoch 127/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9536 - val_loss: 4.9567\n","Epoch 128/150\n","5450/5450 [==============================] - 2s 297us/step - loss: 4.9536 - val_loss: 4.9565\n","Epoch 129/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 130/150\n","5450/5450 [==============================] - 2s 301us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 131/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 132/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9566\n","Epoch 133/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9535 - val_loss: 4.9568\n","Epoch 134/150\n","5450/5450 [==============================] - 2s 300us/step - loss: 4.9535 - val_loss: 4.9573\n","Epoch 135/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 136/150\n","5450/5450 [==============================] - 2s 298us/step - loss: 4.9535 - val_loss: 4.9574\n","Epoch 137/150\n","5450/5450 [==============================] - 2s 299us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 138/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 139/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9535 - val_loss: 4.9570\n","Epoch 140/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9537 - val_loss: 4.9576\n","Epoch 141/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9536 - val_loss: 4.9570\n","Epoch 142/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9536 - val_loss: 4.9568\n","Epoch 143/150\n","5450/5450 [==============================] - 2s 303us/step - loss: 4.9536 - val_loss: 4.9571\n","Epoch 144/150\n","5450/5450 [==============================] - 2s 304us/step - loss: 4.9534 - val_loss: 4.9567\n","Epoch 145/150\n","5450/5450 [==============================] - 2s 308us/step - loss: 4.9534 - val_loss: 4.9568\n","Epoch 146/150\n","5450/5450 [==============================] - 2s 305us/step - loss: 4.9534 - val_loss: 4.9566\n","Epoch 147/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9568\n","Epoch 148/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9533 - val_loss: 4.9567\n","Epoch 149/150\n","5450/5450 [==============================] - 2s 307us/step - loss: 4.9533 - val_loss: 4.9566\n","Epoch 150/150\n","5450/5450 [==============================] - 2s 302us/step - loss: 4.9534 - val_loss: 4.9569\n","(lamda,Threshold) 0.0 0.0\n","The type of b is ..., its len is  <class 'numpy.ndarray'> (6056, 784) 784\n","Iteration NUmber is :  0\n","NUmber of non zero elements  for N,lamda 4747716 0.0\n","The shape of N (6056, 784)\n","The minimum value of N  -0.9998827\n","The max value of N 0.9996737\n","side: 28\n","channel: 1\n","\n","Saving results for best after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//best/\n","\n","Saving results for worst after being encoded and decoded: @\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE//worst/\n","=====================\n","AUROC 0.0 0.9585414176014561\n","=======================\n","Saved model to disk....\n","========================================================================\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","AUROC computed  [0.9818694658063495, 0.9796141008357286, 0.9213137642267462, 0.9807870036713272, 0.9885422937457429, 0.9508991784027607, 0.9583944514630197, 0.9632302026719574, 0.9814144360315751, 0.9585414176014561]\n","AUROC ===== 0.9664606314456663 +/- 0.019353341504765486\n","========================================================================\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAYUAAAEVCAYAAAAPRfkLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nk6QQERGdJIWIiOgkKUREROcvH/zgC9oJRNMAAAAASUVORK5CYII=\n","text/plain":["<matplotlib.figure.Figure at 0x7efc3a563c18>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"2tXWQZ9FJiNw","colab_type":"text"},"cell_type":"markdown","source":["# **DCAE-MNIST 1_Vs_all**"]},{"metadata":{"id":"_X9HA3E5JeIZ","colab_type":"code","outputId":"fc514d78-f7de-47d9-fb41-d45fd4d731aa","executionInfo":{"status":"ok","timestamp":1541292671890,"user_tz":-660,"elapsed":207640,"user":{"displayName":"Raghav Chalapathy","photoUrl":"https://lh6.googleusercontent.com/-mTnxyVZGf6Y/AAAAAAAAAAI/AAAAAAAAAac/HoKq7cFHz2Q/s64/photo.jpg","userId":"14403072526210189011"}},"colab":{"base_uri":"https://localhost:8080/","height":5083}},"cell_type":"code","source":["from src.models.RCAE import RCAE_AD\n","import numpy as np \n","from src.config import Configuration as Cfg\n","\n","DATASET = \"mnist\"\n","IMG_DIM= 784\n","IMG_HGT =28\n","IMG_WDT=28\n","IMG_CHANNEL=1\n","HIDDEN_LAYER_SIZE= 32\n","MODEL_SAVE_PATH = PROJECT_DIR + \"/models/MNIST/RCAE/\"\n","REPORT_SAVE_PATH = PROJECT_DIR + \"/reports/figures/MNIST/RCAE/\"\n","PRETRAINED_WT_PATH = \"\"\n","RANDOM_SEED = [42,56,81,67,33,25,90,77,15,11]\n","AUC = []\n","\n","for seed in RANDOM_SEED:  \n","  Cfg.seed = seed\n","  rcae = RCAE_AD(DATASET,IMG_DIM, HIDDEN_LAYER_SIZE, IMG_HGT, IMG_WDT,IMG_CHANNEL, MODEL_SAVE_PATH, REPORT_SAVE_PATH,PRETRAINED_WT_PATH,seed)\n","  print(\"Train Data Shape: \",rcae.data._X_train.shape)\n","  print(\"Train Label Shape: \",rcae.data._y_train.shape)\n","  print(\"Validation Data Shape: \",rcae.data._X_val.shape)\n","  print(\"Validation Label Shape: \",rcae.data._y_val.shape)\n","  print(\"Test Data Shape: \",rcae.data._X_test.shape)\n","  print(\"Test Label Shape: \",rcae.data._y_test.shape)\n","  print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","  auc_roc = rcae.fit_and_predict()\n","  print(\"========================================================================\")\n","  AUC.append(auc_roc)\n","  \n","print(\"===========TRAINING AND PREDICTING WITH DCAE============================\")\n","print(\"AUROC computed \", AUC)\n","auc_roc_mean = np.mean(np.asarray(AUC))\n","auc_roc_std = np.std(np.asarray(AUC))\n","print (\"AUROC =====\", auc_roc_mean ,\"+/-\",auc_roc_std)\n","print(\"========================================================================\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/utils/visualization/mosaic_plot.py:2: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n","/content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks/src/data/GTSRB.py:9: UserWarning: \n","This call to matplotlib.use() has no effect because the backend has already\n","been chosen; matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n","or matplotlib.backends is imported for the first time.\n","\n","The backend was *originally* set to 'module://ipykernel.pylab.backend_inline' by the following code:\n","  File \"/usr/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n","    \"__main__\", mod_spec)\n","  File \"/usr/lib/python3.6/runpy.py\", line 85, in _run_code\n","    exec(code, run_globals)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py\", line 16, in <module>\n","    app.launch_new_instance()\n","  File \"/usr/local/lib/python3.6/dist-packages/traitlets/config/application.py\", line 658, in launch_instance\n","    app.start()\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelapp.py\", line 477, in start\n","    ioloop.IOLoop.instance().start()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/ioloop.py\", line 177, in start\n","    super(ZMQIOLoop, self).start()\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/ioloop.py\", line 888, in start\n","    handler_func(fd_obj, events)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n","    self._handle_recv()\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n","    self._run_callback(callback, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n","    callback(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/tornado/stack_context.py\", line 277, in null_wrapper\n","    return fn(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n","    return self.dispatch_shell(stream, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n","    handler(stream, idents, msg)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n","    user_expressions, allow_stdin)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n","    res = shell.run_cell(code, store_history=store_history, silent=silent)\n","  File \"/usr/local/lib/python3.6/dist-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n","    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2718, in run_cell\n","    interactivity=interactivity, compiler=compiler, result=result)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2822, in run_ast_nodes\n","    if self.run_code(code, result):\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2882, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"<ipython-input-2-b449f8e83a07>\", line 1, in <module>\n","    get_ipython().magic('matplotlib inline')\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2160, in magic\n","    return self.run_line_magic(magic_name, magic_arg_s)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2081, in run_line_magic\n","    result = fn(*args,**kwargs)\n","  File \"<decorator-gen-105>\", line 2, in matplotlib\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\", line 188, in <lambda>\n","    call = lambda f, *a, **k: f(*a, **k)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/magics/pylab.py\", line 100, in matplotlib\n","    gui, backend = self.shell.enable_matplotlib(args.gui)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\", line 2950, in enable_matplotlib\n","    pt.activate_matplotlib(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/IPython/core/pylabtools.py\", line 309, in activate_matplotlib\n","    matplotlib.pyplot.switch_backend(backend)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\", line 232, in switch_backend\n","    matplotlib.use(newbackend, warn=False, force=True)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\", line 1305, in use\n","    reload(sys.modules['matplotlib.backends'])\n","  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n","    _bootstrap._exec(spec, module)\n","  File \"/usr/local/lib/python3.6/dist-packages/matplotlib/backends/__init__.py\", line 14, in <module>\n","    line for line in traceback.format_stack()\n","\n","\n","  matplotlib.use('Agg')  # or 'PS', 'PDF', 'SVG'\n"],"name":"stderr"},{"output_type":"stream","text":["RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","[INFO:] Assertions of memory muted\n","Inside the MNIST_DataLoader RCAE.RESULT_PATH: /content/drive/My Drive/2018/Colab_Deep_Learning/one_class_neural_networks//reports/figures/MNIST/RCAE/\n","[INFO: ] Loading data...\n","[INFO:] The  label  of normal points are  1\n","[INFO:] THe shape of X is  (60000, 28, 28, 1)\n","[INFO:] THe shape of y is  (60000,)\n","[INFO] : The idx_normal is:  [False False False ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True  True ...  True  True  True]\n","[INFO] : The shape of X is:  (60000, 28, 28, 1)\n","[INFO] : The shape of y is:  (60000,)\n","[INFO:] Random Seed used is   42\n","[INFO:] THe shape of X is  (10000, 28, 28, 1)\n","[INFO:] THe shape of y is  (10000,)\n","[INFO] : The idx_normal is:  [False False  True ... False False False]\n","[INFO] : The idx_outlier is:  [ True  True False ...  True  True  True]\n","[INFO] : The shape of X is:  (10000, 28, 28, 1)\n","[INFO] : The shape of y is:  (10000,)\n","[INFO: ] Data loaded.\n","Train Data Shape:  (5677, 28, 28, 1)\n","Train Label Shape:  (5677,)\n","Validation Data Shape:  (1134, 28, 28, 1)\n","Validation Label Shape:  (1134,)\n","Test Data Shape:  (10000, 28, 28, 1)\n","Test Label Shape:  (10000,)\n","===========TRAINING AND PREDICTING WITH DCAE============================\n","[INFO: ] Shape of One Class Input Data used in training (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in training (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in training (67, 28, 28, 1)\n","[INFO: ] Shape of One Class Input Data used in testing (6821, 28, 28, 1)\n","[INFO: ] Shape of (Positive) One Class Input Data used in testing (6754, 28, 28, 1)\n","[INFO: ] Shape of (Negative) One Class Input Data used in testing (67, 28, 28, 1)\n","[INFO] compiling model...\n","Train on 6138 samples, validate on 683 samples\n","Epoch 1/150\n","6138/6138 [==============================] - 9s 1ms/step - loss: 4.9951 - val_loss: 4.9776\n","Epoch 2/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9622 - val_loss: 4.9638\n","Epoch 3/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9572 - val_loss: 4.9619\n","Epoch 4/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9557 - val_loss: 4.9618\n","Epoch 5/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9551 - val_loss: 4.9602\n","Epoch 6/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9547 - val_loss: 4.9596\n","Epoch 7/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9543 - val_loss: 4.9585\n","Epoch 8/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9540 - val_loss: 4.9589\n","Epoch 9/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9537 - val_loss: 4.9586\n","Epoch 10/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9535 - val_loss: 4.9585\n","Epoch 11/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9544 - val_loss: 4.9594\n","Epoch 12/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9535 - val_loss: 4.9576\n","Epoch 13/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9532 - val_loss: 4.9575\n","Epoch 14/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9530 - val_loss: 4.9577\n","Epoch 15/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9527 - val_loss: 4.9575\n","Epoch 16/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9527 - val_loss: 4.9568\n","Epoch 17/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9526 - val_loss: 4.9567\n","Epoch 18/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9525 - val_loss: 4.9573\n","Epoch 19/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9525 - val_loss: 4.9562\n","Epoch 20/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9524 - val_loss: 4.9572\n","Epoch 21/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9523 - val_loss: 4.9567\n","Epoch 22/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9522 - val_loss: 4.9566\n","Epoch 23/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9522 - val_loss: 4.9563\n","Epoch 24/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9529 - val_loss: 4.9589\n","Epoch 25/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9524 - val_loss: 4.9566\n","Epoch 26/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9522 - val_loss: 4.9567\n","Epoch 27/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9520 - val_loss: 4.9559\n","Epoch 28/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9520 - val_loss: 4.9561\n","Epoch 29/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9519 - val_loss: 4.9585\n","Epoch 30/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9518 - val_loss: 4.9566\n","Epoch 31/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9519 - val_loss: 4.9557\n","Epoch 32/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9518 - val_loss: 4.9558\n","Epoch 33/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9518 - val_loss: 4.9558\n","Epoch 34/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9518 - val_loss: 4.9557\n","Epoch 35/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9517 - val_loss: 4.9557\n","Epoch 36/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9517 - val_loss: 4.9554\n","Epoch 37/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 38/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9516 - val_loss: 4.9555\n","Epoch 39/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 40/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9515 - val_loss: 4.9554\n","Epoch 41/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9514 - val_loss: 4.9554\n","Epoch 42/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9514 - val_loss: 4.9554\n","Epoch 43/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 44/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9514 - val_loss: 4.9555\n","Epoch 45/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9514 - val_loss: 4.9562\n","Epoch 46/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 47/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9514 - val_loss: 4.9553\n","Epoch 48/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9513 - val_loss: 4.9552\n","Epoch 49/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9513 - val_loss: 4.9561\n","Epoch 50/150\n","6138/6138 [==============================] - 6s 1ms/step - loss: 4.9513 - val_loss: 4.9553\n","Epoch 51/150\n","5088/6138 [=======================>......] - ETA: 1s - loss: 4.9513"],"name":"stdout"}]},{"metadata":{"id":"NrfXlSbpGvOZ","colab_type":"text"},"cell_type":"markdown","source":["Produce Embeddings"]},{"metadata":{"id":"kFetanMC7Yi_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"_HiGOs197YsG","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"_etAfbNH7YvN","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"tjnkvt_y7Y2B","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"IKP8mM5i7Y5r","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"noGHYp_wbdZ4","colab_type":"text"},"cell_type":"markdown","source":["### Pretrain Autoencoder"]},{"metadata":{"id":"7Zbkh68AbdZ_","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"8oQZWfYvbdaB","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"gBEmq6iHbdaC","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"xo4yNY9ZbdaF","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]},{"metadata":{"id":"7URgC_zOGs4x","colab_type":"text"},"cell_type":"markdown","source":[""]}]}